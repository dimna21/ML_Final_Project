# Walmart store sales forecasting

# Timeseries მოდელები
### Data exploration and preprocessing
Timeseries მოდელების გამოკვლევისა და პრეპროცესინგის ნაწილში შემდეგი დაკვირვებებია ნაწარმოები თითოეული ცვლადისთვის (გრაფიკები მოცემულია ერთი მაღაზიის ჯამური გაყიდვებისთვის):

1. IsHoliday ცვლადის ნაცვლად თითოეული თარიღისთვის ამ კონკრეტული დღესასწაულის ცვლადია გამოყვანილი. მადლიერების დღისა და შობის დროს გაყიდვები ყველაზე მეტად იზრდება, superbowl-ს და labor day-ს კი შესამჩნევი, მაგრამ შედარებით ნაკლები ზრდა მოჰყვება.
<img width="1389" height="790" alt="Unknown-4" src="https://github.com/user-attachments/assets/9f9a5b3a-625a-4d19-ab60-87862cebc67f" />


2. CPI, Fuel price, unemployment rate და temperature ცვლადები ერთი შეხედვით გაყიდვების პატერნების შესახებ მკაფიო ინფორმაციას არ იძლევიან, მაგრამ ექსპერიმენტის ჩატარებისთვის ვტოვებთ.
<img width="1189" height="589" alt="Unknown" src="https://github.com/user-attachments/assets/bcc06d55-9e7b-4263-b181-80d049034399" />
<img width="1189" height="589" alt="Unknown-1" src="https://github.com/user-attachments/assets/0f47deff-5d5b-402f-9b4f-68bc460c3bf5" />
<img width="1189" height="589" alt="Unknown-2" src="https://github.com/user-attachments/assets/0f196fad-b440-4474-9395-050fe2def9cf" />
<img width="1189" height="589" alt="Unknown-3" src="https://github.com/user-attachments/assets/e082980c-cae6-4885-b2c8-5b3e22bb1872" />


3. ვინაიდან დღესასწაულების ცვლადები ყველაზე გამოსადეგი იყო, ორი შედარებით მძლავრი პრედიქტორისთვის დამატებულია days_to_{holiday} ცვლადები. როგორც მოსალოდნელია, ეს ცვლადები პიკს აღწევენ 0 მნიშვნელობისას (days_to_thanksgiving-ის შემთხვევაში პიკი -30 დღეზეა, რადგან ეს მომენტი შობას ემთხვევა). ამასთანავე, მოცემულ ცვლადში შენახულია ინფორმაცია დღესასწაულამდე და მის შემდეგ გაყიდვების ცვლილებაში. როდესაც days_to_{holiday} უარყოფითია, ეს მოდელს ანიშნებს, რომ დღესასწაულის შემდგომ პერიოდს ვუყურებთ, დადებითობის შემთხვევაში კი დღესასწაულის წინა დღეების მანიშნებელია. 
<img width="1143" height="790" alt="Unknown-5" src="https://github.com/user-attachments/assets/07a388c9-f560-4a48-bdd0-909336084d6a" />
<img width="1143" height="790" alt="Unknown-6" src="https://github.com/user-attachments/assets/db10b25d-0280-48f7-84a0-bc63f01e9e04" />



### ARIMA
ARIMA 3 პატამეტრისაგან შედგება:
1. AR - ავტო-რეგრესიული პარამეტრი p აკონკრეტებს იმას, თუ რამდენი კვირის მონაცემებს დააკვირდეს მოდელი წინა კვირებიდან. მოდელი ირჩევს წინა კვირების ტოპ p მონაცემს. კვირების რიგითობა განისაზღვრება ავტოკორელაციის მაღალი მნიშვნელობით, ანუ თუ მიმდინარე კვირის გაყიდვის კორელაციები წინა 5 კვირის გაყიდვებთან არის  (10, 3, -2, 4, -9), აირჩევა მოდულით უდიდესი p ცალი პარამეტრების ოპტიმიზაციისთვის.
2. I - Integrated პარამეტრი d განსაზღვრავს, თუ რა ხარისხის სხვაობა არის მეზობელ სემპლებს შორის, ანუ: d=0 ნიშნავს ორიგინალ მნიშვნელობებს (სტანდარტული ARMA), d=1 მათ შორის სხვაობებს, d=2 მოიაზრებს სხვაობების სხვაობებს და ა.შ.
3. MA - moving average პარამეტრი q აკონკრეტებს, თუ წინა რამდენი ნაბიჯის ერორზე დაფუძნებით გაასწოროს მოდელმა პარამეტრები.

ვინაიდან ARIMA-ს არ შეუძლია გარე ცვლადების მიღება, იგი მხოლოდ weekly_sales ცვლადზეა დატრენინგებული. ოპტიმალური იქნებოდა ყველა store-department წყვილისთვის საკუთარი მოდელის გაკეთება, მაგრამ ზოგ მათგანს არასაკმარისი რაოდენობის დატა აქვს და ~3300 მოდელის დატრეინებისთვის საკმარისი მეხსიერება არცერთი პლატფორმის runtime-ზე არ მოგვეპოვება. ასევე, ტესტ სეტში გვხვდება 11 store-dept წყვილი, რომლებზე დატაც საერთოდ არ გვაქვს მოცემული. ამის გამო, დატრეინებულია 45 ცალი storewise aggregated მოდელი, პრედიქციის დროს კი მთელი მაღაზიის გაყიდვები მრავლდება queried დეპარტამენტის გაყიდვების პროპორციაზე. ხსენებული 11 დეპარტამენტისთვის ბრუნდება მაღაზიის გაყიდვების საშუალო მნიშვნელობა.

ტრენინგის დროს მონაცემები დაყოფილია train/val <=> 123/20 კვირად. გატესტილია შემდეგი (p,q,d) პარამეტრები: [1,1,0], [0,1,1], [1,1,1],[2,1,1],[1,1,2],[2,1,2],[0,1,2],[3,1,1],[1,1,3], [4,1,1]. მათგან საუკეთესო აღმოჩნდა (2,1,1), რომელმაც kaggle-ს ტესტ სეტზე 4896 WMAE აიღო. 
<img width="975" height="90" alt="Screenshot 2025-07-28 at 10 12 15 pm" src="https://github.com/user-attachments/assets/d99baabb-5c4e-455e-8b75-655657c7b98f" />

### SARIMAX
SARIMAX მოდელი ARIMA-ზეა დაფუძნებული. იგი დამატებით ითვალისწინებს მონაცემთა სეზონურობასა და გარეგან ცვლადებს. სეზონურობის პარამეტრად გატანებულია (1,1,1,52), რაც გულისხმობს, რომ არასამიზნე მონაცემები ერთწლიანი სეზონურობით ხასიათდება და მათზე [1,1,1] ARIMA უნდა გაეშვას. (p,q,d) პარამეტრები არის (1,1,1) და ამ მოდელმა ძალიან მცირედი გაუმჯობესება მოგვცა 4836 WMAE-ს სახით.
https://dagshub.com/dimna21/ML_Final_Project/experiments#/experiment/m_be089dceb9b647249e74dbb42fe85561
<img width="966" height="66" alt="Screenshot 2025-07-28 at 10 12 30 pm" src="https://github.com/user-attachments/assets/cca68d34-8083-4d65-bc54-9c54cd621cf5" />

ARIMA და SARIMAX მოდელების პრობლემა ისაა, რომ ინფუთად იღებენ მხოლოდ ერთ ცალ Timeseries ობიექტს. შეუძლებელია გლობალური ტენდენციების ინფორმაციის ჩადება ერთ ცალ მოდელში რაიმე ტიპის აგრეგაციის გარეშე, რის გამოც იძულებულნი ვართ, storewise აგრეგაცია გამოვიყენოთ. ეს მიდგომა დამაკმაყოფილებელი იქნებოდა იმ შემთხვევაში, თუ დეპარტამენტების გაყიდვების პროპორციების განაწილება თითოეული მაღაზიის შიგნით მეტ-ნაკლებად თანაბარი აღმოჩნდებოდა, ვინაიდან ბევრი დეპარტამენტის პირობებში ყველას წონა დაბალი გამოვიდოდა, თუმცა ეს ასე არ არის:
<img width="1989" height="3555" alt="Unknown" src="https://github.com/user-attachments/assets/57527dac-9ec4-4f86-9624-bb8bb31a6c31" />
არათანაბარი განაწილების პირობებში გვაქვს მაღალი პროპორციის მქონე დეპარტამენტები, ამ დიდ პროპორციაზე ფრედიქშენის გამრავლება კი ერორს ადიდებს. კიდევ ერთი პრობლემა არის ის, რომ დღესასწაულების დღეებში ზოგიერთი დეპარტამენტის პროპორცია ბევრად უფრო დიდია, ვიდრე ჩვენს მიერ გამოყვანილი საშუალო მაჩვენებლები, რის გამოც მოდელი ამ კვირებზე undershooting-ს აკეთებს და შედარებით ცუდად იჭერს holiday სპეციფიკებს. მეტი რესურსის ქონის შემთხვევაში ყველა store-dept წყვილისთვის დავატრეინებდით მოდელებს და სტატისტიკური გასაშუალოების გარეშე უკეთესი შედეგი გვექნებოდა. 

### Prophet
Facebook Prophet არის additive regression მოდელი, რაც მოიაზრებს იმას, რომ ფრედიქშენები რამდენიმე კომპონენტის აჯამვითაა მიღებული: y(t) = g(t) + s(t) + h(t). g კომპონენტი ამოდელირებს სამიზნე ცვლადის არაწრფივ ქცევას, s სეზონურობას, h კი დღესასწაულებთან დაკავშირებულ თავისებურებებს. g ახდენს changepoint indicator ვექტორის ოპტიმიზაციას, ანუ სწავლობს ზოგად ქცევას მიმდევრობის მომდევნო წევრებს შორის. სეზონურობის კომპონენტი ცდილობს n რიგის ფურიეს მწკრივის კოეფიციენტების ოპტიმიზაციას s = Σₙ₌₁¹⁰ [aₙʸ cos(2πnt/365) + bₙʸ sin(2πnt/365)]. h კომპონენტი შედარებით მარტივია. იგი თითოეული i-ური დღესასწაულისთვის სწავლობს სამიზნე ცვლადზე გავლენის წონას და კონკრეტულ t timestamp-ზე h-ის გამოსათვლელად თითოეულს ჯამავს. მეხსიერების შეზღუდვის გამო გამოყენებულია იგივე აგრეგაციული მიდგომა, რაც წინა ორი მოდელის შემთხვევაში იყო და ტესტ სეტზე შედეგიც დაახლოებით იგივეა. 
<img width="945" height="123" alt="Screenshot 2025-07-29 at 5 39 02 pm" src="https://github.com/user-attachments/assets/9cfe0cd0-e783-4cd8-98a7-7a7d21d511ef" />

# ხის მოდელები
### Data exploration and preprocessing
# Seasonal
სეზონური data exploration მსგავსია ზემოთ time series მოდელებისათვის. ერთი განსხვავება ისაა, რომ back to school პერიოდიც განვიხილე, თუმცა ამ პერიოდის შესაბამისმა ცვლადებს საგულისხმო გავლენა
არ ჰქონდათ გაყიდვებზე, რაც პლოტზეც ჩანს, ამიტომ საბოლოოდ აღარ გამოვიყენეთ პრეპროცესინგში.
<img width="1389" height="590" alt="image" src="https://github.com/user-attachments/assets/8116442f-13f0-46c9-ad18-b23fd1c10997" />

# CPI, Unemployment
feature.csv თეიბლში CPI და Unemployment ცვლადი დაახლოებით 7% ცარიელი იყო, ზუსტად ერთი და იგივე რაოდენობის. დაკვირვების შედეგად ეს ცვლადები ტესტ სეტის ბოლო პერიოდში იყო უცნობი, ყველა სთორისთვის, რაც ნაჩვენებია ქვემო heatmap პლოტებზე.
<img width="1489" height="989" alt="image" src="https://github.com/user-attachments/assets/ccd00025-4a76-476e-a2ad-f7a7250785fb" />

cpi და unemployment თარიღების მიხედვით ასე გამოიყურებოდა. ჩანს, რომ დაახლოებიტ 2013-04-05 თარიღის შემდეგ არ გვაქვს ეს ცვლადები.
<img width="1189" height="790" alt="image" src="https://github.com/user-attachments/assets/048cc1fc-d079-4691-84e9-e217cab0bb82" />

ზემოთა პლოტებზე შეგვიძლია ტრენდები შევნიშნოთ. cpi დროთა განმავლობაში მატულობს, ხოლო unemployment მცირდება. სავარუდოდ უმუშევრობის საფეხურად შემცირება იქიდანაა გამოწვეული, რომ
ეს ცვლადი ყოველი თვის დასაწყისში განისაზღვრებოდა, და მთელ თვეზე იგივე რჩებოდა, როცა სინამდვილეში, ალბათ დაეცემოდა თვის განმავლობაშიც.
ამ დაკვირვების საფუძველზე, გავაკეთეთ ორივე ცვლადის წრფივი ინტერპოლაცია ბოლო სრული მონაცემების მიხედვითაც და მხოლოდ ბოლო 12 თვის მიხედვით, სადაც უკანასკნელმა უკეთესი შედეგი აჩვენა. ქვემოთაა მოცემული ინტერპოლირებული ცვლადების პლოტები.
<img width="1189" height="790" alt="image" src="https://github.com/user-attachments/assets/8ec3083f-6b32-4547-b120-bdb8abe0620c" />

# Statistical
შევქმენით ახალი სვეტები, რომლებიც ასახავს საშუალოს, მედიანსა და საშუალო გადახრას თითოეული სთორისთვის, დეპარტამენტისთვის და 
სთორ-დეპარტამენტის კომბინაციისთვის. ამ ცვლადების ინტეგრირების შემდეგ მარტივი xgb მოდელისათვის შეგვიძლია ვნახოთ თუ რამდენად
ეფექტურია ახალი ცვლადები feature importance პლოტის მიხედვით.
<img width="1190" height="790" alt="image" src="https://github.com/user-attachments/assets/689367b2-4e79-4a77-b5fd-67b3b2d45d31" />

გრაფიკი ლოგ სკალაზეა აგებული სადაც მკაფიოდ ჩანს, რომ store/dept კომბინაციის სტატისტიკური ცვლადები ძალიან მნიშვნელოვნად ეხმარება მოდელს.
ხოლო სხვა ახალი ცვლადებიც კარგი ინდიკატორებია.

# Economical
შევქმენით 3 სახის ეკონომიკური ცვლადები:
  1. Fuel price high/low flags and interaction with store Size
      საწვავის ფასის გავლენის დასანახად.
      <img width="766" height="566" alt="image" src="https://github.com/user-attachments/assets/12b7e9a3-5dc0-49ff-97b9-311a32a1a754" />

      პლოტიდან ჩანს, რომ თუ საწვავი ფასის პიკისკენაა (უმაღლესი 25 პროცენტში), მაშინ ნაკლები გაყიდვებია, მაგრამ რაღაც ძალიან განსაკუთრებული
      შედეგი არ არის. მარტივი xgb მოდელეზე ტრენინგის დროსაც ამ ცვლადს არ გაუმჯობესებია შედეგი, არამედ ოდნავ უარესი შედეგი დადო, ამიტომ
      ეს ცვლადები აღარ გამოგვიყენებია.

     დავამატეთ შემდეგი მარტივი ცვლადები: day, month, year, week_of_year, რათა უკეთ დაინახოს სეზონურობა მოდელმა.
     ასევე ციკლური ენკოდინგით შევქმენით: month_sin, month_cos, week_sin, week_cos. ციკლური ენკოდირება თვეს/კვირას ერთეულოვან
     წრეწირზე მაპავს, რომლის თავი და ბოლო (პირველი და ბოლო თვე/კვირის დღე) დაკავშირებულია. ეს მეთოდი ეხმარება მოდელს უკეთ
     დაინახოს თვისა და კვირის ცირკადულობა.
     მსგავსი ენკოდინგის ცვლადები დავამატეთ კვარტალებისთვისა და წლისათვის, თუმცა არ გაუმჯობესებია მოდელი, სავარაუდოდ იმიტომ,
     რომ სულ 2 წლის ტრენინგის დატა გვაქვს.
          
  2. economic_pressure და purchasing_power
      რომლებიც შემდეგნაირად გამოითვლება: CPI x Unemployment და CPI / (Unemployment + 1e-8) (ნოლზე გაყოფის ასარიდებლად).
      <img width="766" height="566" alt="image" src="https://github.com/user-attachments/assets/8f734eaa-05fc-45f2-8b0c-d60f24fe018f" />

      ამ შექმნილმა ცვლადებმა დიდად შედეგი ვერ გამოიღო, როცა მოდელი ავაგეთ და მისი feature importance-ებიც ვნახეთ. ზემოთა პლოტიდანაც ჩანს,
      რომ ეს ცვლადები კარგ ინდიკატორებად არ გამოდგნენ, ამიტომ ეს ცვლადებიც აღარ გამოგვიყენებია.
      
  3. Markdown effectiveness
     total_markdown, markdown_count, avg_markdown, holiday_markdown_boost.
     <img width="770" height="566" alt="image" src="https://github.com/user-attachments/assets/248e6e00-a5c3-46af-bebf-501ae6063833" />

     ამ ცვლადებმა ოდნავ დადებითი გავლენა მოახდინა მოდელის შედეგებზე, რაც ზემოთა პლოტიდანაც ჩანს, რომ როცა სხვადასხვა ტიპის markdown
     გვაქვს, მაშინ უფრო მეტი გაყიდვებია. ამ პლოტში 0-ებზე კონცენტრირებული მონაცემების დიდი ნაწილი, ეს იქიდან გამომდინარეა, რომ
     markdown-ების თითოეული markdown   ცვლადისთვის 50%-ზე მეტი nan იყო, რაც 0-ებით შევავსეთ, ვინაიდან markdown ფასდაკლებების/ფრომოუშენების
     ცვლადია და მისი nan მნიშვნელობა, სავარააუდოდ მის არ არსებობას ნიშნავს.

# Lag features
ვინაიდან ხის მოდელებს დროის "აღქმა" არ შეუძლიათ, მათ სჭირდებათ განსაკუთრებით ესაჭიროებათ ზემოთ ნახსენები სეზონური ცვლადები, და ასევე 
lag ცვლადები. lag ცვლადები არის იმისათვის, რომ მოდელმა დაინახოს წინა (ჩვენს შემთხვევაში წინა კვირ(ებ)ის) გაყიდვების მონაცემები, და მათზე დაყრდნობით
დაისწავლოს სეზონურობა. ქვემოთ მოცემულია პლოტები, რომლებზეც რანდომულად ამორჩეული სთორ/დეპარტამენტის კომბინაციისთვის ვცედავთ გაყიდვების რაოდენობის
ავტოკორელაციასა (ACF) და ნაწილობრივ კორელაციას (PACF). ღია ლურჯი ხაზით ნაჩვენებია 95% confidence band, რაც მარტივად იმას ნიშნავს, რომ თუ ამ ზღვარს სცდება 
მონაცემი, მაშინ მისი მნიშვნელობა საგულისხმოა, და უბრალო ხმაურისგან არაა გენერირებული.
<img width="1589" height="1189" alt="image" src="https://github.com/user-attachments/assets/1158d994-ff3f-4ca8-b7cc-7d418c1c9c62" />

პლოტიდან ადვილად ჩანს, რომ წინა რამდენიმე კვირის გაყიდვებზე ძალიანაა დამოკიდებული მომდევნო კვირის გაყიდვების რაოდენობა. ასევე ვხედავთ დიდი კორელაციას
52 კვირის (1 წლის) წინანდელ მონაცემთან და პატარა პიკს 26 კვირასთან. ამ პლოტების მიხედვით ავარჩიეთ ქვემოთ მოყვანილი ცვლადების პარამეტრები. 

დავაგენერირეთ 3 ტიპის lag ცვლადები: 1. lag, 2. rolling_windows (rw), 3. exponentialy weighted means (ewm). 
1. Lag - ყველაზე მარტივი ცვლადი. ავარჩიეთ 1, 2, 3, 5, 26 და 52 კვირის lag-ები პლოტებზე დაკვირვების და ტესტირების შედეგად.
2. Rolling Windows - წინა N კვირების გასაშუალებული მნიშვნელობები. ეს ეხმარება ზოგადი ტრენდი დაინახოს მოდელმა, ანუ წინა რამდენიმე lag-ის გაერთიანება. ავარჩიეთ 4 და 8.
3. Exponentially Weighted Means - იგივე რაც rolling windows, იმ განსხვავებით, რომ ახალ ინფორმაციას უფრო დიდ მნიშვნელობას ანიჭებს, ვიდრე ძველებს. მაგ. ewm_4 ბოლო 4 კვირის
   lag-ს მიანიჭებს უფრო დიდ მნიშვნელობას, ვიდრე კვირებს მანამდე, თუმცა 4 კვირაზე უკანაც "იხედაბა". ავარჩიეთ 4, 8.

# Data Split
მოდელისთვის ტრენინგ სეტი ტრენინგად და ვალიდაციად გავყე ისე, რომ ბოლო 37 კვირა ტრენინგიდან იყო ვალიდაციისთვის. ტესტ სეტში 39 კვირაა
და რადგან ტრეინ სეტი ოდნავ პატარაა, 39-ის ნაცვლად 37 კვირა ავიღეთ ვალიდაციისთვის.

### XGBoost
ეს არაწრფივი მოდელი ხეებს თანმიმდევრობით ცაკ-ცალკე აგებს, სადაც ყოველი შემდეგი ხე ცდილობს წინა ხის ერორის შესწორებას. საკმაოდ მარტივი და ადვილად 
სატრენინგებელი მოდელია, რომელიც წინა დავალებებშიც გამოვიყენეთ. XGB დამუშავებულ, numerical მონაცემებს ადვილად ერგება, მიუხდავად მათი რაოდენობის - პრეპროცესინგის 
შედეგად 55-მდე ცვლადი შემოვიდა. სეზონური და lag ცვლადები განსაკუთრებით გამოადგება ამ მოდელს.

XGB ერთი და იგივე მოდელი გავტესტეთ feature engineered ცვლადების მიხედვით split-ის შემდეგ, რომლის შედეგადაც შემდეგი WMAE-ები მივიღეთ:
1. ცვლადების ინჟინერიის გარეშე: WMAE=3026.6993
2. მხოლოდ lag ცვლადების დამატებით: WMAE=454.8395
3. მხოლოდ სეზონური ცვლადები დამატებით: WMAE=2090.7656
4. მხოლოდ სტატისტიკური ცვლადები დამატებით: WMAE=2771.7729
5. მხოლოდ ეკონომიკური ცვლადები დამატებით: WMAE=3203.2226

RFE-თ დადგენილი ტოპ 20 ცვლადის პლოტია ქვემოთ მოცემული:
<img width="790" height="590" alt="image" src="https://github.com/user-attachments/assets/a6854422-d76a-4ac8-a27e-29f84da90293" />

ტრენინგისას დავატუნინგეთ შემდეგი ცვლადები - learning rate, n_estimators, max_depth, subsample, colsample_bytree, gamma, reg_alpha, reg_lambda.
ამ პარამეტრების ტუნინგიდან თითქმის მთელი გავლენა პირველმა 3-მა იქონია, სავარაუდოდ იმიტომ, რომ ოვერფიტზე ისედაც არ გადიოდა მოდელი.

საბოოო საუკეთესო მოდელის პარამეტრები:
1. learning rate = 0.03
2. n_estimators = 5000
3. max_depth = 9
4. subsample = 0.9
5. colsample_bytree = 0.8
6. gamma = 0
7. reg_alpha = 1
8. reg_lambda = 5

submission scores: private: 3001.69, public: 2875.75
<img width="1363" height="114" alt="image" src="https://github.com/user-attachments/assets/cb9f5c1c-734b-46fe-985b-b225631eeb62" />


### LightGBM



ხის მოდელების ტრენინგის ნახვა შეგიძლიათ ნახოთ გიტჰაბის შესაბამის ნოუთვუქებში (ბევრი პლოტი ახლავს თან) და/ან mlflow ლინკზე: https://dagshub.com/nkhar21/ML_Final_Project.mlflow/#/experiments/0searchFilter=&orderByKey=attributes.start_time&orderByAsc=false&startTime=ALL&lifecycleFilter=Active&modelVersionFilter=All+Runs&datasetsFilter=W10%3D

# ნეირონული ქსელები
### Data exploration and preprocessing

### N-BEATS

### Temporal Fusion Transformer

### PatchTST

### DLinear


