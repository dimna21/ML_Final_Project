{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":3816,"databundleVersionId":32105,"sourceType":"competition"}],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-07-18T09:15:57.709643Z","iopub.execute_input":"2025-07-18T09:15:57.710138Z","iopub.status.idle":"2025-07-18T09:15:59.851121Z","shell.execute_reply.started":"2025-07-18T09:15:57.710098Z","shell.execute_reply":"2025-07-18T09:15:59.850311Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/walmart-recruiting-store-sales-forecasting/train.csv.zip\n/kaggle/input/walmart-recruiting-store-sales-forecasting/sampleSubmission.csv.zip\n/kaggle/input/walmart-recruiting-store-sales-forecasting/stores.csv\n/kaggle/input/walmart-recruiting-store-sales-forecasting/features.csv.zip\n/kaggle/input/walmart-recruiting-store-sales-forecasting/test.csv.zip\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"features = pd.read_csv(\"/kaggle/input/walmart-recruiting-store-sales-forecasting/features.csv.zip\")\ntrain = pd.read_csv(\"/kaggle/input/walmart-recruiting-store-sales-forecasting/train.csv.zip\")\nstores = pd.read_csv(\"/kaggle/input/walmart-recruiting-store-sales-forecasting/stores.csv\")\ntest = pd.read_csv(\"/kaggle/input/walmart-recruiting-store-sales-forecasting/test.csv.zip\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-18T09:16:02.237692Z","iopub.execute_input":"2025-07-18T09:16:02.238168Z","iopub.status.idle":"2025-07-18T09:16:02.686089Z","shell.execute_reply.started":"2025-07-18T09:16:02.238141Z","shell.execute_reply":"2025-07-18T09:16:02.685217Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"!pip install darts\n!pip install mlflow\n!pip install dagshub","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import dagshub\nimport mlflow\ndagshub.init(repo_owner='dimna21', repo_name='ML_Final_Project', mlflow=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-18T11:18:33.508092Z","iopub.execute_input":"2025-07-18T11:18:33.508884Z","iopub.status.idle":"2025-07-18T11:18:44.341823Z","shell.execute_reply.started":"2025-07-18T11:18:33.508843Z","shell.execute_reply":"2025-07-18T11:18:44.340989Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"                                       \u001b[1m❗❗❗ AUTHORIZATION REQUIRED ❗❗❗\u001b[0m                                        \n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">                                       <span style=\"font-weight: bold\">❗❗❗ AUTHORIZATION REQUIRED ❗❗❗</span>                                        \n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Output()","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"\n\nOpen the following link in your browser to authorize the client:\nhttps://dagshub.com/login/oauth/authorize?state=30f0c980-266f-47c9-aae1-79ed1f686cca&client_id=32b60ba385aa7cecf24046d8195a71c07dd345d9657977863b52e7748e0f0f28&middleman_request_id=49db9bdb8f7808f7f6262904eba4aac6e5433bdd41fabf82245a867ec93cd91b\n\n\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Accessing as dimna21\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Accessing as dimna21\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Initialized MLflow to track repo \u001b[32m\"dimna21/ML_Final_Project\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Initialized MLflow to track repo <span style=\"color: #008000; text-decoration-color: #008000\">\"dimna21/ML_Final_Project\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Repository dimna21/ML_Final_Project initialized!\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Repository dimna21/ML_Final_Project initialized!\n</pre>\n"},"metadata":{}}],"execution_count":73},{"cell_type":"code","source":"from sklearn.base import BaseEstimator, TransformerMixin\n\nclass BaseMerger(BaseEstimator, TransformerMixin):\n    \n    def __init__(self, features, stores):\n        self.feature_store = features.merge(stores, how='inner', on='Store')\n        self.feature_store['Date'] = pd.to_datetime(self.feature_store['Date'])\n\n    def fit(self, X, y=None):\n        return self\n\n    def transform(self, X):\n        X = X.copy()\n        X['Date'] = pd.to_datetime(X['Date'])\n        merged = X.merge(self.feature_store, how='inner', on=['Store', 'Date', 'IsHoliday'])\n        merged = merged.sort_values(by=['Date', 'Store', 'Dept']).reset_index(drop=True)\n        return merged","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-18T09:20:40.158115Z","iopub.execute_input":"2025-07-18T09:20:40.158802Z","iopub.status.idle":"2025-07-18T09:20:40.165642Z","shell.execute_reply.started":"2025-07-18T09:20:40.158771Z","shell.execute_reply":"2025-07-18T09:20:40.164780Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"class FeatureAdder(BaseEstimator, TransformerMixin):\n    def __init__(self):\n        self.superbowl = pd.to_datetime(['2010-02-12', '2011-02-11', '2012-02-10', '2013-02-08'])\n        self.labor_day = pd.to_datetime(['2010-09-10', '2011-09-09', '2012-09-07', '2013-09-06'])\n        self.thanksgiving = pd.to_datetime(['2010-11-26', '2011-11-25', '2012-11-23', '2013-11-29'])\n        self.christmas = pd.to_datetime(['2010-12-31', '2011-12-30', '2012-12-28', '2013-12-27'])\n\n    def fit(self, X, y=None):\n        return self\n\n    def transform(self, X):\n        X = X.copy()\n\n        # Convert temperature to Celsius\n        if 'Temperature' in X.columns:\n            X['Temperature'] = (X['Temperature'] - 32) * (5.0 / 9.0)\n\n        # Basic date parts\n        X['Day'] = X['Date'].dt.day\n        X['Month'] = X['Date'].dt.month\n        X['Year'] = X['Date'].dt.year\n\n        # Extract ISO week and year for holiday matching\n        X['Week'] = X['Date'].dt.isocalendar().week\n        X['YearNum'] = X['Date'].dt.year\n\n        # Helper to flag if a date is in same ISO week/year as a known holiday\n        def is_holiday_week(date_series, holidays):\n            holiday_weeks = set((d.isocalendar().week, d.year) for d in holidays)\n            return date_series.apply(lambda d: (d.isocalendar().week, d.year) in holiday_weeks if pd.notnull(d) else False).astype(int)\n\n        X['SuperbowlWeek'] = is_holiday_week(X['Date'], self.superbowl)\n        X['LaborDayWeek'] = is_holiday_week(X['Date'], self.labor_day)\n        X['ThanksgivingWeek'] = is_holiday_week(X['Date'], self.thanksgiving)\n        X['ChristmasWeek'] = is_holiday_week(X['Date'], self.christmas)\n\n        # Calculate days to Thanksgiving and Christmas (using Nov 24 and Dec 24 as anchor dates)\n        thanksgiving_dates = pd.to_datetime(X['Year'].astype(str) + \"-11-24\")\n        christmas_dates = pd.to_datetime(X['Year'].astype(str) + \"-12-24\")\n\n        X['Days_to_Thanksgiving'] = (thanksgiving_dates - X['Date']).dt.days\n        X['Days_to_Christmas'] = (christmas_dates - X['Date']).dt.days\n\n        # Clean up helper cols\n        X = X.drop(columns=['Week', 'YearNum'])\n\n        return X","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-18T09:20:40.812455Z","iopub.execute_input":"2025-07-18T09:20:40.812742Z","iopub.status.idle":"2025-07-18T09:20:40.822997Z","shell.execute_reply.started":"2025-07-18T09:20:40.812723Z","shell.execute_reply":"2025-07-18T09:20:40.822257Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"class MissingValueFiller(BaseEstimator, TransformerMixin):\n    def __init__(self):\n        self.markdown_cols = ['MarkDown1', 'MarkDown2', 'MarkDown3', 'MarkDown4', 'MarkDown5']\n        self.mean_cols = ['CPI', 'Unemployment']\n        self.mean_values = {}\n\n    def fit(self, X, y=None):\n        for col in self.mean_cols:\n            if col in X.columns:\n                self.mean_values[col] = X[col].mean()\n        return self\n\n    def transform(self, X):\n        X = X.copy()\n\n        # Fill markdowns with 0\n        for col in self.markdown_cols:\n            if col in X.columns:\n                X[col] = X[col].fillna(0.0)\n\n        # Fill CPI and Unemployment with learned mean\n        for col in self.mean_cols:\n            if col in X.columns and col in self.mean_values:\n                X[col] = X[col].fillna(self.mean_values[col])\n\n        return X","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-18T09:20:41.543233Z","iopub.execute_input":"2025-07-18T09:20:41.544075Z","iopub.status.idle":"2025-07-18T09:20:41.550621Z","shell.execute_reply.started":"2025-07-18T09:20:41.544047Z","shell.execute_reply":"2025-07-18T09:20:41.549581Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"class CategoricalEncoder(BaseEstimator, TransformerMixin):\n    def __init__(self):\n        self.type_mapping = {'A': 3, 'B': 2, 'C': 1}\n        self.holiday_mapping = {False: 0, True: 1}\n\n    def fit(self, X, y=None):\n        return self\n\n    def transform(self, X):\n        X = X.copy()\n\n        if 'Type' in X.columns:\n            X['Type'] = X['Type'].map(self.type_mapping)\n\n        if 'IsHoliday' in X.columns:\n            X['IsHoliday'] = X['IsHoliday'].map(self.holiday_mapping)\n\n        return X","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-18T09:20:42.188475Z","iopub.execute_input":"2025-07-18T09:20:42.189400Z","iopub.status.idle":"2025-07-18T09:20:42.195130Z","shell.execute_reply.started":"2025-07-18T09:20:42.189358Z","shell.execute_reply":"2025-07-18T09:20:42.194148Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"class StoreAggregator(BaseEstimator, TransformerMixin):\n    def __init__(self):\n        self.timeseries = {}\n    \n    def fit(self, X, y=None):\n        return self\n    \n    def transform(self, X):\n        self.timeseries = {}\n        for store in X['Store'].unique():\n            self.aggregate_store_info(store, X)\n        return self.timeseries\n    \n    def aggregate_store_info(self, store_id, X):\n        store_data = X[X['Store'] == store_id].copy()\n        \n        # Check if Weekly_Sales exists (train data) or not (test data)\n        has_weekly_sales = 'Weekly_Sales' in store_data.columns\n        \n        if has_weekly_sales:\n            sum_columns = ['Weekly_Sales', 'MarkDown1', 'MarkDown2', 'MarkDown3', 'MarkDown4', 'MarkDown5']\n        else:\n            sum_columns = ['MarkDown1', 'MarkDown2', 'MarkDown3', 'MarkDown4', 'MarkDown5']\n        \n        first_columns = ['IsHoliday', 'Temperature', 'Fuel_Price', 'CPI', 'Unemployment',\n                        'Type', 'Size', 'Day', 'Month', 'Year', 'SuperbowlWeek',\n                        'LaborDayWeek', 'ThanksgivingWeek', 'ChristmasWeek',\n                        'Days_to_Thanksgiving', 'Days_to_Christmas']\n        \n        agg_dict = {}\n        \n        # Add sum columns that exist in the data\n        for col in sum_columns:\n            if col in store_data.columns:\n                agg_dict[col] = 'sum'\n        \n        # Add first columns that exist in the data\n        for col in first_columns:\n            if col in store_data.columns:\n                agg_dict[col] = 'first'\n        \n        aggregated = store_data.groupby(['Date', 'Store']).agg(agg_dict).reset_index()\n        aggregated = aggregated.sort_values('Date').reset_index(drop=True)\n        \n        # Calculate department proportions only if Weekly_Sales exists\n        if has_weekly_sales:\n            dept_proportions = self.calculate_dept_proportions(store_data)\n        else:\n            dept_proportions = None\n        \n        self.timeseries[store_id] = (aggregated, dept_proportions)\n        return aggregated\n    \n    def calculate_dept_proportions(self, store_data):\n        dept_totals = store_data.groupby('Dept')['Weekly_Sales'].sum()\n        store_total = store_data['Weekly_Sales'].sum()\n        \n        if store_total == 0:\n            num_depts = len(dept_totals)\n            return {dept: 1.0/num_depts for dept in dept_totals.index}\n        \n        dept_proportions = (dept_totals / store_total).to_dict()\n        return dept_proportions","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-18T09:20:45.138002Z","iopub.execute_input":"2025-07-18T09:20:45.138314Z","iopub.status.idle":"2025-07-18T09:20:45.148543Z","shell.execute_reply.started":"2025-07-18T09:20:45.138290Z","shell.execute_reply":"2025-07-18T09:20:45.147705Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"from sklearn.pipeline import Pipeline\n\npipeline = Pipeline([\n    ('merge', BaseMerger(features, stores)),\n    ('feature_add', FeatureAdder()),\n    ('value_fill', MissingValueFiller()),\n    ('cat_encoder', CategoricalEncoder()),\n    ('store_aggregator', StoreAggregator())\n])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-18T09:20:48.081522Z","iopub.execute_input":"2025-07-18T09:20:48.081872Z","iopub.status.idle":"2025-07-18T09:20:48.105279Z","shell.execute_reply.started":"2025-07-18T09:20:48.081846Z","shell.execute_reply":"2025-07-18T09:20:48.104533Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"train_dict = pipeline.fit_transform(train)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-18T09:22:56.759712Z","iopub.execute_input":"2025-07-18T09:22:56.760240Z","iopub.status.idle":"2025-07-18T09:23:06.690007Z","shell.execute_reply.started":"2025-07-18T09:22:56.760208Z","shell.execute_reply":"2025-07-18T09:23:06.689374Z"}},"outputs":[],"execution_count":21},{"cell_type":"code","source":"from darts import TimeSeries\nfrom darts.models.forecasting.arima import ARIMA\n\ndef train_store_models(p, d, q, train_weeks):\n    mape_array = []\n    store_models = {}\n    \n    for store in train_dict.keys():\n        #extract store data\n        df, prop = train_dict[store]\n    \n        #Make timeseries object\n        full_ts = TimeSeries.from_dataframe(df,time_col='Date',value_cols=['Weekly_Sales'])\n        train_ts = full_ts[:train_weeks]\n        val_ts = full_ts[train_weeks:]\n    \n        #Train and validate model\n        model = ARIMA(p=p,d=d,q=q)\n        model.fit(train_ts)\n        predictions = model.predict(len(val_ts))\n        \n        # Calculate MAPE\n        actual_values = val_ts.values().flatten()\n        pred_values = predictions.values().flatten()\n        mask = actual_values != 0\n        mape = np.mean(np.abs((actual_values[mask] - pred_values[mask]) / actual_values[mask])) * 100\n        mape_array.append(mape)\n\n        #Save store model\n        store_models[store] = (model, prop)\n        \n    return (mape_array, store_models)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-18T11:27:16.456615Z","iopub.execute_input":"2025-07-18T11:27:16.457021Z","iopub.status.idle":"2025-07-18T11:27:16.477565Z","shell.execute_reply.started":"2025-07-18T11:27:16.456995Z","shell.execute_reply":"2025-07-18T11:27:16.476477Z"}},"outputs":[],"execution_count":74},{"cell_type":"code","source":"import dagshub\nimport mlflow\nimport pickle\nimport numpy as np\nfrom datetime import datetime\n\n# Set experiment name\nmlflow.set_experiment(\"ARIMA_Parameter_Optimization\")\n\n# Start MLflow run for the entire parameter search\nwith mlflow.start_run(run_name=f\"ARIMA_Grid_Search_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"):\n    \n    # Log experiment configuration\n    mlflow.log_param(\"model_type\", \"ARIMA\")\n    mlflow.log_param(\"validation_weeks\", 123)\n    mlflow.log_param(\"total_param_combinations\", len(param_list))\n    mlflow.log_param(\"param_list\", str(param_list))\n    \n    from tqdm import tqdm\n    param_list = [\n        [1,1,0],\n        [0,1,1], \n        [1,1,1],\n        [2,1,1],\n        [1,1,2],\n        [2,1,2],\n        [0,1,2],\n        [3,1,1],\n        [1,1,3],\n        [4,1,1]\n    ]\n    \n    max_mape = 100\n    best_models = {}\n    best_params = []\n    \n    for p in tqdm(param_list):\n        # Create nested run for each parameter combination\n        with mlflow.start_run(run_name=f\"ARIMA({p[0]},{p[1]},{p[2]})\", nested=True):\n            \n            # Log individual parameters\n            mlflow.log_param(\"p\", p[0])\n            mlflow.log_param(\"d\", p[1]) \n            mlflow.log_param(\"q\", p[2])\n            \n            # Train models\n            mape_array, store_models = train_store_models(p[0], p[1], p[2], 123)\n            mean_mape = sum(mape_array)/len(mape_array)\n            \n            # Log metrics\n            mlflow.log_metric(\"mean_mape\", mean_mape)\n            mlflow.log_metric(\"mape_std\", np.std(mape_array))\n            mlflow.log_metric(\"num_successful_stores\", len(store_models))\n            \n            print(f'Storewise mean mape: {mean_mape} for params{p[0], p[1], p[2]}')\n            \n            # Check if this is the best model\n            if mean_mape < max_mape:\n                best_models = store_models\n                max_mape = mean_mape\n                best_params = [p[0], p[1], p[2]]\n                \n                # Log as best model so far\n                mlflow.log_metric(\"is_best_model\", 1)\n            else:\n                mlflow.log_metric(\"is_best_model\", 0)\n    \n    # Log best model results\n    mlflow.log_param(\"best_p\", best_params[0])\n    mlflow.log_param(\"best_d\", best_params[1])\n    mlflow.log_param(\"best_q\", best_params[2])\n    mlflow.log_metric(\"best_mean_mape\", max_mape)\n    \n    # Save final best model\n    with open(\"final_best_arima_models.pkl\", \"wb\") as f:\n        pickle.dump(best_models, f)\n    mlflow.log_artifact(\"final_best_arima_models.pkl\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"predictions = {}\ntrain_end_date = pd.to_datetime(train['Date'].max())\nfor index, entry in tqdm(test.iterrows()):\n    store = entry['Store']\n    dept = entry['Dept']\n    date = pd.to_datetime(entry['Date'])  # Convert to datetime\n    pred_weeks = (date - train_end_date).days // 7\n\n    model, sale_proportions = best_models[store]\n    prediction = model.predict(pred_weeks).values()[-1, 0]\n    \n    if dept in sale_proportions.keys():\n        prediction = prediction*sale_proportions[dept]\n    else:\n        prediction = prediction/len(sale_proportions)\n        \n    predictions[(store, dept, date)] = prediction","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-18T10:50:07.623103Z","iopub.execute_input":"2025-07-18T10:50:07.623956Z","iopub.status.idle":"2025-07-18T10:55:40.182276Z","shell.execute_reply.started":"2025-07-18T10:50:07.623926Z","shell.execute_reply":"2025-07-18T10:55:40.181319Z"}},"outputs":[{"name":"stderr","text":"115064it [05:32, 346.04it/s]\n","output_type":"stream"}],"execution_count":65},{"cell_type":"code","source":"submission_df = pd.DataFrame([\n    {\n        'Id': f\"{store}_{dept}_{date.strftime('%Y-%m-%d')}\",\n        'Weekly_Sales': weekly_sales\n    }\n    for (store, dept, date), weekly_sales in predictions.items()\n])\n\nsubmission_df.to_csv('submission_arima.csv', index=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-18T10:56:56.847766Z","iopub.execute_input":"2025-07-18T10:56:56.848518Z","iopub.status.idle":"2025-07-18T10:56:57.759451Z","shell.execute_reply.started":"2025-07-18T10:56:56.848490Z","shell.execute_reply":"2025-07-18T10:56:57.758449Z"}},"outputs":[],"execution_count":66}]}
