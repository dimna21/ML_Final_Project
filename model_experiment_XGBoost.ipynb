{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNYAmq5tJ+B75ibMS5aG39k",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "2967a2c81f514cf0b6b5069e64e421a0": {
          "model_module": "@jupyter-widgets/output",
          "model_name": "OutputModel",
          "model_module_version": "1.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/output",
            "_model_module_version": "1.0.0",
            "_model_name": "OutputModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/output",
            "_view_module_version": "1.0.0",
            "_view_name": "OutputView",
            "layout": "IPY_MODEL_e5ca99aed7794c179b9f23f980d06cf9",
            "msg_id": "",
            "outputs": [
              {
                "output_type": "display_data",
                "data": {
                  "text/plain": "\u001b[32m⠼\u001b[0m Waiting for authorization\n",
                  "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000\">⠼</span> Waiting for authorization\n</pre>\n"
                },
                "metadata": {}
              }
            ]
          }
        },
        "e5ca99aed7794c179b9f23f980d06cf9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dimna21/ML_Final_Project/blob/main/model_experiment_XGBoost.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f5eac2FayicZ",
        "outputId": "bc318c15-eafe-46cb-b697-50f97135baed"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install darts"
      ],
      "metadata": {
        "collapsed": true,
        "id": "7akDmJVtypKp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocessing 1"
      ],
      "metadata": {
        "id": "9N6d1PxAALkl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load your data\n",
        "import pandas as pd\n",
        "\n",
        "features = pd.read_csv('/content/drive/MyDrive/ML_Final_Project/features.csv')\n",
        "stores = pd.read_csv('/content/drive/MyDrive/ML_Final_Project/stores.csv')\n",
        "train = pd.read_csv('/content/drive/MyDrive/ML_Final_Project/train.csv')\n",
        "test = pd.read_csv('/content/drive/MyDrive/ML_Final_Project/test.csv')"
      ],
      "metadata": {
        "id": "t5sI3gt5yp1Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "\n",
        "class BaseMerger(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self, features, stores):\n",
        "        self.feature_store = features.merge(stores, how='inner', on='Store')\n",
        "        self.feature_store['Date'] = pd.to_datetime(self.feature_store['Date'])\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        X = X.copy()\n",
        "        X['Date'] = pd.to_datetime(X['Date'])\n",
        "        merged = X.merge(self.feature_store, how='inner', on=['Store', 'Date', 'IsHoliday'])\n",
        "        merged = merged.sort_values(by=['Store', 'Dept', 'Date']).reset_index(drop=True)\n",
        "        return merged"
      ],
      "metadata": {
        "id": "n7-PZhw4yu8z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FeatureAdder(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self):\n",
        "        self.superbowl = pd.to_datetime(['2010-02-12', '2011-02-11', '2012-02-10', '2013-02-08'])\n",
        "        self.labor_day = pd.to_datetime(['2010-09-10', '2011-09-09', '2012-09-07', '2013-09-06'])\n",
        "        self.thanksgiving = pd.to_datetime(['2010-11-26', '2011-11-25', '2012-11-23', '2013-11-29'])\n",
        "        self.christmas = pd.to_datetime(['2010-12-31', '2011-12-30', '2012-12-28', '2013-12-27'])\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        X = X.copy()\n",
        "\n",
        "        # Convert temperature to Celsius\n",
        "        if 'Temperature' in X.columns:\n",
        "            X['Temperature'] = (X['Temperature'] - 32) * (5.0 / 9.0)\n",
        "\n",
        "        # Basic date parts\n",
        "        X['Day'] = X['Date'].dt.day\n",
        "        X['Month'] = X['Date'].dt.month\n",
        "        X['Year'] = X['Date'].dt.year\n",
        "\n",
        "        # Extract ISO week and year for holiday matching\n",
        "        X['Week'] = X['Date'].dt.isocalendar().week\n",
        "        X['YearNum'] = X['Date'].dt.year\n",
        "\n",
        "        # Helper to flag if a date is in same ISO week/year as a known holiday\n",
        "        def is_holiday_week(date_series, holidays):\n",
        "            holiday_weeks = set((d.isocalendar().week, d.year) for d in holidays)\n",
        "            return date_series.apply(lambda d: (d.isocalendar().week, d.year) in holiday_weeks if pd.notnull(d) else False).astype(int)\n",
        "\n",
        "        X['SuperbowlWeek'] = is_holiday_week(X['Date'], self.superbowl)\n",
        "        X['LaborDayWeek'] = is_holiday_week(X['Date'], self.labor_day)\n",
        "        X['ThanksgivingWeek'] = is_holiday_week(X['Date'], self.thanksgiving)\n",
        "        X['ChristmasWeek'] = is_holiday_week(X['Date'], self.christmas)\n",
        "\n",
        "        # Calculate days to Thanksgiving and Christmas (using Nov 24 and Dec 24 as anchor dates)\n",
        "        thanksgiving_dates = pd.to_datetime(X['Year'].astype(str) + \"-11-24\")\n",
        "        christmas_dates = pd.to_datetime(X['Year'].astype(str) + \"-12-24\")\n",
        "\n",
        "        X['Days_to_Thanksgiving'] = (thanksgiving_dates - X['Date']).dt.days\n",
        "        X['Days_to_Christmas'] = (christmas_dates - X['Date']).dt.days\n",
        "\n",
        "        # Clean up helper cols\n",
        "        X = X.drop(columns=['Week', 'YearNum'])\n",
        "\n",
        "        return X"
      ],
      "metadata": {
        "id": "cjGhYvsPy3Je"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MissingValueFiller(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self):\n",
        "        self.markdown_cols = ['MarkDown1', 'MarkDown2', 'MarkDown3', 'MarkDown4', 'MarkDown5']\n",
        "        self.mean_cols = ['CPI', 'Unemployment']\n",
        "        self.mean_values = {}\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        for col in self.mean_cols:\n",
        "            if col in X.columns:\n",
        "                self.mean_values[col] = X[col].mean()\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        X = X.copy()\n",
        "\n",
        "        # Fill markdowns with 0\n",
        "        for col in self.markdown_cols:\n",
        "            if col in X.columns:\n",
        "                X[col] = X[col].fillna(0.0)\n",
        "\n",
        "        # Fill CPI and Unemployment with learned mean\n",
        "        for col in self.mean_cols:\n",
        "            if col in X.columns and col in self.mean_values:\n",
        "                X[col] = X[col].fillna(self.mean_values[col])\n",
        "\n",
        "        return X"
      ],
      "metadata": {
        "id": "UdjO1zkNy5rF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CategoricalEncoder(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self):\n",
        "        self.type_mapping = {'A': 3, 'B': 2, 'C': 1}\n",
        "        self.holiday_mapping = {False: 0, True: 1}\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        X = X.copy()\n",
        "\n",
        "        if 'Type' in X.columns:\n",
        "            X['Type'] = X['Type'].map(self.type_mapping)\n",
        "\n",
        "        if 'IsHoliday' in X.columns:\n",
        "            X['IsHoliday'] = X['IsHoliday'].map(self.holiday_mapping)\n",
        "\n",
        "        return X"
      ],
      "metadata": {
        "id": "M0QyyvT8y-Tj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocessing 2 - Deal with date features"
      ],
      "metadata": {
        "id": "li3nB_cgAP--"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "class LagFeatureTransformer(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self,\n",
        "                 lags=[1, 2, 3, 4],\n",
        "                 rolling_windows=[4, 8],\n",
        "                 drop_na=True):\n",
        "        self.lags = lags\n",
        "        self.rolling_windows = rolling_windows\n",
        "        self.drop_na = drop_na\n",
        "        self.history_ = None\n",
        "        self.lag_values_ = {}\n",
        "        self.rolling_values_ = {}\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        df = X.copy()\n",
        "        df['Date'] = pd.to_datetime(df['Date'])\n",
        "\n",
        "        # Sort by Store, Dept, Date\n",
        "        df = df.sort_values(['Store', 'Dept', 'Date'])\n",
        "\n",
        "        # Store the last few values for each Store-Dept combination\n",
        "        # This will be used to compute lags for test data\n",
        "        max_lag = max(self.lags)\n",
        "        max_window = max(self.rolling_windows) if self.rolling_windows else 0\n",
        "        history_length = max(max_lag, max_window)\n",
        "\n",
        "        self.history_ = (\n",
        "            df[['Store', 'Dept', 'Date', 'Weekly_Sales']]\n",
        "            .groupby(['Store', 'Dept'], as_index=False)\n",
        "            .tail(history_length)\n",
        "        )\n",
        "\n",
        "        # Pre-compute lag and rolling features for the last rows\n",
        "        # This will help with test data transformation\n",
        "        self.lag_values_ = {}\n",
        "        self.rolling_values_ = {}\n",
        "\n",
        "        for (store, dept), group in df.groupby(['Store', 'Dept']):\n",
        "            group = group.sort_values('Date')\n",
        "\n",
        "            # Store last lag values\n",
        "            self.lag_values_[(store, dept)] = {}\n",
        "            for lag in self.lags:\n",
        "                if len(group) >= lag:\n",
        "                    self.lag_values_[(store, dept)][lag] = group['Weekly_Sales'].iloc[-lag]\n",
        "                else:\n",
        "                    self.lag_values_[(store, dept)][lag] = np.nan\n",
        "\n",
        "            # Store last rolling values\n",
        "            self.rolling_values_[(store, dept)] = {}\n",
        "            for window in self.rolling_windows:\n",
        "                if len(group) >= window:\n",
        "                    self.rolling_values_[(store, dept)][window] = group['Weekly_Sales'].iloc[-window:].mean()\n",
        "                else:\n",
        "                    self.rolling_values_[(store, dept)][window] = np.nan\n",
        "\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        df = X.copy()\n",
        "        df['Date'] = pd.to_datetime(df['Date'])\n",
        "        has_sales = 'Weekly_Sales' in df.columns\n",
        "\n",
        "        # Create DateOrdinal\n",
        "        df['DateOrdinal'] = df['Date'].map(pd.Timestamp.toordinal)\n",
        "        df = df.sort_values(['Store', 'Dept', 'Date'])\n",
        "\n",
        "        if has_sales:\n",
        "            # Training data - compute lags normally\n",
        "            for lag in self.lags:\n",
        "                df[f'lag_{lag}'] = df.groupby(['Store', 'Dept'])['Weekly_Sales'].shift(lag)\n",
        "\n",
        "            # Compute rolling means\n",
        "            for window in self.rolling_windows:\n",
        "                df[f'rolling_mean_{window}'] = (\n",
        "                    df.groupby(['Store', 'Dept'])['Weekly_Sales']\n",
        "                    .transform(lambda s: s.rolling(window).mean())\n",
        "                )\n",
        "        else:\n",
        "            # Test data - use pre-computed values from training\n",
        "            # Initialize lag columns\n",
        "            for lag in self.lags:\n",
        "                df[f'lag_{lag}'] = np.nan\n",
        "            for window in self.rolling_windows:\n",
        "                df[f'rolling_mean_{window}'] = np.nan\n",
        "\n",
        "            # Fill with pre-computed values\n",
        "            for idx, row in df.iterrows():\n",
        "                store_dept = (row['Store'], row['Dept'])\n",
        "\n",
        "                if store_dept in self.lag_values_:\n",
        "                    for lag in self.lags:\n",
        "                        if lag in self.lag_values_[store_dept]:\n",
        "                            df.loc[idx, f'lag_{lag}'] = self.lag_values_[store_dept][lag]\n",
        "\n",
        "                if store_dept in self.rolling_values_:\n",
        "                    for window in self.rolling_windows:\n",
        "                        if window in self.rolling_values_[store_dept]:\n",
        "                            df.loc[idx, f'rolling_mean_{window}'] = self.rolling_values_[store_dept][window]\n",
        "\n",
        "        # Drop helper columns\n",
        "        drop_cols = [c for c in ['Day', 'Year', 'Date'] if c in df.columns]\n",
        "        df = df.drop(columns=drop_cols)\n",
        "\n",
        "        # Handle NaN values\n",
        "        if self.drop_na and has_sales:\n",
        "            # Only drop NaN for training data\n",
        "            required = [f'lag_{l}' for l in self.lags] + [f'rolling_mean_{w}' for w in self.rolling_windows]\n",
        "            df = df.dropna(subset=required).reset_index(drop=True)\n",
        "        elif not has_sales:\n",
        "            # For test data, fill remaining NaN values with appropriate defaults\n",
        "            # You might want to adjust these defaults based on your domain knowledge\n",
        "            for lag in self.lags:\n",
        "                df[f'lag_{lag}'] = df[f'lag_{lag}'].fillna(0)  # or use median/mean from training\n",
        "            for window in self.rolling_windows:\n",
        "                df[f'rolling_mean_{window}'] = df[f'rolling_mean_{window}'].fillna(0)  # or use median/mean from training\n",
        "\n",
        "        return df"
      ],
      "metadata": {
        "id": "vXYmPxsh1C26"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocessing 3 - Correlation filter"
      ],
      "metadata": {
        "id": "nXVyy3jBQOCC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "from sklearn.feature_selection import SelectKBest, f_regression\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "class SmartCorrelationDropper(BaseEstimator, TransformerMixin):\n",
        "    \"\"\"\n",
        "    More conservative correlation filter that protects important feature groups\n",
        "    \"\"\"\n",
        "    def __init__(self, threshold=0.95, protect_groups=None, verbose=False):\n",
        "        self.threshold = threshold  # Higher threshold\n",
        "        self.protect_groups = protect_groups or [\n",
        "            ['lag_1', 'lag_2', 'lag_3', 'lag_4'],  # Protect lag features\n",
        "            ['rolling_mean_4', 'rolling_mean_8'],   # Protect rolling features\n",
        "            ['SuperbowlWeek', 'LaborDayWeek', 'ThanksgivingWeek', 'ChristmasWeek'],  # Protect holiday features\n",
        "            ['Days_to_Thanksgiving', 'Days_to_Christmas'],  # Protect seasonal features\n",
        "        ]\n",
        "        self.verbose = verbose\n",
        "        self.features_to_drop_ = []\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        if self.verbose:\n",
        "            print(\"----- Smart Correlation Filter Fitting -----\")\n",
        "\n",
        "        # Get target if it's in X\n",
        "        if y is None and 'Weekly_Sales' in X.columns:\n",
        "            y = X['Weekly_Sales']\n",
        "            X_numeric = X.select_dtypes(include=[np.number]).drop(columns='Weekly_Sales')\n",
        "        else:\n",
        "            X_numeric = X.select_dtypes(include=[np.number]).copy()\n",
        "\n",
        "        # Create protected features set\n",
        "        protected_features = set()\n",
        "        for group in self.protect_groups:\n",
        "            for feature in group:\n",
        "                if feature in X_numeric.columns:\n",
        "                    protected_features.add(feature)\n",
        "\n",
        "        # Calculate correlation matrix\n",
        "        corr_matrix = X_numeric.corr().abs()\n",
        "        mask = np.triu(np.ones_like(corr_matrix, dtype=bool))\n",
        "        corr_matrix = corr_matrix.where(~mask)\n",
        "\n",
        "        # Find highly correlated pairs\n",
        "        high_corr_pairs = []\n",
        "        cols = corr_matrix.columns\n",
        "        for i in range(len(cols)):\n",
        "            for j in range(i):\n",
        "                val = corr_matrix.iloc[i, j]\n",
        "                if pd.notnull(val) and val > self.threshold:\n",
        "                    high_corr_pairs.append((cols[i], cols[j], val))\n",
        "\n",
        "        # Decide which features to drop\n",
        "        features_to_drop = set()\n",
        "        for feat1, feat2, corr_val in high_corr_pairs:\n",
        "            # Don't drop if both features are protected\n",
        "            if feat1 in protected_features and feat2 in protected_features:\n",
        "                continue\n",
        "\n",
        "            # If one is protected, drop the other\n",
        "            if feat1 in protected_features:\n",
        "                features_to_drop.add(feat2)\n",
        "            elif feat2 in protected_features:\n",
        "                features_to_drop.add(feat1)\n",
        "            else:\n",
        "                # Neither is protected, use target correlation if available\n",
        "                if y is not None:\n",
        "                    corr1 = abs(X_numeric[feat1].corr(y)) if feat1 in X_numeric.columns else 0\n",
        "                    corr2 = abs(X_numeric[feat2].corr(y)) if feat2 in X_numeric.columns else 0\n",
        "                    to_drop = feat1 if corr1 < corr2 else feat2\n",
        "                else:\n",
        "                    to_drop = feat2\n",
        "                features_to_drop.add(to_drop)\n",
        "\n",
        "        self.features_to_drop_ = list(features_to_drop)\n",
        "\n",
        "        if self.verbose:\n",
        "            print(f\"Highly correlated pairs (>{self.threshold}): {len(high_corr_pairs)}\")\n",
        "            print(f\"Protected features: {protected_features}\")\n",
        "            print(f\"Features to drop: {self.features_to_drop_}\")\n",
        "\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        return X.drop(columns=self.features_to_drop_, errors='ignore')\n",
        "\n",
        "\n",
        "class ImportanceBasedSelector(BaseEstimator, TransformerMixin):\n",
        "    \"\"\"\n",
        "    Feature selection based on feature importance, keeping top K features\n",
        "    \"\"\"\n",
        "    def __init__(self, k_features=20, estimator=None, verbose=False):\n",
        "        self.k_features = k_features\n",
        "        self.estimator = estimator or RandomForestRegressor(\n",
        "            n_estimators=100,\n",
        "            max_depth=10,\n",
        "            random_state=42,\n",
        "            n_jobs=-1\n",
        "        )\n",
        "        self.verbose = verbose\n",
        "        self.selected_features_ = []\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        # Get target if it's in X\n",
        "        if y is None and 'Weekly_Sales' in X.columns:\n",
        "            y = X['Weekly_Sales']\n",
        "            X_features = X.drop(columns=['Weekly_Sales'])\n",
        "        else:\n",
        "            X_features = X.copy()\n",
        "\n",
        "        # Fit estimator and get feature importances\n",
        "        self.estimator.fit(X_features, y)\n",
        "\n",
        "        # Get feature importances\n",
        "        if hasattr(self.estimator, 'feature_importances_'):\n",
        "            importances = self.estimator.feature_importances_\n",
        "        else:\n",
        "            # Fallback for estimators without feature_importances_\n",
        "            importances = np.abs(self.estimator.coef_) if hasattr(self.estimator, 'coef_') else np.ones(len(X_features.columns))\n",
        "\n",
        "        # Create feature importance dataframe\n",
        "        feature_importance = pd.DataFrame({\n",
        "            'feature': X_features.columns,\n",
        "            'importance': importances\n",
        "        }).sort_values('importance', ascending=False)\n",
        "\n",
        "        # Select top k features\n",
        "        self.selected_features_ = feature_importance.head(self.k_features)['feature'].tolist()\n",
        "\n",
        "        if self.verbose:\n",
        "            print(f\"Top {self.k_features} features by importance:\")\n",
        "            print(feature_importance.head(self.k_features))\n",
        "\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        # Preserve target if it exists\n",
        "        result = X[self.selected_features_].copy()\n",
        "        if 'Weekly_Sales' in X.columns:\n",
        "            result['Weekly_Sales'] = X['Weekly_Sales']\n",
        "        return result\n",
        "\n",
        "\n",
        "class MinimalFeatureSelector(BaseEstimator, TransformerMixin):\n",
        "    \"\"\"\n",
        "    Very conservative feature selection that only removes clearly redundant features\n",
        "    \"\"\"\n",
        "    def __init__(self, variance_threshold=0.01, verbose=False):\n",
        "        self.variance_threshold = variance_threshold\n",
        "        self.verbose = verbose\n",
        "        self.features_to_drop_ = []\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        # Get numeric features\n",
        "        if 'Weekly_Sales' in X.columns:\n",
        "            X_numeric = X.select_dtypes(include=[np.number]).drop(columns='Weekly_Sales')\n",
        "        else:\n",
        "            X_numeric = X.select_dtypes(include=[np.number])\n",
        "\n",
        "        # Find features with very low variance (almost constant)\n",
        "        low_variance_features = []\n",
        "        for col in X_numeric.columns:\n",
        "            if X_numeric[col].var() < self.variance_threshold:\n",
        "                low_variance_features.append(col)\n",
        "\n",
        "        self.features_to_drop_ = low_variance_features\n",
        "\n",
        "        if self.verbose:\n",
        "            print(f\"Features with low variance (< {self.variance_threshold}): {self.features_to_drop_}\")\n",
        "\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        return X.drop(columns=self.features_to_drop_, errors='ignore')\n",
        "\n",
        "\n",
        "# Example usage in your pipeline:\n",
        "def create_feature_selection_pipeline(approach='minimal'):\n",
        "    \"\"\"\n",
        "    Create feature selection pipeline based on approach\n",
        "\n",
        "    Args:\n",
        "        approach: 'none', 'minimal', 'conservative', or 'importance'\n",
        "    \"\"\"\n",
        "    if approach == 'none':\n",
        "        return []\n",
        "    elif approach == 'minimal':\n",
        "        return [('feature_select', MinimalFeatureSelector(verbose=True))]\n",
        "    elif approach == 'conservative':\n",
        "        return [\n",
        "            ('correlation_filter', SmartCorrelationDropper(threshold=0.95, verbose=True)),\n",
        "            ('minimal_select', MinimalFeatureSelector(verbose=True))\n",
        "        ]\n",
        "    elif approach == 'importance':\n",
        "        return [\n",
        "            ('correlation_filter', SmartCorrelationDropper(threshold=0.95, verbose=True)),\n",
        "            ('importance_select', ImportanceBasedSelector(k_features=20, verbose=True))\n",
        "        ]\n",
        "    else:\n",
        "        raise ValueError(\"approach must be 'none', 'minimal', 'conservative', or 'importance'\")\n",
        "\n",
        "\n",
        "# Modified pipeline example:\n",
        "\"\"\"\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "# Option 1: No feature selection (recommended)\n",
        "pipeline = Pipeline([\n",
        "    ('merge', BaseMerger(features, stores)),\n",
        "    ('feature_add', FeatureAdder()),\n",
        "    ('fillna', MissingValueFiller()),\n",
        "    ('label_encode', CategoricalEncoder()),\n",
        "    ('lags', LagFeatureTransformer(lags=[1,2,3,4], rolling_windows=[4,8], drop_na=True)),\n",
        "])\n",
        "\n",
        "# Option 2: Conservative feature selection\n",
        "pipeline = Pipeline([\n",
        "    ('merge', BaseMerger(features, stores)),\n",
        "    ('feature_add', FeatureAdder()),\n",
        "    ('fillna', MissingValueFiller()),\n",
        "    ('label_encode', CategoricalEncoder()),\n",
        "    ('lags', LagFeatureTransformer(lags=[1,2,3,4], rolling_windows=[4,8], drop_na=True)),\n",
        "    ('feature_select', SmartCorrelationDropper(threshold=0.95, verbose=True)),\n",
        "])\n",
        "\n",
        "# Option 3: Importance-based selection\n",
        "pipeline = Pipeline([\n",
        "    ('merge', BaseMerger(features, stores)),\n",
        "    ('feature_add', FeatureAdder()),\n",
        "    ('fillna', MissingValueFiller()),\n",
        "    ('label_encode', CategoricalEncoder()),\n",
        "    ('lags', LagFeatureTransformer(lags=[1,2,3,4], rolling_windows=[4,8], drop_na=True)),\n",
        "    ('importance_select', ImportanceBasedSelector(k_features=20, verbose=True)),\n",
        "])\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "0zS3WctBQS14",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "outputId": "f44ef922-bcca-41ab-a89c-ff1e5bf54392"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\nfrom sklearn.pipeline import Pipeline\\n\\n# Option 1: No feature selection (recommended)\\npipeline = Pipeline([\\n    ('merge', BaseMerger(features, stores)),\\n    ('feature_add', FeatureAdder()),\\n    ('fillna', MissingValueFiller()),\\n    ('label_encode', CategoricalEncoder()),\\n    ('lags', LagFeatureTransformer(lags=[1,2,3,4], rolling_windows=[4,8], drop_na=True)),\\n])\\n\\n# Option 2: Conservative feature selection\\npipeline = Pipeline([\\n    ('merge', BaseMerger(features, stores)),\\n    ('feature_add', FeatureAdder()),\\n    ('fillna', MissingValueFiller()),\\n    ('label_encode', CategoricalEncoder()),\\n    ('lags', LagFeatureTransformer(lags=[1,2,3,4], rolling_windows=[4,8], drop_na=True)),\\n    ('feature_select', SmartCorrelationDropper(threshold=0.95, verbose=True)),\\n])\\n\\n# Option 3: Importance-based selection\\npipeline = Pipeline([\\n    ('merge', BaseMerger(features, stores)),\\n    ('feature_add', FeatureAdder()),\\n    ('fillna', MissingValueFiller()),\\n    ('label_encode', CategoricalEncoder()),\\n    ('lags', LagFeatureTransformer(lags=[1,2,3,4], rolling_windows=[4,8], drop_na=True)),\\n    ('importance_select', ImportanceBasedSelector(k_features=20, verbose=True)),\\n])\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 220
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y = train_df['Weekly_Sales']\n",
        "X = train_df.drop(columns=['Weekly_Sales'])\n",
        "X.shape, y.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hPLZBBhGkdqi",
        "outputId": "c13e9f88-34dd-4c5b-e8ce-906d007fb23a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((398796, 28), (398796,))"
            ]
          },
          "metadata": {},
          "execution_count": 221
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocessing 4 - Seasonal Feature Engineering"
      ],
      "metadata": {
        "id": "EUaL8oP66-wz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class AdvancedSeasonalFeatures(BaseEstimator, TransformerMixin):\n",
        "    \"\"\"\n",
        "    Creates sophisticated seasonal and cyclical features\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        # Define holiday periods more precisely\n",
        "        self.holiday_periods = {\n",
        "            'thanksgiving_period': [\n",
        "                ('2010-11-19', '2010-11-26'),\n",
        "                ('2011-11-18', '2011-11-25'),\n",
        "                ('2012-11-16', '2012-11-23'),\n",
        "                ('2013-11-22', '2013-11-29')\n",
        "            ],\n",
        "            'christmas_period': [\n",
        "                ('2010-12-17', '2010-12-31'),\n",
        "                ('2011-12-16', '2011-12-30'),\n",
        "                ('2012-12-21', '2012-12-28'),\n",
        "                ('2013-12-20', '2013-12-27')\n",
        "            ],\n",
        "            'superbowl_period': [\n",
        "                ('2010-02-05', '2010-02-12'),\n",
        "                ('2011-02-04', '2011-02-11'),\n",
        "                ('2012-02-03', '2012-02-10'),\n",
        "                ('2013-02-01', '2013-02-08')\n",
        "            ],\n",
        "            'labor_day_period': [\n",
        "                ('2010-09-03', '2010-09-10'),\n",
        "                ('2011-09-02', '2011-09-09'),\n",
        "                ('2012-08-31', '2012-09-07'),\n",
        "                ('2013-08-30', '2013-09-06')\n",
        "            ]\n",
        "        }\n",
        "\n",
        "        # Back-to-school period (typically July-August)\n",
        "        self.back_to_school_period = [\n",
        "            ('2010-07-15', '2010-08-31'),\n",
        "            ('2011-07-15', '2011-08-31'),\n",
        "            ('2012-07-15', '2012-08-31'),\n",
        "            ('2013-07-15', '2013-08-31')\n",
        "        ]\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        df = X.copy()\n",
        "        df['Date'] = pd.to_datetime(df['Date'])\n",
        "\n",
        "        # Cyclical encoding of time features\n",
        "        df['month_sin'] = np.sin(2 * np.pi * df['Month'] / 12)\n",
        "        df['month_cos'] = np.cos(2 * np.pi * df['Month'] / 12)\n",
        "\n",
        "        # Week of year for seasonal patterns\n",
        "        df['week_of_year'] = df['Date'].dt.isocalendar().week\n",
        "        df['week_sin'] = np.sin(2 * np.pi * df['week_of_year'] / 52)\n",
        "        df['week_cos'] = np.cos(2 * np.pi * df['week_of_year'] / 52)\n",
        "\n",
        "        # Quarter features\n",
        "        df['quarter'] = df['Date'].dt.quarter\n",
        "        df['quarter_sin'] = np.sin(2 * np.pi * df['quarter'] / 4)\n",
        "        df['quarter_cos'] = np.cos(2 * np.pi * df['quarter'] / 4)\n",
        "\n",
        "        # Advanced holiday features\n",
        "        for holiday_name, periods in self.holiday_periods.items():\n",
        "            df[f'{holiday_name}_flag'] = 0\n",
        "            for start, end in periods:\n",
        "                mask = (df['Date'] >= start) & (df['Date'] <= end)\n",
        "                df.loc[mask, f'{holiday_name}_flag'] = 1\n",
        "\n",
        "        # Back-to-school period\n",
        "        df['back_to_school_flag'] = 0\n",
        "        for start, end in self.back_to_school_period:\n",
        "            mask = (df['Date'] >= start) & (df['Date'] <= end)\n",
        "            df.loc[mask, 'back_to_school_flag'] = 1\n",
        "\n",
        "        # Days since major holidays (continuous features)\n",
        "        for year in [2010, 2011, 2012, 2013]:\n",
        "            thanksgiving = pd.to_datetime(f'{year}-11-24')  # Approximate\n",
        "            christmas = pd.to_datetime(f'{year}-12-25')\n",
        "\n",
        "            year_mask = df['Date'].dt.year == year\n",
        "            if year_mask.any():\n",
        "                df.loc[year_mask, 'days_since_thanksgiving'] = (df.loc[year_mask, 'Date'] - thanksgiving).dt.days\n",
        "                df.loc[year_mask, 'days_since_christmas'] = (df.loc[year_mask, 'Date'] - christmas).dt.days\n",
        "\n",
        "        # Seasonal shopping intensity (pre-holiday buildup)\n",
        "        df['pre_thanksgiving_intensity'] = np.where(\n",
        "            (df['Days_to_Thanksgiving'] <= 14) & (df['Days_to_Thanksgiving'] > 0),\n",
        "            15 - df['Days_to_Thanksgiving'], 0\n",
        "        )\n",
        "\n",
        "        df['pre_christmas_intensity'] = np.where(\n",
        "            (df['Days_to_Christmas'] <= 21) & (df['Days_to_Christmas'] > 0),\n",
        "            22 - df['Days_to_Christmas'], 0\n",
        "        )\n",
        "\n",
        "        # Post-holiday effect (returns, clearance)\n",
        "        df['post_holiday_effect'] = np.where(\n",
        "            ((df['days_since_thanksgiving'] > 0) & (df['days_since_thanksgiving'] <= 7)) |\n",
        "            ((df['days_since_christmas'] > 0) & (df['days_since_christmas'] <= 14)),\n",
        "            1, 0\n",
        "        )\n",
        "\n",
        "        # Clean up intermediate columns\n",
        "        df = df.drop(columns=['week_of_year'], errors='ignore')\n",
        "\n",
        "        return df"
      ],
      "metadata": {
        "id": "fUiikMeH7Fa_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocessing 5 - add lag features"
      ],
      "metadata": {
        "id": "gDxV_KwG7Mo0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ImprovedLagFeatureTransformer(BaseEstimator, TransformerMixin):\n",
        "    \"\"\"\n",
        "    Enhanced lag features with more sophisticated patterns\n",
        "    \"\"\"\n",
        "    def __init__(self,\n",
        "                 lags=[1, 2, 3, 4, 8, 12, 52],  # Include yearly lag\n",
        "                 rolling_windows=[2, 4, 8, 12, 26],  # More diverse windows\n",
        "                 ewm_spans=[4, 8, 12],  # Exponential weighted moving averages\n",
        "                 drop_na=True):\n",
        "        self.lags = lags\n",
        "        self.rolling_windows = rolling_windows\n",
        "        self.ewm_spans = ewm_spans\n",
        "        self.drop_na = drop_na\n",
        "        self.history_ = None\n",
        "        self.lag_stats_ = {}\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        df = X.copy()\n",
        "        df['Date'] = pd.to_datetime(df['Date'])\n",
        "        df = df.sort_values(['Store', 'Dept', 'Date'])\n",
        "\n",
        "        # Store more comprehensive history\n",
        "        max_lag = max(self.lags) if self.lags else 0\n",
        "        max_window = max(self.rolling_windows) if self.rolling_windows else 0\n",
        "        max_ewm = max(self.ewm_spans) if self.ewm_spans else 0\n",
        "        history_length = max(max_lag, max_window, max_ewm, 60)  # At least 60 weeks\n",
        "\n",
        "        self.history_ = (\n",
        "            df[['Store', 'Dept', 'Date', 'Weekly_Sales']]\n",
        "            .groupby(['Store', 'Dept'], as_index=False)\n",
        "            .tail(history_length)\n",
        "        )\n",
        "\n",
        "        # Calculate statistics for each store-dept combination\n",
        "        for (store, dept), group in df.groupby(['Store', 'Dept']):\n",
        "            group = group.sort_values('Date')\n",
        "            sales = group['Weekly_Sales']\n",
        "\n",
        "            self.lag_stats_[(store, dept)] = {\n",
        "                'mean': sales.mean(),\n",
        "                'std': sales.std(),\n",
        "                'median': sales.median(),\n",
        "                'q25': sales.quantile(0.25),\n",
        "                'q75': sales.quantile(0.75),\n",
        "                'trend': self._calculate_trend(sales),\n",
        "                'seasonality': self._calculate_seasonality(sales)\n",
        "            }\n",
        "\n",
        "        return self\n",
        "\n",
        "    def _calculate_trend(self, series):\n",
        "        \"\"\"Calculate simple trend\"\"\"\n",
        "        if len(series) < 4:\n",
        "            return 0\n",
        "        x = np.arange(len(series))\n",
        "        try:\n",
        "            trend = np.polyfit(x, series, 1)[0]\n",
        "            return trend\n",
        "        except:\n",
        "            return 0\n",
        "\n",
        "    def _calculate_seasonality(self, series):\n",
        "        \"\"\"Calculate seasonal strength\"\"\"\n",
        "        if len(series) < 52:\n",
        "            return 0\n",
        "        try:\n",
        "            # Simple seasonal strength measure\n",
        "            return series.std() / series.mean() if series.mean() > 0 else 0\n",
        "        except:\n",
        "            return 0\n",
        "\n",
        "    def transform(self, X):\n",
        "        df = X.copy()\n",
        "        df['Date'] = pd.to_datetime(df['Date'])\n",
        "        has_sales = 'Weekly_Sales' in df.columns\n",
        "\n",
        "        df = df.sort_values(['Store', 'Dept', 'Date'])\n",
        "\n",
        "        if has_sales:\n",
        "            # Training data\n",
        "            for lag in self.lags:\n",
        "                df[f'lag_{lag}'] = df.groupby(['Store', 'Dept'])['Weekly_Sales'].shift(lag)\n",
        "\n",
        "            # Rolling statistics\n",
        "            for window in self.rolling_windows:\n",
        "                df[f'rolling_mean_{window}'] = df.groupby(['Store', 'Dept'])['Weekly_Sales'].transform(\n",
        "                    lambda x: x.rolling(window, min_periods=1).mean()\n",
        "                )\n",
        "                df[f'rolling_std_{window}'] = df.groupby(['Store', 'Dept'])['Weekly_Sales'].transform(\n",
        "                    lambda x: x.rolling(window, min_periods=1).std()\n",
        "                )\n",
        "                df[f'rolling_median_{window}'] = df.groupby(['Store', 'Dept'])['Weekly_Sales'].transform(\n",
        "                    lambda x: x.rolling(window, min_periods=1).median()\n",
        "                )\n",
        "\n",
        "            # Exponential weighted moving averages\n",
        "            for span in self.ewm_spans:\n",
        "                df[f'ewm_{span}'] = df.groupby(['Store', 'Dept'])['Weekly_Sales'].transform(\n",
        "                    lambda x: x.ewm(span=span, min_periods=1).mean()\n",
        "                )\n",
        "\n",
        "            # Sales momentum and acceleration\n",
        "            df['sales_momentum'] = df.groupby(['Store', 'Dept'])['Weekly_Sales'].transform(\n",
        "                lambda x: x.diff()\n",
        "            )\n",
        "            df['sales_acceleration'] = df.groupby(['Store', 'Dept'])['sales_momentum'].transform(\n",
        "                lambda x: x.diff()\n",
        "            )\n",
        "\n",
        "        else:\n",
        "            # Test data - use historical statistics\n",
        "            for lag in self.lags:\n",
        "                df[f'lag_{lag}'] = np.nan\n",
        "\n",
        "            for window in self.rolling_windows:\n",
        "                df[f'rolling_mean_{window}'] = np.nan\n",
        "                df[f'rolling_std_{window}'] = np.nan\n",
        "                df[f'rolling_median_{window}'] = np.nan\n",
        "\n",
        "            for span in self.ewm_spans:\n",
        "                df[f'ewm_{span}'] = np.nan\n",
        "\n",
        "            df['sales_momentum'] = np.nan\n",
        "            df['sales_acceleration'] = np.nan\n",
        "\n",
        "            # Fill with historical statistics\n",
        "            for idx, row in df.iterrows():\n",
        "                store_dept = (row['Store'], row['Dept'])\n",
        "                if store_dept in self.lag_stats_:\n",
        "                    stats = self.lag_stats_[store_dept]\n",
        "                    # Use mean for missing lags\n",
        "                    for lag in self.lags:\n",
        "                        df.loc[idx, f'lag_{lag}'] = stats['mean']\n",
        "\n",
        "                    # Use historical statistics for rolling features\n",
        "                    for window in self.rolling_windows:\n",
        "                        df.loc[idx, f'rolling_mean_{window}'] = stats['mean']\n",
        "                        df.loc[idx, f'rolling_std_{window}'] = stats['std']\n",
        "                        df.loc[idx, f'rolling_median_{window}'] = stats['median']\n",
        "\n",
        "                    for span in self.ewm_spans:\n",
        "                        df.loc[idx, f'ewm_{span}'] = stats['mean']\n",
        "\n",
        "                    df.loc[idx, 'sales_momentum'] = stats['trend']\n",
        "                    df.loc[idx, 'sales_acceleration'] = 0\n",
        "\n",
        "        # Clean up\n",
        "        if self.drop_na and has_sales:\n",
        "            numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
        "            df = df.dropna(subset=numeric_cols).reset_index(drop=True)\n",
        "\n",
        "        return df\n"
      ],
      "metadata": {
        "id": "Keh5tZh47L1u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocess 6 - add specific features"
      ],
      "metadata": {
        "id": "HyLNm53S7tiQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class StoreSpecificFeatures(BaseEstimator, TransformerMixin):\n",
        "    \"\"\"\n",
        "    Store and department specific features\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        self.store_stats_ = {}\n",
        "        self.dept_stats_ = {}\n",
        "        self.store_dept_stats_ = {}\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        df = X.copy()\n",
        "        has_sales = 'Weekly_Sales' in df.columns\n",
        "\n",
        "        if has_sales:\n",
        "            # Store-level statistics\n",
        "            store_groups = df.groupby('Store')['Weekly_Sales']\n",
        "            self.store_stats_ = {\n",
        "                'mean': store_groups.mean().to_dict(),\n",
        "                'std': store_groups.std().to_dict(),\n",
        "                'median': store_groups.median().to_dict(),\n",
        "                'volume': store_groups.count().to_dict()\n",
        "            }\n",
        "\n",
        "            # Department-level statistics\n",
        "            dept_groups = df.groupby('Dept')['Weekly_Sales']\n",
        "            self.dept_stats_ = {\n",
        "                'mean': dept_groups.mean().to_dict(),\n",
        "                'std': dept_groups.std().to_dict(),\n",
        "                'median': dept_groups.median().to_dict(),\n",
        "                'volume': dept_groups.count().to_dict()\n",
        "            }\n",
        "\n",
        "            # Store-Department level statistics\n",
        "            store_dept_groups = df.groupby(['Store', 'Dept'])['Weekly_Sales']\n",
        "            self.store_dept_stats_ = {\n",
        "                'mean': store_dept_groups.mean().to_dict(),\n",
        "                'std': store_dept_groups.std().to_dict(),\n",
        "                'volume': store_dept_groups.count().to_dict()\n",
        "            }\n",
        "\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        df = X.copy()\n",
        "\n",
        "        # Store performance indicators\n",
        "        df['store_avg_sales'] = df['Store'].map(self.store_stats_.get('mean', {})).fillna(0)\n",
        "        df['store_sales_volatility'] = df['Store'].map(self.store_stats_.get('std', {})).fillna(0)\n",
        "        df['store_volume'] = df['Store'].map(self.store_stats_.get('volume', {})).fillna(0)\n",
        "\n",
        "        # Department performance indicators\n",
        "        df['dept_avg_sales'] = df['Dept'].map(self.dept_stats_.get('mean', {})).fillna(0)\n",
        "        df['dept_sales_volatility'] = df['Dept'].map(self.dept_stats_.get('std', {})).fillna(0)\n",
        "        df['dept_volume'] = df['Dept'].map(self.dept_stats_.get('volume', {})).fillna(0)\n",
        "\n",
        "        # Store-Department specific features\n",
        "        df['store_dept_key'] = list(zip(df['Store'], df['Dept']))\n",
        "        df['store_dept_avg_sales'] = df['store_dept_key'].map(self.store_dept_stats_.get('mean', {})).fillna(0)\n",
        "        df['store_dept_volatility'] = df['store_dept_key'].map(self.store_dept_stats_.get('std', {})).fillna(0)\n",
        "\n",
        "        # Performance ratios\n",
        "        df['store_vs_overall_ratio'] = df['store_avg_sales'] / (df['store_avg_sales'].mean() + 1e-8)\n",
        "        df['dept_vs_overall_ratio'] = df['dept_avg_sales'] / (df['dept_avg_sales'].mean() + 1e-8)\n",
        "\n",
        "        # Size-normalized features\n",
        "        df['sales_per_sqft'] = df['store_avg_sales'] / (df['Size'] + 1e-8)\n",
        "        df['dept_penetration'] = df['dept_volume'] / (df['store_volume'] + 1e-8)\n",
        "\n",
        "        # Store type interactions\n",
        "        df['type_size_interaction'] = df['Type'] * np.log1p(df['Size'])\n",
        "\n",
        "        # Clean up\n",
        "        df = df.drop(columns=['store_dept_key'], errors='ignore')\n",
        "\n",
        "        return df\n"
      ],
      "metadata": {
        "id": "lnpgvE_S7tC7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocess 6 - add economic features"
      ],
      "metadata": {
        "id": "7on5Zcsv7x8u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class EconomicInteractionFeatures(BaseEstimator, TransformerMixin):\n",
        "    \"\"\"\n",
        "    Economic indicators and their interactions\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        df = X.copy()\n",
        "\n",
        "        # Temperature-based features\n",
        "        if 'Temperature' in df.columns:\n",
        "            # Comfort zone indicators\n",
        "            df['temp_comfort_zone'] = ((df['Temperature'] >= 15) & (df['Temperature'] <= 25)).astype(int)\n",
        "            df['temp_too_hot'] = (df['Temperature'] > 30).astype(int)\n",
        "            df['temp_too_cold'] = (df['Temperature'] < 5).astype(int)\n",
        "\n",
        "            # Seasonal temperature anomalies\n",
        "            df['temp_month_interaction'] = df['Temperature'] * df['Month']\n",
        "\n",
        "            # Weather-driven shopping patterns\n",
        "            df['weather_shopping_boost'] = np.where(\n",
        "                (df['Temperature'] < 0) | (df['Temperature'] > 35), 1, 0\n",
        "            )\n",
        "\n",
        "        # Fuel price interactions\n",
        "        if 'Fuel_Price' in df.columns:\n",
        "            df['fuel_price_high'] = (df['Fuel_Price'] > df['Fuel_Price'].quantile(0.75)).astype(int)\n",
        "            df['fuel_price_low'] = (df['Fuel_Price'] < df['Fuel_Price'].quantile(0.25)).astype(int)\n",
        "\n",
        "            # Fuel price vs store size (larger stores might be more affected)\n",
        "            df['fuel_size_interaction'] = df['Fuel_Price'] * np.log1p(df['Size'])\n",
        "\n",
        "        # Economic pressure indicators\n",
        "        if 'CPI' in df.columns and 'Unemployment' in df.columns:\n",
        "            df['economic_pressure'] = df['CPI'] * df['Unemployment']\n",
        "            df['economic_stability'] = 1 / (1 + df['economic_pressure'])\n",
        "\n",
        "            # Purchasing power proxy\n",
        "            df['purchasing_power'] = df['CPI'] / (df['Unemployment'] + 1e-8)\n",
        "\n",
        "        # Markdown effectiveness\n",
        "        markdown_cols = ['MarkDown1', 'MarkDown2', 'MarkDown3', 'MarkDown4', 'MarkDown5']\n",
        "        existing_markdowns = [col for col in markdown_cols if col in df.columns]\n",
        "\n",
        "        if existing_markdowns:\n",
        "            df['total_markdown'] = df[existing_markdowns].sum(axis=1)\n",
        "            df['markdown_count'] = (df[existing_markdowns] > 0).sum(axis=1)\n",
        "            df['avg_markdown'] = df['total_markdown'] / (df['markdown_count'] + 1e-8)\n",
        "\n",
        "            # Markdown intensity by store type\n",
        "            df['markdown_type_interaction'] = df['total_markdown'] * df['Type']\n",
        "\n",
        "            # Holiday markdown boost\n",
        "            if 'IsHoliday' in df.columns:\n",
        "                df['holiday_markdown_boost'] = df['IsHoliday'] * df['total_markdown']\n",
        "\n",
        "        return df\n"
      ],
      "metadata": {
        "id": "KcnuiQ1_75_G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preproces 7 - add date features"
      ],
      "metadata": {
        "id": "wDoENwKE79MA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class AdvancedDateFeatures(BaseEstimator, TransformerMixin):\n",
        "    \"\"\"\n",
        "    Advanced date-based features\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        df = X.copy()\n",
        "        df['Date'] = pd.to_datetime(df['Date'])\n",
        "\n",
        "        # Paycheck cycles (bi-weekly and monthly)\n",
        "        df['is_paycheck_week'] = ((df['Date'].dt.day <= 7) |\n",
        "                                  ((df['Date'].dt.day >= 14) & (df['Date'].dt.day <= 21))).astype(int)\n",
        "        df['is_month_end'] = (df['Date'].dt.day >= 25).astype(int)\n",
        "        df['is_month_start'] = (df['Date'].dt.day <= 7).astype(int)\n",
        "\n",
        "        # School calendar effects\n",
        "        df['is_school_week'] = ((df['Date'].dt.month >= 9) |\n",
        "                                (df['Date'].dt.month <= 5)).astype(int)\n",
        "        df['is_summer_break'] = ((df['Date'].dt.month >= 6) &\n",
        "                                 (df['Date'].dt.month <= 8)).astype(int)\n",
        "\n",
        "        # Tax season\n",
        "        df['is_tax_season'] = ((df['Date'].dt.month >= 1) &\n",
        "                               (df['Date'].dt.month <= 4)).astype(int)\n",
        "\n",
        "        # Weekend proximity\n",
        "        df['days_to_weekend'] = 6 - df['Date'].dt.dayofweek\n",
        "        df['days_from_weekend'] = df['Date'].dt.dayofweek\n",
        "\n",
        "        # Seasonal shopping patterns\n",
        "        df['is_spring_shopping'] = ((df['Date'].dt.month >= 3) &\n",
        "                                    (df['Date'].dt.month <= 5)).astype(int)\n",
        "        df['is_summer_shopping'] = ((df['Date'].dt.month >= 6) &\n",
        "                                    (df['Date'].dt.month <= 8)).astype(int)\n",
        "        df['is_fall_shopping'] = ((df['Date'].dt.month >= 9) &\n",
        "                                  (df['Date'].dt.month <= 11)).astype(int)\n",
        "        df['is_winter_shopping'] = ((df['Date'].dt.month == 12) |\n",
        "                                    (df['Date'].dt.month <= 2)).astype(int)\n",
        "\n",
        "        return df"
      ],
      "metadata": {
        "id": "aYANFQ4X7-zk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# End preprocess"
      ],
      "metadata": {
        "id": "frVC6m5f8DDh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "enhanced_pipeline = Pipeline([\n",
        "    ('merge', BaseMerger(features, stores)),\n",
        "    ('feature_add', FeatureAdder()),  # Your existing feature adder\n",
        "    ('advanced_seasonal', AdvancedSeasonalFeatures()),\n",
        "    ('advanced_date', AdvancedDateFeatures()),\n",
        "    ('store_features', StoreSpecificFeatures()),\n",
        "    ('economic_features', EconomicInteractionFeatures()),\n",
        "    ('fillna', MissingValueFiller()),\n",
        "    ('label_encode', CategoricalEncoder()),\n",
        "    ('improved_lags', ImprovedLagFeatureTransformer(\n",
        "        lags=[1, 2, 3, 4, 8, 12, 52],\n",
        "        rolling_windows=[2, 4, 8, 12, 26],\n",
        "        ewm_spans=[4, 8, 12],\n",
        "        drop_na=True\n",
        "    )),\n",
        "])"
      ],
      "metadata": {
        "id": "fLv1twYP8FQr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df = enhanced_pipeline.fit_transform(train)\n",
        "test_df = enhanced_pipeline.transform(test)\n",
        "train_df.shape, test_df.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "id": "UV9PWY8f8URH",
        "outputId": "ec5b8fd9-d69b-489f-d361-efbf53a5f71a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "can't multiply sequence by non-int of type 'float'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/ops/array_ops.py\u001b[0m in \u001b[0;36m_na_arithmetic_op\u001b[0;34m(left, right, op, is_cmp)\u001b[0m\n\u001b[1;32m    217\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 218\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mleft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mright\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    219\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/computation/expressions.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(op, a, b, use_numexpr)\u001b[0m\n\u001b[1;32m    241\u001b[0m             \u001b[0;31m# error: \"None\" not callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 242\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0m_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop_str\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    243\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_evaluate_standard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop_str\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/computation/expressions.py\u001b[0m in \u001b[0;36m_evaluate_numexpr\u001b[0;34m(op, op_str, a, b)\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_evaluate_standard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop_str\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/computation/expressions.py\u001b[0m in \u001b[0;36m_evaluate_standard\u001b[0;34m(op, op_str, a, b)\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0m_store_test_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: can't multiply sequence by non-int of type 'float'",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-228-1286487471.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menhanced_pipeline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtest_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menhanced_pipeline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtrain_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1387\u001b[0m                 )\n\u001b[1;32m   1388\u001b[0m             ):\n\u001b[0;32m-> 1389\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfit_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1390\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1391\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, X, y, **params)\u001b[0m\n\u001b[1;32m    716\u001b[0m         \"\"\"\n\u001b[1;32m    717\u001b[0m         \u001b[0mrouted_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_method_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"fit_transform\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprops\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 718\u001b[0;31m         \u001b[0mXt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrouted_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    719\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    720\u001b[0m         \u001b[0mlast_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_final_estimator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, X, y, routed_params, raw_params)\u001b[0m\n\u001b[1;32m    586\u001b[0m             )\n\u001b[1;32m    587\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 588\u001b[0;31m             X, fitted_transformer = fit_transform_one_cached(\n\u001b[0m\u001b[1;32m    589\u001b[0m                 \u001b[0mcloned_transformer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    590\u001b[0m                 \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/joblib/memory.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    327\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcall_and_shelve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36m_fit_transform_one\u001b[0;34m(transformer, X, y, weight, message_clsname, message, params)\u001b[0m\n\u001b[1;32m   1549\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0m_print_elapsed_time\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage_clsname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1550\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"fit_transform\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1551\u001b[0;31m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"fit_transform\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1552\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1553\u001b[0m             res = transformer.fit(X, y, **params.get(\"fit\", {})).transform(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/utils/_set_output.py\u001b[0m in \u001b[0;36mwrapped\u001b[0;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[1;32m    317\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mwraps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 319\u001b[0;31m         \u001b[0mdata_to_wrap\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    320\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_to_wrap\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m             \u001b[0;31m# only wrap the first output for cross decomposition\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    916\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    917\u001b[0m             \u001b[0;31m# fit method of arity 1 (unsupervised transformation)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 918\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    919\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    920\u001b[0m             \u001b[0;31m# fit method of arity 2 (supervised transformation)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/utils/_set_output.py\u001b[0m in \u001b[0;36mwrapped\u001b[0;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[1;32m    317\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mwraps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 319\u001b[0;31m         \u001b[0mdata_to_wrap\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    320\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_to_wrap\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m             \u001b[0;31m# only wrap the first output for cross decomposition\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-224-2750623673.py\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[0;31m# Store type interactions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m         \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'type_size_interaction'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Type'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog1p\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Size'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0;31m# Clean up\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/ops/common.py\u001b[0m in \u001b[0;36mnew_method\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0mother\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mitem_from_zerodim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mnew_method\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/arraylike.py\u001b[0m in \u001b[0;36m__mul__\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0munpack_zerodim_and_defer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"__mul__\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__mul__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 202\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_arith_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moperator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    203\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0munpack_zerodim_and_defer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"__rmul__\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m_arith_method\u001b[0;34m(self, other, op)\u001b[0m\n\u001b[1;32m   6133\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_arith_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6134\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_align_for_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6135\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mbase\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIndexOpsMixin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_arith_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6136\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6137\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_align_for_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mright\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malign_asobject\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/base.py\u001b[0m in \u001b[0;36m_arith_method\u001b[0;34m(self, other, op)\u001b[0m\n\u001b[1;32m   1380\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1381\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merrstate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"ignore\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1382\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marithmetic_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1383\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1384\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_construct_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mres_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/ops/array_ops.py\u001b[0m in \u001b[0;36marithmetic_op\u001b[0;34m(left, right, op)\u001b[0m\n\u001b[1;32m    281\u001b[0m         \u001b[0;31m# error: Argument 1 to \"_na_arithmetic_op\" has incompatible type\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    282\u001b[0m         \u001b[0;31m# \"Union[ExtensionArray, ndarray[Any, Any]]\"; expected \"ndarray[Any, Any]\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 283\u001b[0;31m         \u001b[0mres_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_na_arithmetic_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mleft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mright\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[arg-type]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    284\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mres_values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/ops/array_ops.py\u001b[0m in \u001b[0;36m_na_arithmetic_op\u001b[0;34m(left, right, op, is_cmp)\u001b[0m\n\u001b[1;32m    225\u001b[0m             \u001b[0;31m# Don't do this for comparisons, as that will handle complex numbers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m             \u001b[0;31m#  incorrectly, see GH#32047\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 227\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_masked_arith_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mleft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mright\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    228\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m             \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/ops/array_ops.py\u001b[0m in \u001b[0;36m_masked_arith_op\u001b[0;34m(x, y, op)\u001b[0m\n\u001b[1;32m    161\u001b[0m         \u001b[0;31m# See GH#5284, GH#5035, GH#19448 for historical reference\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 163\u001b[0;31m             \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxrav\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myrav\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: can't multiply sequence by non-int of type 'float'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# from sklearn.pipeline import Pipeline\n",
        "# pipeline = Pipeline([\n",
        "#     ('merge',        BaseMerger(features, stores)),\n",
        "#     ('feature_add',  FeatureAdder()),\n",
        "#     ('fillna',       MissingValueFiller()),\n",
        "#     ('label_encode', CategoricalEncoder()),\n",
        "#     ('lags',         LagFeatureTransformer(\n",
        "#                         lags=[1,2,3,4],           # e.g. use 4 lags\n",
        "#                         rolling_windows=[4,8],    # e.g. 4‑wk & 8‑wk rolling avg\n",
        "#                     )),\n",
        "# ])\n",
        "\n",
        "# train_df = pipeline.fit_transform(train)\n",
        "# test_df = pipeline.transform(test)\n",
        "\n",
        "# y = train_df['Weekly_Sales']\n",
        "# X = train_df.drop(columns=['Weekly_Sales'])\n",
        "\n",
        "# train_df.columns, train_df.shape, test_df.columns, test_df.shape, X.shape, y.shape\n",
        "\n",
        "from sklearn.pipeline import Pipeline\n",
        "pipeline = Pipeline([\n",
        "    ('merge',        BaseMerger(features, stores)),\n",
        "    ('fillna',       MissingValueFiller()),\n",
        "    ('label_encode', CategoricalEncoder()),\n",
        "    ('feature_add',  FeatureAdder()),\n",
        "    ('lags',         LagFeatureTransformer(\n",
        "                        lags=[1,2,3,4],           # e.g. use 4 lags\n",
        "                        rolling_windows=[4,8],    # e.g. 4‑wk & 8‑wk rolling avg\n",
        "                    )),\n",
        "])\n",
        "\n",
        "train_df = pipeline.fit_transform(train)\n",
        "test_df = pipeline.transform(test)\n",
        "\n",
        "y = train_df['Weekly_Sales']\n",
        "X = train_df.drop(columns=['Weekly_Sales'])\n",
        "\n",
        "train_df.columns, train_df.shape, test_df.columns, test_df.shape, X.shape, y.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2QP2dvM592Ax",
        "outputId": "f6b77f48-49be-47f6-faa1-ad056b922537"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(Index(['Store', 'Dept', 'Weekly_Sales', 'IsHoliday', 'Temperature',\n",
              "        'Fuel_Price', 'MarkDown1', 'MarkDown2', 'MarkDown3', 'MarkDown4',\n",
              "        'MarkDown5', 'CPI', 'Unemployment', 'Type', 'Size', 'Month',\n",
              "        'SuperbowlWeek', 'LaborDayWeek', 'ThanksgivingWeek', 'ChristmasWeek',\n",
              "        'Days_to_Thanksgiving', 'Days_to_Christmas', 'DateOrdinal', 'lag_1',\n",
              "        'lag_2', 'lag_3', 'lag_4', 'rolling_mean_4', 'rolling_mean_8'],\n",
              "       dtype='object'),\n",
              " (398796, 29),\n",
              " Index(['Store', 'Dept', 'IsHoliday', 'Temperature', 'Fuel_Price', 'MarkDown1',\n",
              "        'MarkDown2', 'MarkDown3', 'MarkDown4', 'MarkDown5', 'CPI',\n",
              "        'Unemployment', 'Type', 'Size', 'Month', 'SuperbowlWeek',\n",
              "        'LaborDayWeek', 'ThanksgivingWeek', 'ChristmasWeek',\n",
              "        'Days_to_Thanksgiving', 'Days_to_Christmas', 'DateOrdinal', 'lag_1',\n",
              "        'lag_2', 'lag_3', 'lag_4', 'rolling_mean_4', 'rolling_mean_8'],\n",
              "       dtype='object'),\n",
              " (115064, 28),\n",
              " (398796, 28),\n",
              " (398796,))"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ML flow tracking for cleaning and feature engineering"
      ],
      "metadata": {
        "id": "sHa-eS60_q50"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install -q dagshub mlflow\n",
        "import mlflow\n",
        "import dagshub\n",
        "import mlflow.xgboost\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "import xgboost as xgb\n",
        "\n",
        "\n",
        "dagshub.init(repo_owner='nkhar21', repo_name='ML_Final_Project', mlflow=True)\n",
        "mlflow.set_tracking_uri(\"https://dagshub.com/nkhar21/ML_Final_Project.mlflow\")\n",
        "\n",
        "experiment_name = \"XGBoost_Training\"\n",
        "mlflow.set_experiment(experiment_name)"
      ],
      "metadata": {
        "id": "ncpvGaLm_tGu",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 570,
          "referenced_widgets": [
            "2967a2c81f514cf0b6b5069e64e421a0",
            "e5ca99aed7794c179b9f23f980d06cf9"
          ]
        },
        "collapsed": true,
        "outputId": "243b8be5-f5f0-4798-c021-1c95f20abbec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m261.0/261.0 kB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.7/24.7 MB\u001b[0m \u001b[31m64.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m63.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m242.7/242.7 kB\u001b[0m \u001b[31m17.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m147.8/147.8 kB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.9/114.9 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.0/85.0 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.9/139.9 kB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m87.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m733.8/733.8 kB\u001b[0m \u001b[31m42.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m203.4/203.4 kB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.8/65.8 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m118.5/118.5 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.2/196.2 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.2/85.2 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m74.3/74.3 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "                                       \u001b[1m❗❗❗ AUTHORIZATION REQUIRED ❗❗❗\u001b[0m                                        \n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">                                       <span style=\"font-weight: bold\">❗❗❗ AUTHORIZATION REQUIRED ❗❗❗</span>                                        \n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Output()"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2967a2c81f514cf0b6b5069e64e421a0"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Open the following link in your browser to authorize the client:\n",
            "https://dagshub.com/login/oauth/authorize?state=5efc9838-c6bb-4c44-b531-55941041d2ba&client_id=32b60ba385aa7cecf24046d8195a71c07dd345d9657977863b52e7748e0f0f28&middleman_request_id=73c27564289ba72ee211b6d4b42263cc313d5e6755ade72147245c83389255ce\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Accessing as nkhar21\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Accessing as nkhar21\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Initialized MLflow to track repo \u001b[32m\"nkhar21/ML_Final_Project\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Initialized MLflow to track repo <span style=\"color: #008000; text-decoration-color: #008000\">\"nkhar21/ML_Final_Project\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Repository nkhar21/ML_Final_Project initialized!\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Repository nkhar21/ML_Final_Project initialized!\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<Experiment: artifact_location='mlflow-artifacts:/b6bb21150f0c4042863bc345b7edb3cd', creation_time=1751801003477, experiment_id='0', last_update_time=1751801003477, lifecycle_stage='active', name='XGBoost_Training', tags={}>"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Log Cleaning"
      ],
      "metadata": {
        "id": "aHrMU5KGbCk9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cleaning_pipeline = Pipeline([\n",
        "    (\"merge\",       BaseMerger(features, stores)),     # joins store + feature tables\n",
        "    (\"fillna\",      MissingValueFiller()),             # impute mark‑downs, CPI, Unemployment\n",
        "    (\"label_encode\", CategoricalEncoder()),            # encode Type, IsHoliday\n",
        "])\n",
        "\n",
        "with mlflow.start_run(run_name=\"XGBoost_Cleaning\"):\n",
        "    mlflow.log_params({\n",
        "        \"merge_on\": [\"Store\",\"Date\",\"IsHoliday\"],\n",
        "        \"fill_markdowns_with\": 0,\n",
        "        \"impute_CPI_with\": \"mean\",\n",
        "        \"impute_Unemp_with\": \"mean\",\n",
        "        \"encode_Type_map\": str({'A':3,'B':2,'C':1}),\n",
        "        \"encode_Holiday_map\": str({False:0,True:1})\n",
        "    })\n",
        "\n",
        "    cleaned = cleaning_pipeline.fit_transform(train)\n",
        "    cleaned_test = cleaning_pipeline.transform(test)\n",
        "\n",
        "    mlflow.log_metric(\"n_clean_rows\", cleaned.shape[0])\n",
        "    mlflow.log_metric(\"n_clean_cols\", cleaned.shape[1])\n",
        "\n",
        "    # Optionally pickle & log the pipeline\n",
        "    import pickle\n",
        "    with open(\"cleaning_pipeline.pkl\", \"wb\") as f:\n",
        "        pickle.dump(cleaning_pipeline, f)\n",
        "    mlflow.log_artifact(\"cleaning_pipeline.pkl\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L87cecicbEBC",
        "outputId": "c0200d8f-2a2c-4ed7-f4bc-8caecf581ed0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/pipeline.py:62: FutureWarning: This Pipeline instance is not fitted yet. Call 'fit' with appropriate arguments before using other methods such as transform, predict, etc. This will raise an error in 1.8 instead of the current warning.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🏃 View run XGBoost_Cleaning at: https://dagshub.com/nkhar21/ML_Final_Project.mlflow/#/experiments/0/runs/56a1e343070144ab9a89628a8ce4bb46\n",
            "🧪 View experiment at: https://dagshub.com/nkhar21/ML_Final_Project.mlflow/#/experiments/0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Log Feature Engineering"
      ],
      "metadata": {
        "id": "Jop5MdGGbLb-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "feature_pipeline = Pipeline([\n",
        "    (\"feature_add\",  FeatureAdder()),\n",
        "    (\"lags\",         LagFeatureTransformer(\n",
        "                        lags=[1,2,3,4],\n",
        "                        rolling_windows=[4,8],\n",
        "                    )),\n",
        "])\n",
        "\n",
        "with mlflow.start_run(run_name=\"XGBoost_Feature_Engineering\"):\n",
        "    mlflow.log_params({\n",
        "        \"use_superbowl_holiday\": True,\n",
        "        \"lags\": [1,2,3,4],\n",
        "        \"rolling_windows\": [4,8],\n",
        "    })\n",
        "\n",
        "    fe_train = feature_pipeline.fit_transform(cleaned)\n",
        "    fe_test  = feature_pipeline.transform(cleaned_test)\n",
        "\n",
        "    mlflow.log_metric(\"n_fe_rows\", fe_train.shape[0])\n",
        "    mlflow.log_metric(\"n_fe_cols\", fe_train.shape[1])\n",
        "\n",
        "    with open(\"feature_pipeline.pkl\", \"wb\") as f:\n",
        "        pickle.dump(feature_pipeline, f)\n",
        "    mlflow.log_artifact(\"feature_pipeline.pkl\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZNV8mqlkbFXU",
        "outputId": "e5e9d3ef-614d-4c1a-940c-cd2916dccb14"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🏃 View run XGBoost_Feature_Engineering at: https://dagshub.com/nkhar21/ML_Final_Project.mlflow/#/experiments/0/runs/f51bf64a6a6a45a28c7885ba6a56099e\n",
            "🧪 View experiment at: https://dagshub.com/nkhar21/ML_Final_Project.mlflow/#/experiments/0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training Helper functions\n"
      ],
      "metadata": {
        "id": "otSYIzPolQmj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def log_xgb_params(model):\n",
        "    # Pull hyper‑params straight off the fitted model\n",
        "    p = model.get_params()\n",
        "    mlflow.log_params({\n",
        "        \"n_estimators\":    p[\"n_estimators\"],\n",
        "        \"max_depth\":       p[\"max_depth\"],\n",
        "        \"learning_rate\":   p[\"learning_rate\"],\n",
        "        \"subsample\":       p.get(\"subsample\"),\n",
        "        \"colsample_bytree\":p.get(\"colsample_bytree\"),\n",
        "        \"gamma\":           p.get(\"gamma\"),\n",
        "        \"reg_alpha\":       p.get(\"reg_alpha\"),\n",
        "        \"reg_lambda\":      p.get(\"reg_lambda\"),\n",
        "        \"objective\":       p.get(\"objective\"),\n",
        "        \"random_state\":    p.get(\"random_state\"),\n",
        "    })\n",
        "\n",
        "def evaluate(model, X, y, weights, split):\n",
        "    preds = model.predict(X)\n",
        "    wmae = (weights * np.abs(y - preds)).sum() / weights.sum()\n",
        "    mae  = mean_absolute_error(y, preds)\n",
        "    rmse = np.sqrt(mean_squared_error(y, preds))\n",
        "    r2   = r2_score(y, preds)\n",
        "    # MAPE (skip zero‑actual rows)\n",
        "    mask = y != 0\n",
        "    mape = np.mean(np.abs((y[mask] - preds[mask]) / y[mask])) * 100\n",
        "\n",
        "    mlflow.log_metrics({\n",
        "        f\"{split}_WMAE\": wmae,\n",
        "        f\"{split}_MAE\":  mae,\n",
        "        f\"{split}_RMSE\": rmse,\n",
        "        f\"{split}_R2\":   r2,\n",
        "        f\"{split}_MAPE\": mape\n",
        "    })\n",
        "\n",
        "    print(f\"[{split.upper()}] WMAE={wmae:.4f}, MAE={mae:.4f}, RMSE={rmse:.4f}, R²={r2:.4f}, MAPE={mape:.2f}%\")\n",
        "    return {\"WMAE\": wmae, \"MAE\": mae, \"RMSE\": rmse, \"R2\": r2, \"MAPE\": mape}\n"
      ],
      "metadata": {
        "id": "yH7buyGqsJi4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "pipeline = Pipeline([\n",
        "    ('merge',        BaseMerger(features, stores)),\n",
        "    ('fillna',       MissingValueFiller()),\n",
        "    ('label_encode', CategoricalEncoder()),\n",
        "    ('feature_add',  FeatureAdder()),\n",
        "    ('lags',         LagFeatureTransformer(\n",
        "                        lags=[1,2,3,4],           # e.g. use 4 lags\n",
        "                        rolling_windows=[4,8],    # e.g. 4‑wk & 8‑wk rolling avg\n",
        "                    )),\n",
        "])\n",
        "\n",
        "train_df = pipeline.fit_transform(train)"
      ],
      "metadata": {
        "id": "vQanRNHYbU9L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training 1"
      ],
      "metadata": {
        "id": "L_cwrTa4QCWI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import mlflow\n",
        "import mlflow.xgboost\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "import xgboost as xgb\n",
        "import numpy as np\n",
        "\n",
        "y = train_df[\"Weekly_Sales\"]\n",
        "X = train_df.drop(columns=[\"Weekly_Sales\"])\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "w_train = X_train[\"IsHoliday\"].astype(bool).map({True:5, False:1}).values\n",
        "w_val   = X_val  [\"IsHoliday\"].astype(bool).map({True:5, False:1}).values\n",
        "\n",
        "with mlflow.start_run(run_name=\"XGBoost_Regressor_1\"):\n",
        "    mlflow.xgboost.autolog()\n",
        "\n",
        "    model = xgb.XGBRegressor(\n",
        "        n_estimators=200,\n",
        "        max_depth=6,\n",
        "        learning_rate=0.1,\n",
        "        subsample=0.8,\n",
        "        colsample_bytree=0.8,\n",
        "        gamma=0,\n",
        "        reg_alpha=0,\n",
        "        reg_lambda=1,\n",
        "        objective=\"reg:squarederror\",\n",
        "        random_state=42,\n",
        "        n_jobs=-1,\n",
        "    )\n",
        "\n",
        "    log_xgb_params(model)\n",
        "\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    train_metrics = evaluate(model, X_train, y_train, w_train, split=\"train\")\n",
        "    val_metrics   = evaluate(model, X_val,   y_val,   w_val,   split=\"val\")\n",
        "\n",
        "    delta_metrics = {\n",
        "        f\"delta_{m}\": train_metrics[m] - val_metrics[m]\n",
        "        for m in train_metrics\n",
        "    }\n",
        "    mlflow.log_metrics(delta_metrics)\n",
        "    print(\"Overfitting deltas:\", delta_metrics)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6R9RTZVqoOGd",
        "outputId": "36810ad6-5f9a-4b84-ee0c-f456c35ecd45"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025/07/06 12:13:52 WARNING mlflow.utils.autologging_utils: MLflow autologging encountered a warning: \"/usr/local/lib/python3.11/dist-packages/mlflow/types/utils.py:452: UserWarning: Hint: Inferred schema contains integer column(s). Integer columns in Python cannot represent missing values. If your input data contains missing values at inference time, it will be encoded as floats and will cause a schema enforcement error. The best way to avoid this problem is to infer the model schema based on a realistic data sample (training dataset) that includes missing values. Alternatively, you can declare integer columns as doubles (float64) whenever these columns may have missing values. See `Handling Integers With Missing Values <https://www.mlflow.org/docs/latest/models.html#handling-integers-with-missing-values>`_ for more details.\"\n",
            "2025/07/06 12:13:52 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n",
            "2025/07/06 12:13:52 WARNING mlflow.utils.autologging_utils: Encountered unexpected error during xgboost autologging: INTERNAL_ERROR: Response: {'error': 'unsupported endpoint, please contact support@dagshub.com'}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[TRAIN] WMAE=930.2843, MAE=875.2516, RMSE=1675.6686, R²=0.9946, MAPE=356.97%\n",
            "[VAL] WMAE=1012.2616, MAE=928.8814, RMSE=2184.3777, R²=0.9905, MAPE=297.24%\n",
            "Overfitting deltas: {'delta_WMAE': np.float64(-81.97734535682025), 'delta_MAE': -53.62976558877472, 'delta_RMSE': np.float64(-508.7091047403974), 'delta_R2': 0.0041069145288110676, 'delta_MAPE': np.float64(59.72380464369479)}\n",
            "🏃 View run XGBoost_Regressor_1 at: https://dagshub.com/nkhar21/ML_Final_Project.mlflow/#/experiments/0/runs/675aace5d0fc418fb5c3673136d470fb\n",
            "🧪 View experiment at: https://dagshub.com/nkhar21/ML_Final_Project.mlflow/#/experiments/0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import mlflow\n",
        "import mlflow.xgboost\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "import xgboost as xgb\n",
        "import numpy as np\n",
        "\n",
        "# --- 1) Prepare X, y and filter out bad labels ---\n",
        "y_raw = train_df[\"Weekly_Sales\"]\n",
        "X = train_df.drop(columns=[\"Weekly_Sales\"])\n",
        "\n",
        "# log transform\n",
        "y_log = np.log1p(y_raw)\n",
        "\n",
        "# filter non‑finite\n",
        "mask = np.isfinite(y_log)\n",
        "if not mask.all():\n",
        "    bad = (~mask).sum()\n",
        "    print(f\"Dropping {bad} rows due to NaN/inf after log1p.\")\n",
        "    X = X.loc[mask].reset_index(drop=True)\n",
        "    y_log = y_log.loc[mask].reset_index(drop=True)\n",
        "\n",
        "# weighted MAE weights\n",
        "w_all = X[\"IsHoliday\"].astype(bool).map({True:5, False:1}).values\n",
        "\n",
        "# train/val split\n",
        "X_train, X_val, y_train_log, y_val_log, w_train, w_val = train_test_split(\n",
        "    X, y_log, w_all, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# --- 2) Helpers (unchanged) ---\n",
        "def log_xgb_params(model):\n",
        "    p = model.get_params()\n",
        "    mlflow.log_params({\n",
        "        \"n_estimators\":    p[\"n_estimators\"],\n",
        "        \"max_depth\":       p[\"max_depth\"],\n",
        "        \"learning_rate\":   p[\"learning_rate\"],\n",
        "        \"subsample\":       p.get(\"subsample\"),\n",
        "        \"colsample_bytree\":p.get(\"colsample_bytree\"),\n",
        "        \"gamma\":           p.get(\"gamma\"),\n",
        "        \"reg_alpha\":       p.get(\"reg_alpha\"),\n",
        "        \"reg_lambda\":      p.get(\"reg_lambda\"),\n",
        "        \"objective\":       p.get(\"objective\"),\n",
        "        \"random_state\":    p.get(\"random_state\"),\n",
        "    })\n",
        "\n",
        "def evaluate(model, X, y_log, weights, split):\n",
        "    preds_log = model.predict(X)\n",
        "    preds = np.expm1(preds_log)\n",
        "    y = np.expm1(y_log)\n",
        "\n",
        "    wmae = (weights * np.abs(y - preds)).sum() / weights.sum()\n",
        "    mae  = mean_absolute_error(y, preds)\n",
        "    rmse = np.sqrt(mean_squared_error(y, preds))\n",
        "    r2   = r2_score(y, preds)\n",
        "    mask = y != 0\n",
        "    mape = np.mean(np.abs((y[mask] - preds[mask]) / y[mask])) * 100\n",
        "\n",
        "    mlflow.log_metrics({\n",
        "        f\"{split}_WMAE\": wmae,\n",
        "        f\"{split}_MAE\":  mae,\n",
        "        f\"{split}_RMSE\": rmse,\n",
        "        f\"{split}_R2\":   r2,\n",
        "        f\"{split}_MAPE\": mape\n",
        "    })\n",
        "\n",
        "    print(f\"[{split.upper()}] WMAE={wmae:.4f}, MAE={mae:.4f}, RMSE={rmse:.4f}, R²={r2:.4f}, MAPE={mape:.2f}%\")\n",
        "    return {\"WMAE\": wmae, \"MAE\": mae, \"RMSE\": rmse, \"R2\": r2, \"MAPE\": mape}\n",
        "\n",
        "# --- 3) Training Run ---\n",
        "with mlflow.start_run(run_name=\"XGBoost_Regressor_LogTarget\"):\n",
        "    mlflow.xgboost.autolog()\n",
        "\n",
        "    model = xgb.XGBRegressor(\n",
        "        n_estimators=300,\n",
        "        max_depth=6,\n",
        "        learning_rate=0.1,\n",
        "        subsample=0.8,\n",
        "        colsample_bytree=0.8,\n",
        "        gamma=0,\n",
        "        reg_alpha=0,\n",
        "        reg_lambda=1,\n",
        "        objective=\"reg:squarederror\",\n",
        "        random_state=42,\n",
        "        n_jobs=-1,\n",
        "    )\n",
        "\n",
        "    log_xgb_params(model)\n",
        "    model.fit(X_train, y_train_log)\n",
        "\n",
        "    train_metrics = evaluate(model, X_train, y_train_log, w_train, split=\"train\")\n",
        "    val_metrics   = evaluate(model, X_val,   y_val_log,   w_val,   split=\"val\")\n",
        "\n",
        "    delta = {f\"delta_{k}\": train_metrics[k] - val_metrics[k] for k in train_metrics}\n",
        "    mlflow.log_metrics(delta)\n",
        "    print(\"Overfitting deltas:\", delta)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "arjDlYz-u0RA",
        "outputId": "45b041f7-438a-4743-d57c-25de708c52be"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/pandas/core/arraylike.py:399: RuntimeWarning: divide by zero encountered in log1p\n",
            "  result = getattr(ufunc, method)(*inputs, **kwargs)\n",
            "/usr/local/lib/python3.11/dist-packages/pandas/core/arraylike.py:399: RuntimeWarning: invalid value encountered in log1p\n",
            "  result = getattr(ufunc, method)(*inputs, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dropping 956 rows due to NaN/inf after log1p.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025/07/06 15:00:19 WARNING mlflow.utils.autologging_utils: MLflow autologging encountered a warning: \"/usr/local/lib/python3.11/dist-packages/mlflow/types/utils.py:452: UserWarning: Hint: Inferred schema contains integer column(s). Integer columns in Python cannot represent missing values. If your input data contains missing values at inference time, it will be encoded as floats and will cause a schema enforcement error. The best way to avoid this problem is to infer the model schema based on a realistic data sample (training dataset) that includes missing values. Alternatively, you can declare integer columns as doubles (float64) whenever these columns may have missing values. See `Handling Integers With Missing Values <https://www.mlflow.org/docs/latest/models.html#handling-integers-with-missing-values>`_ for more details.\"\n",
            "2025/07/06 15:00:19 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n",
            "2025/07/06 15:00:19 WARNING mlflow.utils.autologging_utils: Encountered unexpected error during xgboost autologging: INTERNAL_ERROR: Response: {'error': 'unsupported endpoint, please contact support@dagshub.com'}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[TRAIN] WMAE=1239.1977, MAE=1099.2810, RMSE=3212.7891, R²=0.9801, MAPE=12.85%\n",
            "[VAL] WMAE=1273.9650, MAE=1130.9875, RMSE=3086.1107, R²=0.9817, MAPE=24.80%\n",
            "Overfitting deltas: {'delta_WMAE': np.float64(-34.76728909052076), 'delta_MAE': -31.706501328909553, 'delta_RMSE': np.float64(126.67836117897286), 'delta_R2': -0.0016264944645584256, 'delta_MAPE': np.float64(-11.950057416129955)}\n",
            "🏃 View run XGBoost_Regressor_LogTarget at: https://dagshub.com/nkhar21/ML_Final_Project.mlflow/#/experiments/0/runs/58f590a7633b46fd9cc4a6970ac7ae95\n",
            "🧪 View experiment at: https://dagshub.com/nkhar21/ML_Final_Project.mlflow/#/experiments/0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training 3 - Tune Hyperparams and use Scaler"
      ],
      "metadata": {
        "id": "nVTY6VnSSsSl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import mlflow\n",
        "import pickle\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler, FunctionTransformer\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, make_scorer\n",
        "import xgboost as xgb\n",
        "import numpy as np\n",
        "\n",
        "# 0) Prepare data & WMAE helper\n",
        "y = train_df[\"Weekly_Sales\"]\n",
        "X = train_df.drop(columns=[\"Weekly_Sales\"])\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "w_val = X_val[\"IsHoliday\"].map({True:5, False:1}).values\n",
        "\n",
        "def wmae(y_true, y_pred, w):\n",
        "    return (w * np.abs(y_true - y_pred)).sum() / w.sum()\n",
        "\n",
        "# 1) Build pipeline: scale → clamp inf → XGB\n",
        "final_pipeline = Pipeline([\n",
        "    (\"scaler\",     StandardScaler()),\n",
        "    (\"clamp_inf\",  FunctionTransformer(lambda X: np.nan_to_num(X, posinf=0, neginf=0))),\n",
        "    (\"regressor\",  xgb.XGBRegressor(\n",
        "                       objective=\"reg:squarederror\",\n",
        "                       subsample=0.8,\n",
        "                       colsample_bytree=0.8,\n",
        "                       gamma=0,\n",
        "                       reg_alpha=0,\n",
        "                       reg_lambda=1,\n",
        "                       random_state=42,\n",
        "                       n_jobs=-1\n",
        "                   )),\n",
        "])\n",
        "\n",
        "# 2) Parameter grid\n",
        "param_grid = {\n",
        "    \"regressor__n_estimators\":   [100, 200, 300],\n",
        "    \"regressor__max_depth\":      [4, 6, 8],\n",
        "    \"regressor__learning_rate\":  [0.05, 0.1, 0.2],\n",
        "}\n",
        "\n",
        "# 3) GridSearchCV on neg MAE\n",
        "grid = GridSearchCV(\n",
        "    final_pipeline,\n",
        "    param_grid=param_grid,\n",
        "    cv=3,\n",
        "    scoring=\"neg_mean_absolute_error\",\n",
        "    n_jobs=-1,\n",
        "    verbose=1,\n",
        "    return_train_score=False\n",
        ")\n",
        "\n",
        "mlflow.set_experiment(\"XGBoost_GridSearch\")\n",
        "with mlflow.start_run(run_name=\"GridSearch_Defaults\"):\n",
        "    # Fit\n",
        "    grid.fit(X_train, y_train)\n",
        "\n",
        "    # Extract best\n",
        "    best_pipe = grid.best_estimator_\n",
        "    best_params = grid.best_params_\n",
        "    mlflow.log_params({\n",
        "        \"n_estimators\":  best_params[\"regressor__n_estimators\"],\n",
        "        \"max_depth\":     best_params[\"regressor__max_depth\"],\n",
        "        \"learning_rate\": best_params[\"regressor__learning_rate\"],\n",
        "    })\n",
        "\n",
        "    # Evaluate on hold‑out\n",
        "    y_val_pred = best_pipe.predict(X_val)\n",
        "    val_wmae   = wmae(y_val, y_val_pred, w_val)\n",
        "    val_mae    = mean_absolute_error(y_val, y_val_pred)\n",
        "    val_rmse   = np.sqrt(mean_squared_error(y_val, y_val_pred))\n",
        "    val_r2     = r2_score(y_val, y_val_pred)\n",
        "\n",
        "    mlflow.log_metrics({\n",
        "        \"val_WMAE\": val_wmae,\n",
        "        \"val_MAE\":  val_mae,\n",
        "        \"val_RMSE\": val_rmse,\n",
        "        \"val_R2\":   val_r2\n",
        "    })\n",
        "\n",
        "    print(f\"Best params: {best_params}\")\n",
        "    print(f\"[VAL] WMAE={val_wmae:.4f}, MAE={val_mae:.4f}, RMSE={val_rmse:.4f}, R²={val_r2:.4f}\")\n",
        "\n",
        "    # 4) Serialize & log the best pipeline\n",
        "    with open(\"best_pipeline.pkl\", \"wb\") as f:\n",
        "        pickle.dump(best_pipe, f)\n",
        "    mlflow.log_artifact(\"best_pipeline.pkl\", artifact_path=\"model_pipeline\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 352
        },
        "id": "m876NUBCSvxm",
        "outputId": "f0b3f356-1c99-4a74-d98b-c3609db08945"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025/07/06 15:22:23 INFO mlflow.tracking.fluent: Experiment with name 'XGBoost_GridSearch' does not exist. Creating a new experiment.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 3 folds for each of 27 candidates, totalling 81 fits\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025/07/06 15:31:36 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n",
            "2025/07/06 15:31:37 WARNING mlflow.utils.autologging_utils: Encountered unexpected error during xgboost autologging: INTERNAL_ERROR: Response: {'error': 'unsupported endpoint, please contact support@dagshub.com'}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best params: {'regressor__learning_rate': 0.2, 'regressor__max_depth': 8, 'regressor__n_estimators': 300}\n",
            "[VAL] WMAE=nan, MAE=600.6671, RMSE=1790.1897, R²=0.9936\n",
            "🏃 View run GridSearch_Defaults at: https://dagshub.com/nkhar21/ML_Final_Project.mlflow/#/experiments/1/runs/46c267c4baa04449ae29653d63a0d478\n",
            "🧪 View experiment at: https://dagshub.com/nkhar21/ML_Final_Project.mlflow/#/experiments/1\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "PicklingError",
          "evalue": "Can't pickle <function <lambda> at 0x7dcdd0123e20>: attribute lookup <lambda> on __main__ failed",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mPicklingError\u001b[0m                             Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-26-3723333657.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0;31m# 4) Serialize & log the best pipeline\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"best_pipeline.pkl\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"wb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m         \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbest_pipe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m     \u001b[0mmlflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_artifact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"best_pipeline.pkl\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0martifact_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"model_pipeline\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mPicklingError\u001b[0m: Can't pickle <function <lambda> at 0x7dcdd0123e20>: attribute lookup <lambda> on __main__ failed"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training 4 - Tuning"
      ],
      "metadata": {
        "id": "ClYwoXftXw3P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import mlflow\n",
        "import pickle\n",
        "import numpy as np\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler, FunctionTransformer\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "import xgboost as xgb\n",
        "\n",
        "# --- Top‑level helper to clamp infinities ---\n",
        "def clamp_inf_array(X):\n",
        "    # convert ±inf to 0 and leave finite values unchanged\n",
        "    return np.nan_to_num(X, posinf=0, neginf=0)\n",
        "\n",
        "# --- 0) Prepare data & WMAE helper ---\n",
        "y = train_df[\"Weekly_Sales\"]\n",
        "X = train_df.drop(columns=[\"Weekly_Sales\"])\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "w_val = X_val[\"IsHoliday\"].map({True:5, False:1}).values\n",
        "\n",
        "def wmae(y_true, y_pred, w):\n",
        "    return (w * np.abs(y_true - y_pred)).sum() / w.sum()\n",
        "\n",
        "# --- 1) Final pipeline: scale → clamp_inf_array → XGB ---\n",
        "final_pipeline = Pipeline([\n",
        "    (\"scaler\",    StandardScaler()),\n",
        "    (\"clamp_inf\", FunctionTransformer(clamp_inf_array)),\n",
        "    (\"regressor\", xgb.XGBRegressor(\n",
        "                      objective=\"reg:squarederror\",\n",
        "                      subsample=0.8,\n",
        "                      colsample_bytree=0.8,\n",
        "                      gamma=0,\n",
        "                      reg_alpha=0,\n",
        "                      reg_lambda=1,\n",
        "                      random_state=42,\n",
        "                      n_jobs=-1\n",
        "                  )),\n",
        "])\n",
        "\n",
        "# --- 2) Parameter grid ---\n",
        "param_grid = {\n",
        "    \"regressor__n_estimators\":   [200, 300, 400],\n",
        "    \"regressor__max_depth\":      [6, 7, 8],\n",
        "    \"regressor__learning_rate\":  [0.15, 0.2, 0.25],\n",
        "}\n",
        "\n",
        "# --- 3) GridSearchCV setup ---\n",
        "grid = GridSearchCV(\n",
        "    final_pipeline,\n",
        "    param_grid=param_grid,\n",
        "    cv=2,\n",
        "    scoring=\"neg_mean_absolute_error\",\n",
        "    n_jobs=-1,\n",
        "    verbose=1,\n",
        "    return_train_score=False\n",
        ")\n",
        "\n",
        "mlflow.set_experiment(\"XGBoost_GridSearch\")\n",
        "with mlflow.start_run(run_name=\"GridSearch_Defaults\"):\n",
        "    # 4) Fit the grid\n",
        "    grid.fit(X_train, y_train)\n",
        "\n",
        "    # 5) Log best params\n",
        "    best_params = {\n",
        "        k.replace(\"regressor__\", \"\"): v\n",
        "        for k, v in grid.best_params_.items()\n",
        "    }\n",
        "    mlflow.log_params(best_params)\n",
        "\n",
        "    # 6) Evaluate on validation\n",
        "    best_pipe = grid.best_estimator_\n",
        "    y_val_pred = best_pipe.predict(X_val)\n",
        "    val_wmae   = wmae(y_val, y_val_pred, w_val)\n",
        "    val_mae    = mean_absolute_error(y_val, y_val_pred)\n",
        "    val_rmse   = np.sqrt(mean_squared_error(y_val, y_val_pred))\n",
        "    val_r2     = r2_score(y_val, y_val_pred)\n",
        "\n",
        "    mlflow.log_metrics({\n",
        "        \"val_WMAE\": val_wmae,\n",
        "        \"val_MAE\":  val_mae,\n",
        "        \"val_RMSE\": val_rmse,\n",
        "        \"val_R2\":   val_r2\n",
        "    })\n",
        "\n",
        "    print(f\"Best params: {best_params}\")\n",
        "    print(f\"[VAL] WMAE={val_wmae:.4f}, MAE={val_mae:.4f}, RMSE={val_rmse:.4f}, R²={val_r2:.4f}\")\n",
        "\n",
        "    # 7) Serialize & log the best pipeline\n",
        "    with open(\"best_pipeline.pkl\", \"wb\") as f:\n",
        "        pickle.dump(best_pipe, f)\n",
        "    mlflow.log_artifact(\"best_pipeline.pkl\", artifact_path=\"model_pipeline\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yzuC0XRCXytL",
        "outputId": "e65b19de-e92f-49dc-a925-9ff909b7dfee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 2 folds for each of 27 candidates, totalling 54 fits\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025/07/06 15:43:52 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n",
            "2025/07/06 15:43:52 WARNING mlflow.utils.autologging_utils: Encountered unexpected error during xgboost autologging: INTERNAL_ERROR: Response: {'error': 'unsupported endpoint, please contact support@dagshub.com'}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best params: {'learning_rate': 0.2, 'max_depth': 8, 'n_estimators': 400}\n",
            "[VAL] WMAE=nan, MAE=573.6104, RMSE=1772.5646, R²=0.9937\n",
            "🏃 View run GridSearch_Defaults at: https://dagshub.com/nkhar21/ML_Final_Project.mlflow/#/experiments/1/runs/8c7f8e34a23c4a0d8c54ec902962f2bd\n",
            "🧪 View experiment at: https://dagshub.com/nkhar21/ML_Final_Project.mlflow/#/experiments/1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training 5 - Tuning"
      ],
      "metadata": {
        "id": "1aelGhUGaYmJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import mlflow\n",
        "import pickle\n",
        "import numpy as np\n",
        "import itertools\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler, FunctionTransformer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "import xgboost as xgb\n",
        "\n",
        "# --- Helper to clamp infinities (named function for pickle) ---\n",
        "def clamp_inf_array(X):\n",
        "    return np.nan_to_num(X, posinf=0, neginf=0)\n",
        "\n",
        "# --- Safe WMAE to guard against NaNs and zero‐weight sums ---\n",
        "def safe_wmae(y_true, y_pred, weights):\n",
        "    mask = (\n",
        "        np.isfinite(y_true) &\n",
        "        np.isfinite(y_pred) &\n",
        "        np.isfinite(weights) &\n",
        "        (weights > 0)\n",
        "    )\n",
        "    if not mask.any():\n",
        "        return np.nan\n",
        "    return (weights[mask] * np.abs(y_true[mask] - y_pred[mask])).sum() / weights[mask].sum()\n",
        "\n",
        "# --- Data prep ---\n",
        "y = train_df[\"Weekly_Sales\"]\n",
        "X = train_df.drop(columns=[\"Weekly_Sales\"])\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Compute weights as plain numeric arrays (no dtype=object)\n",
        "# after your CategoricalEncoder, IsHoliday is 0/1\n",
        "w_train = np.where(X_train[\"IsHoliday\"] == 1, 5, 1)\n",
        "w_val   = np.where(X_val  [\"IsHoliday\"] == 1, 5, 1)\n",
        "\n",
        "# --- Parameter grid ---\n",
        "param_grid = {\n",
        "    \"n_estimators\":   [400, 800, 1600],\n",
        "    \"max_depth\":      [8, 10, 15],\n",
        "    \"learning_rate\":  [0.15, 0.2, 0.25],\n",
        "}\n",
        "param_combinations = list(itertools.product(\n",
        "    param_grid[\"n_estimators\"],\n",
        "    param_grid[\"max_depth\"],\n",
        "    param_grid[\"learning_rate\"]\n",
        "))\n",
        "\n",
        "mlflow.set_experiment(\"XGBoost_ManualGridSearch_2\")\n",
        "\n",
        "best_score = float(\"inf\")\n",
        "best_model = None\n",
        "best_params = None\n",
        "\n",
        "# --- Manual grid search with progress logging ---\n",
        "for idx, (ne, md, lr) in enumerate(param_combinations, start=1):\n",
        "    total = len(param_combinations)\n",
        "    run_name = f\"ne{ne}_md{md}_lr{lr}\"\n",
        "    print(f\"[{idx}/{total}] Training {run_name}...\")\n",
        "\n",
        "    with mlflow.start_run(run_name=run_name):\n",
        "        # Build pipeline\n",
        "        pipe = Pipeline([\n",
        "            (\"scaler\", StandardScaler()),\n",
        "            (\"clamp\",  FunctionTransformer(clamp_inf_array)),\n",
        "            (\"regressor\", xgb.XGBRegressor(\n",
        "                n_estimators=ne,\n",
        "                max_depth=md,\n",
        "                learning_rate=lr,\n",
        "                objective=\"reg:squarederror\",\n",
        "                subsample=0.8,\n",
        "                colsample_bytree=0.8,\n",
        "                gamma=0,\n",
        "                reg_alpha=0,\n",
        "                reg_lambda=1,\n",
        "                random_state=42,\n",
        "                n_jobs=-1\n",
        "            )),\n",
        "        ])\n",
        "\n",
        "        # Fit\n",
        "        pipe.fit(X_train, y_train)\n",
        "\n",
        "        # Predict & evaluate\n",
        "        y_pred = pipe.predict(X_val)\n",
        "        score  = safe_wmae(y_val.values, y_pred, w_val)\n",
        "        mae    = mean_absolute_error(y_val, y_pred)\n",
        "        rmse   = np.sqrt(mean_squared_error(y_val, y_pred))\n",
        "        r2     = r2_score(y_val, y_pred)\n",
        "\n",
        "        # If WMAE is NaN, skip this run\n",
        "        if np.isnan(score):\n",
        "            print(f\"    ⚠️  WMAE is NaN; skipping logging for this run.\")\n",
        "            continue\n",
        "\n",
        "        # Log parameters and metrics\n",
        "        mlflow.log_params({\n",
        "            \"n_estimators\":   ne,\n",
        "            \"max_depth\":      md,\n",
        "            \"learning_rate\":  lr\n",
        "        })\n",
        "        mlflow.log_metrics({\n",
        "            \"val_WMAE\": score,\n",
        "            \"val_MAE\":  mae,\n",
        "            \"val_RMSE\": rmse,\n",
        "            \"val_R2\":   r2\n",
        "        })\n",
        "\n",
        "        print(f\"    WMAE={score:.2f}, MAE={mae:.2f}, RMSE={rmse:.2f}, R²={r2:.4f}\")\n",
        "\n",
        "        # Track best model\n",
        "        if score < best_score:\n",
        "            best_score = score\n",
        "            best_model = pipe\n",
        "            best_params = {\"n_estimators\": ne, \"max_depth\": md, \"learning_rate\": lr}\n",
        "\n",
        "# --- Serialize & log the best pipeline artifact ---\n",
        "if best_model is not None:\n",
        "    with open(\"best_pipeline.pkl\", \"wb\") as f:\n",
        "        pickle.dump(best_model, f)\n",
        "    mlflow.log_artifact(\"best_pipeline.pkl\", artifact_path=\"model_pipeline\")\n",
        "\n",
        "    print(\"\\n✅ Best Model:\")\n",
        "    print(f\"   Params: {best_params}\")\n",
        "    print(f\"   WMAE:   {best_score:.2f}\")\n",
        "else:\n",
        "    print(\"⚠️ No valid model found (all runs had NaN WMAE).\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QCzG2N78adTL",
        "outputId": "7c5bece6-c9db-46a0-f0ea-646ec21d8a5b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1/27] Training ne400_md8_lr0.15...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025/07/06 18:03:02 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n",
            "2025/07/06 18:03:02 WARNING mlflow.utils.autologging_utils: Encountered unexpected error during xgboost autologging: INTERNAL_ERROR: Response: {'error': 'unsupported endpoint, please contact support@dagshub.com'}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    WMAE=642.24, MAE=587.51, RMSE=1754.89, R²=0.9939\n",
            "🏃 View run ne400_md8_lr0.15 at: https://dagshub.com/nkhar21/ML_Final_Project.mlflow/#/experiments/4/runs/65e5ef1ef901435bb03265a92682f854\n",
            "🧪 View experiment at: https://dagshub.com/nkhar21/ML_Final_Project.mlflow/#/experiments/4\n",
            "[2/27] Training ne400_md8_lr0.2...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025/07/06 18:03:29 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n",
            "2025/07/06 18:03:30 WARNING mlflow.utils.autologging_utils: Encountered unexpected error during xgboost autologging: INTERNAL_ERROR: Response: {'error': 'unsupported endpoint, please contact support@dagshub.com'}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    WMAE=632.35, MAE=573.61, RMSE=1772.56, R²=0.9937\n",
            "🏃 View run ne400_md8_lr0.2 at: https://dagshub.com/nkhar21/ML_Final_Project.mlflow/#/experiments/4/runs/4c771b0aaf0b4815b2d79d3df56f3550\n",
            "🧪 View experiment at: https://dagshub.com/nkhar21/ML_Final_Project.mlflow/#/experiments/4\n",
            "[3/27] Training ne400_md8_lr0.25...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025/07/06 18:03:56 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n",
            "2025/07/06 18:03:57 WARNING mlflow.utils.autologging_utils: Encountered unexpected error during xgboost autologging: INTERNAL_ERROR: Response: {'error': 'unsupported endpoint, please contact support@dagshub.com'}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    WMAE=649.16, MAE=581.13, RMSE=1721.84, R²=0.9941\n",
            "🏃 View run ne400_md8_lr0.25 at: https://dagshub.com/nkhar21/ML_Final_Project.mlflow/#/experiments/4/runs/affdd267abd54f7d9acae41ff8dde6c2\n",
            "🧪 View experiment at: https://dagshub.com/nkhar21/ML_Final_Project.mlflow/#/experiments/4\n",
            "[4/27] Training ne400_md10_lr0.15...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025/07/06 18:04:42 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n",
            "2025/07/06 18:04:42 WARNING mlflow.utils.autologging_utils: Encountered unexpected error during xgboost autologging: INTERNAL_ERROR: Response: {'error': 'unsupported endpoint, please contact support@dagshub.com'}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    WMAE=605.82, MAE=546.29, RMSE=1756.26, R²=0.9939\n",
            "🏃 View run ne400_md10_lr0.15 at: https://dagshub.com/nkhar21/ML_Final_Project.mlflow/#/experiments/4/runs/56cb14746f444de485477d67a6c7aaab\n",
            "🧪 View experiment at: https://dagshub.com/nkhar21/ML_Final_Project.mlflow/#/experiments/4\n",
            "[5/27] Training ne400_md10_lr0.2...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025/07/06 18:05:26 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n",
            "2025/07/06 18:05:26 WARNING mlflow.utils.autologging_utils: Encountered unexpected error during xgboost autologging: INTERNAL_ERROR: Response: {'error': 'unsupported endpoint, please contact support@dagshub.com'}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    WMAE=623.74, MAE=563.26, RMSE=1798.30, R²=0.9936\n",
            "🏃 View run ne400_md10_lr0.2 at: https://dagshub.com/nkhar21/ML_Final_Project.mlflow/#/experiments/4/runs/a746e2b1c3f943cbafcc42578f6bed89\n",
            "🧪 View experiment at: https://dagshub.com/nkhar21/ML_Final_Project.mlflow/#/experiments/4\n",
            "[6/27] Training ne400_md10_lr0.25...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025/07/06 18:06:10 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n",
            "2025/07/06 18:06:11 WARNING mlflow.utils.autologging_utils: Encountered unexpected error during xgboost autologging: INTERNAL_ERROR: Response: {'error': 'unsupported endpoint, please contact support@dagshub.com'}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    WMAE=651.73, MAE=577.92, RMSE=1815.51, R²=0.9934\n",
            "🏃 View run ne400_md10_lr0.25 at: https://dagshub.com/nkhar21/ML_Final_Project.mlflow/#/experiments/4/runs/de1d0fd1117e4ca88ea1556feb7b2040\n",
            "🧪 View experiment at: https://dagshub.com/nkhar21/ML_Final_Project.mlflow/#/experiments/4\n",
            "[7/27] Training ne400_md15_lr0.15...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025/07/06 18:09:40 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n",
            "2025/07/06 18:09:40 WARNING mlflow.utils.autologging_utils: Encountered unexpected error during xgboost autologging: INTERNAL_ERROR: Response: {'error': 'unsupported endpoint, please contact support@dagshub.com'}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    WMAE=672.69, MAE=597.14, RMSE=1913.28, R²=0.9927\n",
            "🏃 View run ne400_md15_lr0.15 at: https://dagshub.com/nkhar21/ML_Final_Project.mlflow/#/experiments/4/runs/a1f122713fde4790b5b995103b09607b\n",
            "🧪 View experiment at: https://dagshub.com/nkhar21/ML_Final_Project.mlflow/#/experiments/4\n",
            "[8/27] Training ne400_md15_lr0.2...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025/07/06 18:13:17 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n",
            "2025/07/06 18:13:17 WARNING mlflow.utils.autologging_utils: Encountered unexpected error during xgboost autologging: INTERNAL_ERROR: Response: {'error': 'unsupported endpoint, please contact support@dagshub.com'}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    WMAE=700.16, MAE=622.97, RMSE=1946.77, R²=0.9925\n",
            "🏃 View run ne400_md15_lr0.2 at: https://dagshub.com/nkhar21/ML_Final_Project.mlflow/#/experiments/4/runs/2468fe32eb074b12ae1ceff8e1006ab6\n",
            "🧪 View experiment at: https://dagshub.com/nkhar21/ML_Final_Project.mlflow/#/experiments/4\n",
            "[9/27] Training ne400_md15_lr0.25...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025/07/06 18:16:55 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n",
            "2025/07/06 18:16:55 WARNING mlflow.utils.autologging_utils: Encountered unexpected error during xgboost autologging: INTERNAL_ERROR: Response: {'error': 'unsupported endpoint, please contact support@dagshub.com'}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    WMAE=736.83, MAE=658.28, RMSE=1984.10, R²=0.9922\n",
            "🏃 View run ne400_md15_lr0.25 at: https://dagshub.com/nkhar21/ML_Final_Project.mlflow/#/experiments/4/runs/fba724e5f5074e97a8706f393685b54f\n",
            "🧪 View experiment at: https://dagshub.com/nkhar21/ML_Final_Project.mlflow/#/experiments/4\n",
            "[10/27] Training ne800_md8_lr0.15...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025/07/06 18:17:47 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n",
            "2025/07/06 18:17:48 WARNING mlflow.utils.autologging_utils: Encountered unexpected error during xgboost autologging: INTERNAL_ERROR: Response: {'error': 'unsupported endpoint, please contact support@dagshub.com'}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    WMAE=591.07, MAE=536.83, RMSE=1727.39, R²=0.9941\n",
            "🏃 View run ne800_md8_lr0.15 at: https://dagshub.com/nkhar21/ML_Final_Project.mlflow/#/experiments/4/runs/81f8a50b3cb6457e8d2f502957092cfe\n",
            "🧪 View experiment at: https://dagshub.com/nkhar21/ML_Final_Project.mlflow/#/experiments/4\n",
            "[11/27] Training ne800_md8_lr0.2...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025/07/06 18:18:35 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n",
            "2025/07/06 18:18:35 WARNING mlflow.utils.autologging_utils: Encountered unexpected error during xgboost autologging: INTERNAL_ERROR: Response: {'error': 'unsupported endpoint, please contact support@dagshub.com'}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    WMAE=601.09, MAE=542.23, RMSE=1756.69, R²=0.9939\n",
            "🏃 View run ne800_md8_lr0.2 at: https://dagshub.com/nkhar21/ML_Final_Project.mlflow/#/experiments/4/runs/1b484a7234f9418f8b524f98944a0318\n",
            "🧪 View experiment at: https://dagshub.com/nkhar21/ML_Final_Project.mlflow/#/experiments/4\n",
            "[12/27] Training ne800_md8_lr0.25...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025/07/06 18:19:26 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n",
            "2025/07/06 18:19:26 WARNING mlflow.utils.autologging_utils: Encountered unexpected error during xgboost autologging: INTERNAL_ERROR: Response: {'error': 'unsupported endpoint, please contact support@dagshub.com'}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    WMAE=627.41, MAE=558.84, RMSE=1715.87, R²=0.9941\n",
            "🏃 View run ne800_md8_lr0.25 at: https://dagshub.com/nkhar21/ML_Final_Project.mlflow/#/experiments/4/runs/e5ec99c50da54974aa1ad20b1793c534\n",
            "🧪 View experiment at: https://dagshub.com/nkhar21/ML_Final_Project.mlflow/#/experiments/4\n",
            "[13/27] Training ne800_md10_lr0.15...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025/07/06 18:20:49 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n",
            "2025/07/06 18:20:49 WARNING mlflow.utils.autologging_utils: Encountered unexpected error during xgboost autologging: INTERNAL_ERROR: Response: {'error': 'unsupported endpoint, please contact support@dagshub.com'}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    WMAE=590.92, MAE=531.60, RMSE=1752.51, R²=0.9939\n",
            "🏃 View run ne800_md10_lr0.15 at: https://dagshub.com/nkhar21/ML_Final_Project.mlflow/#/experiments/4/runs/1cd82ce76e7249e79538bd57fb4adbeb\n",
            "🧪 View experiment at: https://dagshub.com/nkhar21/ML_Final_Project.mlflow/#/experiments/4\n",
            "[14/27] Training ne800_md10_lr0.2...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025/07/06 18:22:16 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n",
            "2025/07/06 18:22:16 WARNING mlflow.utils.autologging_utils: Encountered unexpected error during xgboost autologging: INTERNAL_ERROR: Response: {'error': 'unsupported endpoint, please contact support@dagshub.com'}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    WMAE=616.56, MAE=556.01, RMSE=1797.25, R²=0.9936\n",
            "🏃 View run ne800_md10_lr0.2 at: https://dagshub.com/nkhar21/ML_Final_Project.mlflow/#/experiments/4/runs/3b9a882ff46d40a387bce4a231a8f42d\n",
            "🧪 View experiment at: https://dagshub.com/nkhar21/ML_Final_Project.mlflow/#/experiments/4\n",
            "[15/27] Training ne800_md10_lr0.25...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025/07/06 18:23:45 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n",
            "2025/07/06 18:23:45 WARNING mlflow.utils.autologging_utils: Encountered unexpected error during xgboost autologging: INTERNAL_ERROR: Response: {'error': 'unsupported endpoint, please contact support@dagshub.com'}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    WMAE=649.11, MAE=575.34, RMSE=1815.10, R²=0.9934\n",
            "🏃 View run ne800_md10_lr0.25 at: https://dagshub.com/nkhar21/ML_Final_Project.mlflow/#/experiments/4/runs/281c31df55d74c87a453307f69e10005\n",
            "🧪 View experiment at: https://dagshub.com/nkhar21/ML_Final_Project.mlflow/#/experiments/4\n",
            "[16/27] Training ne800_md15_lr0.15...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025/07/06 18:31:07 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n",
            "2025/07/06 18:31:07 WARNING mlflow.utils.autologging_utils: Encountered unexpected error during xgboost autologging: INTERNAL_ERROR: Response: {'error': 'unsupported endpoint, please contact support@dagshub.com'}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    WMAE=672.77, MAE=597.25, RMSE=1913.25, R²=0.9927\n",
            "🏃 View run ne800_md15_lr0.15 at: https://dagshub.com/nkhar21/ML_Final_Project.mlflow/#/experiments/4/runs/d5b9871110924a7b9afad276517946e9\n",
            "🧪 View experiment at: https://dagshub.com/nkhar21/ML_Final_Project.mlflow/#/experiments/4\n",
            "[17/27] Training ne800_md15_lr0.2...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025/07/06 18:38:39 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n",
            "2025/07/06 18:38:39 WARNING mlflow.utils.autologging_utils: Encountered unexpected error during xgboost autologging: INTERNAL_ERROR: Response: {'error': 'unsupported endpoint, please contact support@dagshub.com'}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    WMAE=700.28, MAE=623.10, RMSE=1946.80, R²=0.9925\n",
            "🏃 View run ne800_md15_lr0.2 at: https://dagshub.com/nkhar21/ML_Final_Project.mlflow/#/experiments/4/runs/b0c05a69f9504632b7a7194284ee946a\n",
            "🧪 View experiment at: https://dagshub.com/nkhar21/ML_Final_Project.mlflow/#/experiments/4\n",
            "[18/27] Training ne800_md15_lr0.25...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025/07/06 18:46:24 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n",
            "2025/07/06 18:46:24 WARNING mlflow.utils.autologging_utils: Encountered unexpected error during xgboost autologging: INTERNAL_ERROR: Response: {'error': 'unsupported endpoint, please contact support@dagshub.com'}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    WMAE=736.88, MAE=658.33, RMSE=1984.10, R²=0.9922\n",
            "🏃 View run ne800_md15_lr0.25 at: https://dagshub.com/nkhar21/ML_Final_Project.mlflow/#/experiments/4/runs/3ba42d35acbd4f758ba38a0094a4fed5\n",
            "🧪 View experiment at: https://dagshub.com/nkhar21/ML_Final_Project.mlflow/#/experiments/4\n",
            "[19/27] Training ne1600_md8_lr0.15...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025/07/06 18:48:12 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n",
            "2025/07/06 18:48:12 WARNING mlflow.utils.autologging_utils: Encountered unexpected error during xgboost autologging: INTERNAL_ERROR: Response: {'error': 'unsupported endpoint, please contact support@dagshub.com'}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    WMAE=571.62, MAE=518.15, RMSE=1721.68, R²=0.9941\n",
            "🏃 View run ne1600_md8_lr0.15 at: https://dagshub.com/nkhar21/ML_Final_Project.mlflow/#/experiments/4/runs/fd5d7fd923b840aab066280b3c655b92\n",
            "🧪 View experiment at: https://dagshub.com/nkhar21/ML_Final_Project.mlflow/#/experiments/4\n",
            "[20/27] Training ne1600_md8_lr0.2...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025/07/06 18:49:47 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n",
            "2025/07/06 18:49:47 WARNING mlflow.utils.autologging_utils: Encountered unexpected error during xgboost autologging: INTERNAL_ERROR: Response: {'error': 'unsupported endpoint, please contact support@dagshub.com'}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    WMAE=591.38, MAE=532.85, RMSE=1757.21, R²=0.9939\n",
            "🏃 View run ne1600_md8_lr0.2 at: https://dagshub.com/nkhar21/ML_Final_Project.mlflow/#/experiments/4/runs/c0f9262f042945fc9d92cd5cf540f5dd\n",
            "🧪 View experiment at: https://dagshub.com/nkhar21/ML_Final_Project.mlflow/#/experiments/4\n",
            "[21/27] Training ne1600_md8_lr0.25...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025/07/06 18:51:25 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n",
            "2025/07/06 18:51:25 WARNING mlflow.utils.autologging_utils: Encountered unexpected error during xgboost autologging: INTERNAL_ERROR: Response: {'error': 'unsupported endpoint, please contact support@dagshub.com'}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    WMAE=620.89, MAE=553.02, RMSE=1716.07, R²=0.9941\n",
            "🏃 View run ne1600_md8_lr0.25 at: https://dagshub.com/nkhar21/ML_Final_Project.mlflow/#/experiments/4/runs/a6a34e9c5ae54d50976254a86e3d8f9e\n",
            "🧪 View experiment at: https://dagshub.com/nkhar21/ML_Final_Project.mlflow/#/experiments/4\n",
            "[22/27] Training ne1600_md10_lr0.15...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025/07/06 18:54:21 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n",
            "2025/07/06 18:54:21 WARNING mlflow.utils.autologging_utils: Encountered unexpected error during xgboost autologging: INTERNAL_ERROR: Response: {'error': 'unsupported endpoint, please contact support@dagshub.com'}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    WMAE=588.68, MAE=529.62, RMSE=1752.61, R²=0.9939\n",
            "🏃 View run ne1600_md10_lr0.15 at: https://dagshub.com/nkhar21/ML_Final_Project.mlflow/#/experiments/4/runs/044dfdd305ed4f92a842df2d94bb89bd\n",
            "🧪 View experiment at: https://dagshub.com/nkhar21/ML_Final_Project.mlflow/#/experiments/4\n",
            "[23/27] Training ne1600_md10_lr0.2...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025/07/06 18:57:34 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n",
            "2025/07/06 18:57:34 WARNING mlflow.utils.autologging_utils: Encountered unexpected error during xgboost autologging: INTERNAL_ERROR: Response: {'error': 'unsupported endpoint, please contact support@dagshub.com'}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    WMAE=616.64, MAE=556.31, RMSE=1797.35, R²=0.9936\n",
            "🏃 View run ne1600_md10_lr0.2 at: https://dagshub.com/nkhar21/ML_Final_Project.mlflow/#/experiments/4/runs/a941b8a29ff3481a8073fcf26829defd\n",
            "🧪 View experiment at: https://dagshub.com/nkhar21/ML_Final_Project.mlflow/#/experiments/4\n",
            "[24/27] Training ne1600_md10_lr0.25...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025/07/06 19:00:42 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n",
            "2025/07/06 19:00:42 WARNING mlflow.utils.autologging_utils: Encountered unexpected error during xgboost autologging: INTERNAL_ERROR: Response: {'error': 'unsupported endpoint, please contact support@dagshub.com'}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    WMAE=650.35, MAE=576.86, RMSE=1815.54, R²=0.9934\n",
            "🏃 View run ne1600_md10_lr0.25 at: https://dagshub.com/nkhar21/ML_Final_Project.mlflow/#/experiments/4/runs/aa2a44def2014e6b8625835d661c808b\n",
            "🧪 View experiment at: https://dagshub.com/nkhar21/ML_Final_Project.mlflow/#/experiments/4\n",
            "[25/27] Training ne1600_md15_lr0.15...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:urllib3.connectionpool:Retrying (Retry(total=6, connect=7, read=6, redirect=7, status=7)) after connection broken by 'RemoteDisconnected('Remote end closed connection without response')': /nkhar21/ML_Final_Project.mlflow/api/2.0/mlflow/runs/get?run_uuid=d0407fafc3174a46ada7510bac4f4294&run_id=d0407fafc3174a46ada7510bac4f4294\n",
            "2025/07/06 19:15:57 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n",
            "2025/07/06 19:15:57 WARNING mlflow.utils.autologging_utils: Encountered unexpected error during xgboost autologging: INTERNAL_ERROR: Response: {'error': 'unsupported endpoint, please contact support@dagshub.com'}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    WMAE=672.79, MAE=597.26, RMSE=1913.26, R²=0.9927\n",
            "🏃 View run ne1600_md15_lr0.15 at: https://dagshub.com/nkhar21/ML_Final_Project.mlflow/#/experiments/4/runs/d0407fafc3174a46ada7510bac4f4294\n",
            "🧪 View experiment at: https://dagshub.com/nkhar21/ML_Final_Project.mlflow/#/experiments/4\n",
            "[26/27] Training ne1600_md15_lr0.2...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025/07/06 19:30:41 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n",
            "2025/07/06 19:30:41 WARNING mlflow.utils.autologging_utils: Encountered unexpected error during xgboost autologging: INTERNAL_ERROR: Response: {'error': 'unsupported endpoint, please contact support@dagshub.com'}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    WMAE=700.28, MAE=623.10, RMSE=1946.79, R²=0.9925\n",
            "🏃 View run ne1600_md15_lr0.2 at: https://dagshub.com/nkhar21/ML_Final_Project.mlflow/#/experiments/4/runs/4823d8a342834ba7ab2e7ad0f527bdee\n",
            "🧪 View experiment at: https://dagshub.com/nkhar21/ML_Final_Project.mlflow/#/experiments/4\n",
            "[27/27] Training ne1600_md15_lr0.25...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:urllib3.connectionpool:Retrying (Retry(total=6, connect=7, read=6, redirect=7, status=7)) after connection broken by 'RemoteDisconnected('Remote end closed connection without response')': /nkhar21/ML_Final_Project.mlflow/api/2.0/mlflow/runs/get?run_uuid=58735e550d36430cb5a0d681b93127c4&run_id=58735e550d36430cb5a0d681b93127c4\n",
            "2025/07/06 19:43:57 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n",
            "2025/07/06 19:43:57 WARNING mlflow.utils.autologging_utils: Encountered unexpected error during xgboost autologging: INTERNAL_ERROR: Response: {'error': 'unsupported endpoint, please contact support@dagshub.com'}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    WMAE=736.88, MAE=658.33, RMSE=1984.10, R²=0.9922\n",
            "🏃 View run ne1600_md15_lr0.25 at: https://dagshub.com/nkhar21/ML_Final_Project.mlflow/#/experiments/4/runs/58735e550d36430cb5a0d681b93127c4\n",
            "🧪 View experiment at: https://dagshub.com/nkhar21/ML_Final_Project.mlflow/#/experiments/4\n",
            "\n",
            "✅ Best Model:\n",
            "   Params: {'n_estimators': 1600, 'max_depth': 8, 'learning_rate': 0.15}\n",
            "   WMAE:   571.62\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Best model"
      ],
      "metadata": {
        "id": "gf7FmCdHZqhA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import mlflow\n",
        "import pickle\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler, FunctionTransformer\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, make_scorer\n",
        "import xgboost as xgb\n",
        "import numpy as np\n",
        "\n",
        "preprocessor_pipeline = Pipeline([\n",
        "    (\"merge\",       BaseMerger(features, stores)),\n",
        "    (\"fillna\",      MissingValueFiller()),\n",
        "    (\"label_encode\", CategoricalEncoder()),\n",
        "    (\"feature_add\",  FeatureAdder()),\n",
        "    (\"lags\",         LagFeatureTransformer(\n",
        "                        lags=[1,2,3,4],\n",
        "                        rolling_windows=[4,8],\n",
        "                    )),\n",
        "    ])\n",
        "\n",
        "def clamp_inf_array(X):\n",
        "    return np.nan_to_num(X, posinf=0, neginf=0)\n",
        "\n",
        "\n",
        "regressor_pipeline =  pipe = Pipeline([\n",
        "            (\"scaler\", StandardScaler()),\n",
        "            (\"clamp\",  FunctionTransformer(clamp_inf_array)),\n",
        "            (\"regressor\", xgb.XGBRegressor(\n",
        "                n_estimators=1600,\n",
        "                max_depth=8,\n",
        "                learning_rate=0.15,\n",
        "                objective=\"reg:squarederror\",\n",
        "                subsample=0.8,\n",
        "                colsample_bytree=0.8,\n",
        "                gamma=0,\n",
        "                reg_alpha=0,\n",
        "                reg_lambda=1,\n",
        "                random_state=42,\n",
        "                n_jobs=-1,\n",
        "                eval_metric=\"mae\"\n",
        "            )),\n",
        "        ])\n",
        "\n",
        "train_df = preprocessor_pipeline.fit_transform(train)\n",
        "test_df = preprocessor_pipeline.transform(test)\n",
        "\n",
        "y_train = train_df[\"Weekly_Sales\"]\n",
        "X_train = train_df.drop(columns=[\"Weekly_Sales\"])\n",
        "X_test = test_df\n",
        "\n",
        "X_train.columns, X_train.shape, y_train.shape, X_test.columns, X_test.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OEjZRsUPBp4V",
        "outputId": "5f8d4889-5d2d-42ec-c667-57ed901a1527"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(Index(['Store', 'Dept', 'IsHoliday', 'Temperature', 'Fuel_Price', 'MarkDown1',\n",
              "        'MarkDown2', 'MarkDown3', 'MarkDown4', 'MarkDown5', 'CPI',\n",
              "        'Unemployment', 'Type', 'Size', 'Month', 'SuperbowlWeek',\n",
              "        'LaborDayWeek', 'ThanksgivingWeek', 'ChristmasWeek',\n",
              "        'Days_to_Thanksgiving', 'Days_to_Christmas', 'DateOrdinal', 'lag_1',\n",
              "        'lag_2', 'lag_3', 'lag_4', 'rolling_mean_4', 'rolling_mean_8'],\n",
              "       dtype='object'),\n",
              " (398796, 28),\n",
              " (398796,),\n",
              " Index(['Store', 'Dept', 'IsHoliday', 'Temperature', 'Fuel_Price', 'MarkDown1',\n",
              "        'MarkDown2', 'MarkDown3', 'MarkDown4', 'MarkDown5', 'CPI',\n",
              "        'Unemployment', 'Type', 'Size', 'Month', 'SuperbowlWeek',\n",
              "        'LaborDayWeek', 'ThanksgivingWeek', 'ChristmasWeek',\n",
              "        'Days_to_Thanksgiving', 'Days_to_Christmas', 'DateOrdinal', 'lag_1',\n",
              "        'lag_2', 'lag_3', 'lag_4', 'rolling_mean_4', 'rolling_mean_8'],\n",
              "       dtype='object'),\n",
              " (115064, 28))"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "import mlflow\n",
        "import mlflow.xgboost\n",
        "import mlflow.sklearn\n",
        "import pandas as pd\n",
        "\n",
        "# 1) Hold out a validation set for early stopping\n",
        "X_tr, X_val, y_tr, y_val = train_test_split(\n",
        "    X_train, y_train, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# 2) Extract the scaler, clamp, and regressor from your pipeline\n",
        "scaler    = regressor_pipeline.named_steps[\"scaler\"]\n",
        "clamp     = regressor_pipeline.named_steps[\"clamp\"]\n",
        "xgb_model = regressor_pipeline.named_steps[\"regressor\"]\n",
        "\n",
        "# 3) Fit scaler & clamp on X_tr, transform X_val\n",
        "X_tr_s   = clamp.transform(scaler.fit_transform   (X_tr))\n",
        "X_val_s  = clamp.transform(scaler.transform       (X_val))\n",
        "\n",
        "# 4) Start MLflow run and turn on XGBoost autologging\n",
        "mlflow.set_experiment(\"XGBoost_Training\")\n",
        "with mlflow.start_run(run_name=\"xgb_final\") as run:\n",
        "    print(\"🧪 MLflow Run ID:\", run.info.run_id)\n",
        "    mlflow.xgboost.autolog()\n",
        "\n",
        "    # 5) Fit the raw XGB model with early stopping\n",
        "    xgb_model.fit(\n",
        "        X_tr_s, y_tr,\n",
        "        eval_set=[(X_val_s, y_val)]\n",
        "    )\n",
        "\n",
        "    # 6) Evaluate on hold‑out using the full pipeline (which re‐applies scaler+clamp)\n",
        "    val_preds = regressor_pipeline.predict(X_val)\n",
        "    val_mae   = mean_absolute_error(y_val, val_preds)\n",
        "    val_rmse  = (mean_squared_error(y_val, val_preds) ** 0.5)\n",
        "    val_r2    = r2_score(y_val, val_preds)\n",
        "    # Weighted MAE\n",
        "    weights   = np.where(X_val[\"IsHoliday\"]==1, 5, 1)\n",
        "    val_wmae  = (weights * np.abs(y_val - val_preds)).sum() / weights.sum()\n",
        "\n",
        "    mlflow.log_metrics({\n",
        "        \"val_WMAE\": val_wmae,\n",
        "        \"val_MAE\":  val_mae,\n",
        "        \"val_RMSE\": val_rmse,\n",
        "        \"val_R2\":   val_r2\n",
        "    })\n",
        "    print(f\"✅ Validation – WMAE:{val_wmae:.2f}, MAE:{val_mae:.2f}, RMSE:{val_rmse:.2f}, R²:{val_r2:.4f}\")\n",
        "\n",
        "    # 7) Predict on your test set and write submission\n",
        "    test_preds = regressor_pipeline.predict(X_test)\n",
        "    submission = pd.DataFrame({\n",
        "        \"Id\": test[\"Store\"].astype(str) + \"_\" +\n",
        "              test[\"Dept\"].astype(str)  + \"_\" +\n",
        "              test[\"Date\"].astype(str),\n",
        "        \"Weekly_Sales\": test_preds\n",
        "    })\n",
        "    submission.to_csv(\"xgb_submission.csv\", index=False)\n",
        "    print(\"✅ Written xgb_submission.csv\")\n",
        "\n",
        "    # 8) Save the full fitted pipeline with pickle\n",
        "    with open(\"xgb_model_pipeline.pkl\", \"wb\") as f:\n",
        "        pickle.dump(regressor_pipeline, f)\n",
        "    print(\"✅ Pipeline saved to 'xgb_model_pipeline.pkl'\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tQYqrIzCpvwB",
        "outputId": "8dbff1e9-a7b7-49d0-de10-91a2726979ee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🧪 MLflow Run ID: 63ec8410d6944556bdc70f1c55d68513\n",
            "[0]\tvalidation_0-mae:12913.25087\n",
            "[1]\tvalidation_0-mae:11057.68721\n",
            "[2]\tvalidation_0-mae:9444.76149\n",
            "[3]\tvalidation_0-mae:8072.98007\n",
            "[4]\tvalidation_0-mae:6921.14358\n",
            "[5]\tvalidation_0-mae:5936.29914\n",
            "[6]\tvalidation_0-mae:5101.92087\n",
            "[7]\tvalidation_0-mae:4398.00891\n",
            "[8]\tvalidation_0-mae:3803.31619\n",
            "[9]\tvalidation_0-mae:3308.38510\n",
            "[10]\tvalidation_0-mae:2890.85259\n",
            "[11]\tvalidation_0-mae:2544.55060\n",
            "[12]\tvalidation_0-mae:2257.88078\n",
            "[13]\tvalidation_0-mae:2025.29728\n",
            "[14]\tvalidation_0-mae:1834.83731\n",
            "[15]\tvalidation_0-mae:1688.09211\n",
            "[16]\tvalidation_0-mae:1556.97154\n",
            "[17]\tvalidation_0-mae:1451.31388\n",
            "[18]\tvalidation_0-mae:1376.07417\n",
            "[19]\tvalidation_0-mae:1304.98715\n",
            "[20]\tvalidation_0-mae:1248.33737\n",
            "[21]\tvalidation_0-mae:1209.21994\n",
            "[22]\tvalidation_0-mae:1177.88972\n",
            "[23]\tvalidation_0-mae:1141.63069\n",
            "[24]\tvalidation_0-mae:1110.98688\n",
            "[25]\tvalidation_0-mae:1088.74740\n",
            "[26]\tvalidation_0-mae:1066.18912\n",
            "[27]\tvalidation_0-mae:1046.88320\n",
            "[28]\tvalidation_0-mae:1033.42116\n",
            "[29]\tvalidation_0-mae:1019.79004\n",
            "[30]\tvalidation_0-mae:1007.61371\n",
            "[31]\tvalidation_0-mae:999.15126\n",
            "[32]\tvalidation_0-mae:989.18960\n",
            "[33]\tvalidation_0-mae:980.63613\n",
            "[34]\tvalidation_0-mae:974.06828\n",
            "[35]\tvalidation_0-mae:969.57852\n",
            "[36]\tvalidation_0-mae:966.03156\n",
            "[37]\tvalidation_0-mae:959.91452\n",
            "[38]\tvalidation_0-mae:953.63596\n",
            "[39]\tvalidation_0-mae:946.43637\n",
            "[40]\tvalidation_0-mae:943.26959\n",
            "[41]\tvalidation_0-mae:938.96319\n",
            "[42]\tvalidation_0-mae:937.15713\n",
            "[43]\tvalidation_0-mae:929.77681\n",
            "[44]\tvalidation_0-mae:923.46903\n",
            "[45]\tvalidation_0-mae:920.41309\n",
            "[46]\tvalidation_0-mae:917.71630\n",
            "[47]\tvalidation_0-mae:913.43449\n",
            "[48]\tvalidation_0-mae:910.26243\n",
            "[49]\tvalidation_0-mae:908.25319\n",
            "[50]\tvalidation_0-mae:899.94841\n",
            "[51]\tvalidation_0-mae:893.72182\n",
            "[52]\tvalidation_0-mae:892.80591\n",
            "[53]\tvalidation_0-mae:889.24153\n",
            "[54]\tvalidation_0-mae:887.91646\n",
            "[55]\tvalidation_0-mae:887.03137\n",
            "[56]\tvalidation_0-mae:881.46549\n",
            "[57]\tvalidation_0-mae:879.55283\n",
            "[58]\tvalidation_0-mae:878.67508\n",
            "[59]\tvalidation_0-mae:872.68041\n",
            "[60]\tvalidation_0-mae:869.53233\n",
            "[61]\tvalidation_0-mae:866.52759\n",
            "[62]\tvalidation_0-mae:863.83299\n",
            "[63]\tvalidation_0-mae:859.13800\n",
            "[64]\tvalidation_0-mae:857.26500\n",
            "[65]\tvalidation_0-mae:856.15426\n",
            "[66]\tvalidation_0-mae:854.54703\n",
            "[67]\tvalidation_0-mae:853.79883\n",
            "[68]\tvalidation_0-mae:849.33335\n",
            "[69]\tvalidation_0-mae:848.77428\n",
            "[70]\tvalidation_0-mae:845.79032\n",
            "[71]\tvalidation_0-mae:840.25649\n",
            "[72]\tvalidation_0-mae:839.69155\n",
            "[73]\tvalidation_0-mae:838.46588\n",
            "[74]\tvalidation_0-mae:837.64020\n",
            "[75]\tvalidation_0-mae:834.40075\n",
            "[76]\tvalidation_0-mae:833.34597\n",
            "[77]\tvalidation_0-mae:832.28498\n",
            "[78]\tvalidation_0-mae:831.28022\n",
            "[79]\tvalidation_0-mae:825.46150\n",
            "[80]\tvalidation_0-mae:824.02320\n",
            "[81]\tvalidation_0-mae:818.59963\n",
            "[82]\tvalidation_0-mae:817.60639\n",
            "[83]\tvalidation_0-mae:814.03750\n",
            "[84]\tvalidation_0-mae:812.93833\n",
            "[85]\tvalidation_0-mae:811.63240\n",
            "[86]\tvalidation_0-mae:806.94371\n",
            "[87]\tvalidation_0-mae:806.17453\n",
            "[88]\tvalidation_0-mae:803.42029\n",
            "[89]\tvalidation_0-mae:802.66158\n",
            "[90]\tvalidation_0-mae:799.23486\n",
            "[91]\tvalidation_0-mae:796.95674\n",
            "[92]\tvalidation_0-mae:794.38507\n",
            "[93]\tvalidation_0-mae:793.20803\n",
            "[94]\tvalidation_0-mae:792.32144\n",
            "[95]\tvalidation_0-mae:791.91212\n",
            "[96]\tvalidation_0-mae:787.12106\n",
            "[97]\tvalidation_0-mae:784.94944\n",
            "[98]\tvalidation_0-mae:783.67935\n",
            "[99]\tvalidation_0-mae:783.25696\n",
            "[100]\tvalidation_0-mae:782.37649\n",
            "[101]\tvalidation_0-mae:782.20693\n",
            "[102]\tvalidation_0-mae:781.75326\n",
            "[103]\tvalidation_0-mae:781.79006\n",
            "[104]\tvalidation_0-mae:781.78342\n",
            "[105]\tvalidation_0-mae:780.18692\n",
            "[106]\tvalidation_0-mae:779.68749\n",
            "[107]\tvalidation_0-mae:774.80951\n",
            "[108]\tvalidation_0-mae:773.95824\n",
            "[109]\tvalidation_0-mae:773.29232\n",
            "[110]\tvalidation_0-mae:772.04691\n",
            "[111]\tvalidation_0-mae:771.83444\n",
            "[112]\tvalidation_0-mae:771.53687\n",
            "[113]\tvalidation_0-mae:771.40863\n",
            "[114]\tvalidation_0-mae:770.84925\n",
            "[115]\tvalidation_0-mae:769.78165\n",
            "[116]\tvalidation_0-mae:767.71318\n",
            "[117]\tvalidation_0-mae:764.73925\n",
            "[118]\tvalidation_0-mae:764.15268\n",
            "[119]\tvalidation_0-mae:762.39458\n",
            "[120]\tvalidation_0-mae:761.79648\n",
            "[121]\tvalidation_0-mae:761.60365\n",
            "[122]\tvalidation_0-mae:760.79888\n",
            "[123]\tvalidation_0-mae:760.43166\n",
            "[124]\tvalidation_0-mae:756.49355\n",
            "[125]\tvalidation_0-mae:754.10041\n",
            "[126]\tvalidation_0-mae:753.88039\n",
            "[127]\tvalidation_0-mae:753.41068\n",
            "[128]\tvalidation_0-mae:748.89034\n",
            "[129]\tvalidation_0-mae:745.54204\n",
            "[130]\tvalidation_0-mae:743.90505\n",
            "[131]\tvalidation_0-mae:743.28972\n",
            "[132]\tvalidation_0-mae:740.94789\n",
            "[133]\tvalidation_0-mae:738.51850\n",
            "[134]\tvalidation_0-mae:735.08889\n",
            "[135]\tvalidation_0-mae:734.80270\n",
            "[136]\tvalidation_0-mae:734.59971\n",
            "[137]\tvalidation_0-mae:734.36065\n",
            "[138]\tvalidation_0-mae:733.21120\n",
            "[139]\tvalidation_0-mae:729.49631\n",
            "[140]\tvalidation_0-mae:728.75231\n",
            "[141]\tvalidation_0-mae:728.30730\n",
            "[142]\tvalidation_0-mae:726.34267\n",
            "[143]\tvalidation_0-mae:725.89597\n",
            "[144]\tvalidation_0-mae:725.36818\n",
            "[145]\tvalidation_0-mae:721.83524\n",
            "[146]\tvalidation_0-mae:719.51606\n",
            "[147]\tvalidation_0-mae:719.23086\n",
            "[148]\tvalidation_0-mae:718.32821\n",
            "[149]\tvalidation_0-mae:717.92206\n",
            "[150]\tvalidation_0-mae:715.14885\n",
            "[151]\tvalidation_0-mae:714.92066\n",
            "[152]\tvalidation_0-mae:713.50469\n",
            "[153]\tvalidation_0-mae:713.11980\n",
            "[154]\tvalidation_0-mae:712.50011\n",
            "[155]\tvalidation_0-mae:712.09559\n",
            "[156]\tvalidation_0-mae:711.85842\n",
            "[157]\tvalidation_0-mae:710.86202\n",
            "[158]\tvalidation_0-mae:710.57948\n",
            "[159]\tvalidation_0-mae:710.37816\n",
            "[160]\tvalidation_0-mae:707.63791\n",
            "[161]\tvalidation_0-mae:707.42336\n",
            "[162]\tvalidation_0-mae:705.67357\n",
            "[163]\tvalidation_0-mae:702.69453\n",
            "[164]\tvalidation_0-mae:702.06904\n",
            "[165]\tvalidation_0-mae:698.92304\n",
            "[166]\tvalidation_0-mae:698.51505\n",
            "[167]\tvalidation_0-mae:698.11417\n",
            "[168]\tvalidation_0-mae:697.92663\n",
            "[169]\tvalidation_0-mae:697.57425\n",
            "[170]\tvalidation_0-mae:697.39427\n",
            "[171]\tvalidation_0-mae:697.26461\n",
            "[172]\tvalidation_0-mae:693.81901\n",
            "[173]\tvalidation_0-mae:693.31968\n",
            "[174]\tvalidation_0-mae:692.26595\n",
            "[175]\tvalidation_0-mae:691.68064\n",
            "[176]\tvalidation_0-mae:689.60226\n",
            "[177]\tvalidation_0-mae:689.60197\n",
            "[178]\tvalidation_0-mae:689.26825\n",
            "[179]\tvalidation_0-mae:687.58307\n",
            "[180]\tvalidation_0-mae:687.41297\n",
            "[181]\tvalidation_0-mae:685.55976\n",
            "[182]\tvalidation_0-mae:685.27828\n",
            "[183]\tvalidation_0-mae:685.21443\n",
            "[184]\tvalidation_0-mae:685.04669\n",
            "[185]\tvalidation_0-mae:684.88217\n",
            "[186]\tvalidation_0-mae:684.13228\n",
            "[187]\tvalidation_0-mae:683.98748\n",
            "[188]\tvalidation_0-mae:683.87185\n",
            "[189]\tvalidation_0-mae:683.41647\n",
            "[190]\tvalidation_0-mae:683.28841\n",
            "[191]\tvalidation_0-mae:683.13130\n",
            "[192]\tvalidation_0-mae:681.79608\n",
            "[193]\tvalidation_0-mae:678.78292\n",
            "[194]\tvalidation_0-mae:677.39643\n",
            "[195]\tvalidation_0-mae:677.12701\n",
            "[196]\tvalidation_0-mae:676.77463\n",
            "[197]\tvalidation_0-mae:676.66187\n",
            "[198]\tvalidation_0-mae:674.88885\n",
            "[199]\tvalidation_0-mae:674.44366\n",
            "[200]\tvalidation_0-mae:674.24133\n",
            "[201]\tvalidation_0-mae:674.17114\n",
            "[202]\tvalidation_0-mae:673.69665\n",
            "[203]\tvalidation_0-mae:673.59987\n",
            "[204]\tvalidation_0-mae:672.57169\n",
            "[205]\tvalidation_0-mae:672.40005\n",
            "[206]\tvalidation_0-mae:672.05978\n",
            "[207]\tvalidation_0-mae:671.66240\n",
            "[208]\tvalidation_0-mae:671.60971\n",
            "[209]\tvalidation_0-mae:671.29454\n",
            "[210]\tvalidation_0-mae:670.80636\n",
            "[211]\tvalidation_0-mae:668.37180\n",
            "[212]\tvalidation_0-mae:668.13786\n",
            "[213]\tvalidation_0-mae:667.60005\n",
            "[214]\tvalidation_0-mae:666.95437\n",
            "[215]\tvalidation_0-mae:665.27772\n",
            "[216]\tvalidation_0-mae:665.10021\n",
            "[217]\tvalidation_0-mae:664.64321\n",
            "[218]\tvalidation_0-mae:664.49182\n",
            "[219]\tvalidation_0-mae:663.48926\n",
            "[220]\tvalidation_0-mae:662.23384\n",
            "[221]\tvalidation_0-mae:659.96271\n",
            "[222]\tvalidation_0-mae:659.80165\n",
            "[223]\tvalidation_0-mae:659.33921\n",
            "[224]\tvalidation_0-mae:659.23216\n",
            "[225]\tvalidation_0-mae:658.77140\n",
            "[226]\tvalidation_0-mae:658.60886\n",
            "[227]\tvalidation_0-mae:657.29028\n",
            "[228]\tvalidation_0-mae:657.15680\n",
            "[229]\tvalidation_0-mae:656.95208\n",
            "[230]\tvalidation_0-mae:655.04028\n",
            "[231]\tvalidation_0-mae:654.96590\n",
            "[232]\tvalidation_0-mae:654.93037\n",
            "[233]\tvalidation_0-mae:654.59556\n",
            "[234]\tvalidation_0-mae:653.78138\n",
            "[235]\tvalidation_0-mae:653.75806\n",
            "[236]\tvalidation_0-mae:653.70919\n",
            "[237]\tvalidation_0-mae:652.60967\n",
            "[238]\tvalidation_0-mae:652.49212\n",
            "[239]\tvalidation_0-mae:652.04367\n",
            "[240]\tvalidation_0-mae:651.87038\n",
            "[241]\tvalidation_0-mae:651.40516\n",
            "[242]\tvalidation_0-mae:651.19240\n",
            "[243]\tvalidation_0-mae:650.16748\n",
            "[244]\tvalidation_0-mae:650.10758\n",
            "[245]\tvalidation_0-mae:649.35027\n",
            "[246]\tvalidation_0-mae:648.14165\n",
            "[247]\tvalidation_0-mae:646.36283\n",
            "[248]\tvalidation_0-mae:646.27075\n",
            "[249]\tvalidation_0-mae:645.27628\n",
            "[250]\tvalidation_0-mae:645.15994\n",
            "[251]\tvalidation_0-mae:645.01002\n",
            "[252]\tvalidation_0-mae:644.65746\n",
            "[253]\tvalidation_0-mae:643.24096\n",
            "[254]\tvalidation_0-mae:643.19900\n",
            "[255]\tvalidation_0-mae:642.65069\n",
            "[256]\tvalidation_0-mae:641.71445\n",
            "[257]\tvalidation_0-mae:639.30130\n",
            "[258]\tvalidation_0-mae:639.08673\n",
            "[259]\tvalidation_0-mae:638.52429\n",
            "[260]\tvalidation_0-mae:638.23376\n",
            "[261]\tvalidation_0-mae:638.02509\n",
            "[262]\tvalidation_0-mae:636.99773\n",
            "[263]\tvalidation_0-mae:636.82471\n",
            "[264]\tvalidation_0-mae:635.76255\n",
            "[265]\tvalidation_0-mae:633.91004\n",
            "[266]\tvalidation_0-mae:633.85575\n",
            "[267]\tvalidation_0-mae:633.33079\n",
            "[268]\tvalidation_0-mae:633.25573\n",
            "[269]\tvalidation_0-mae:633.02003\n",
            "[270]\tvalidation_0-mae:632.92675\n",
            "[271]\tvalidation_0-mae:631.46776\n",
            "[272]\tvalidation_0-mae:630.99930\n",
            "[273]\tvalidation_0-mae:630.14863\n",
            "[274]\tvalidation_0-mae:629.94670\n",
            "[275]\tvalidation_0-mae:628.92156\n",
            "[276]\tvalidation_0-mae:628.64412\n",
            "[277]\tvalidation_0-mae:628.55271\n",
            "[278]\tvalidation_0-mae:628.60622\n",
            "[279]\tvalidation_0-mae:627.85683\n",
            "[280]\tvalidation_0-mae:627.68654\n",
            "[281]\tvalidation_0-mae:627.61841\n",
            "[282]\tvalidation_0-mae:627.64506\n",
            "[283]\tvalidation_0-mae:627.34413\n",
            "[284]\tvalidation_0-mae:625.52422\n",
            "[285]\tvalidation_0-mae:624.95863\n",
            "[286]\tvalidation_0-mae:624.51560\n",
            "[287]\tvalidation_0-mae:624.16443\n",
            "[288]\tvalidation_0-mae:623.24769\n",
            "[289]\tvalidation_0-mae:622.70013\n",
            "[290]\tvalidation_0-mae:622.44406\n",
            "[291]\tvalidation_0-mae:621.89745\n",
            "[292]\tvalidation_0-mae:621.64307\n",
            "[293]\tvalidation_0-mae:621.36687\n",
            "[294]\tvalidation_0-mae:620.45737\n",
            "[295]\tvalidation_0-mae:620.28886\n",
            "[296]\tvalidation_0-mae:619.12485\n",
            "[297]\tvalidation_0-mae:619.02421\n",
            "[298]\tvalidation_0-mae:618.99151\n",
            "[299]\tvalidation_0-mae:618.86909\n",
            "[300]\tvalidation_0-mae:618.75248\n",
            "[301]\tvalidation_0-mae:618.70036\n",
            "[302]\tvalidation_0-mae:617.10689\n",
            "[303]\tvalidation_0-mae:617.01805\n",
            "[304]\tvalidation_0-mae:616.98246\n",
            "[305]\tvalidation_0-mae:617.00275\n",
            "[306]\tvalidation_0-mae:616.97990\n",
            "[307]\tvalidation_0-mae:616.75170\n",
            "[308]\tvalidation_0-mae:614.83558\n",
            "[309]\tvalidation_0-mae:614.78024\n",
            "[310]\tvalidation_0-mae:614.33316\n",
            "[311]\tvalidation_0-mae:614.18569\n",
            "[312]\tvalidation_0-mae:613.59433\n",
            "[313]\tvalidation_0-mae:613.45785\n",
            "[314]\tvalidation_0-mae:613.11698\n",
            "[315]\tvalidation_0-mae:612.81361\n",
            "[316]\tvalidation_0-mae:612.65965\n",
            "[317]\tvalidation_0-mae:611.43097\n",
            "[318]\tvalidation_0-mae:610.65384\n",
            "[319]\tvalidation_0-mae:610.54998\n",
            "[320]\tvalidation_0-mae:610.39855\n",
            "[321]\tvalidation_0-mae:609.75621\n",
            "[322]\tvalidation_0-mae:609.49670\n",
            "[323]\tvalidation_0-mae:609.10298\n",
            "[324]\tvalidation_0-mae:608.99130\n",
            "[325]\tvalidation_0-mae:608.88998\n",
            "[326]\tvalidation_0-mae:608.74484\n",
            "[327]\tvalidation_0-mae:608.02840\n",
            "[328]\tvalidation_0-mae:607.88075\n",
            "[329]\tvalidation_0-mae:607.91156\n",
            "[330]\tvalidation_0-mae:607.74700\n",
            "[331]\tvalidation_0-mae:607.10369\n",
            "[332]\tvalidation_0-mae:606.66037\n",
            "[333]\tvalidation_0-mae:606.62900\n",
            "[334]\tvalidation_0-mae:605.92362\n",
            "[335]\tvalidation_0-mae:605.82565\n",
            "[336]\tvalidation_0-mae:605.72067\n",
            "[337]\tvalidation_0-mae:604.16645\n",
            "[338]\tvalidation_0-mae:603.53377\n",
            "[339]\tvalidation_0-mae:602.92930\n",
            "[340]\tvalidation_0-mae:602.85951\n",
            "[341]\tvalidation_0-mae:601.78813\n",
            "[342]\tvalidation_0-mae:601.71677\n",
            "[343]\tvalidation_0-mae:601.50432\n",
            "[344]\tvalidation_0-mae:601.38853\n",
            "[345]\tvalidation_0-mae:601.40983\n",
            "[346]\tvalidation_0-mae:600.75666\n",
            "[347]\tvalidation_0-mae:600.55113\n",
            "[348]\tvalidation_0-mae:600.25228\n",
            "[349]\tvalidation_0-mae:600.17777\n",
            "[350]\tvalidation_0-mae:600.12323\n",
            "[351]\tvalidation_0-mae:599.56897\n",
            "[352]\tvalidation_0-mae:599.25437\n",
            "[353]\tvalidation_0-mae:599.26153\n",
            "[354]\tvalidation_0-mae:599.19678\n",
            "[355]\tvalidation_0-mae:598.87055\n",
            "[356]\tvalidation_0-mae:598.51675\n",
            "[357]\tvalidation_0-mae:598.15304\n",
            "[358]\tvalidation_0-mae:597.62228\n",
            "[359]\tvalidation_0-mae:597.02859\n",
            "[360]\tvalidation_0-mae:596.66204\n",
            "[361]\tvalidation_0-mae:596.66950\n",
            "[362]\tvalidation_0-mae:596.32110\n",
            "[363]\tvalidation_0-mae:594.69878\n",
            "[364]\tvalidation_0-mae:594.71421\n",
            "[365]\tvalidation_0-mae:594.73313\n",
            "[366]\tvalidation_0-mae:593.76531\n",
            "[367]\tvalidation_0-mae:593.60654\n",
            "[368]\tvalidation_0-mae:593.45880\n",
            "[369]\tvalidation_0-mae:593.43636\n",
            "[370]\tvalidation_0-mae:593.41535\n",
            "[371]\tvalidation_0-mae:593.28584\n",
            "[372]\tvalidation_0-mae:592.95984\n",
            "[373]\tvalidation_0-mae:592.68161\n",
            "[374]\tvalidation_0-mae:592.63288\n",
            "[375]\tvalidation_0-mae:592.08571\n",
            "[376]\tvalidation_0-mae:592.05394\n",
            "[377]\tvalidation_0-mae:591.93581\n",
            "[378]\tvalidation_0-mae:591.62492\n",
            "[379]\tvalidation_0-mae:591.55116\n",
            "[380]\tvalidation_0-mae:591.51125\n",
            "[381]\tvalidation_0-mae:591.45839\n",
            "[382]\tvalidation_0-mae:591.39253\n",
            "[383]\tvalidation_0-mae:591.01372\n",
            "[384]\tvalidation_0-mae:590.17936\n",
            "[385]\tvalidation_0-mae:590.26073\n",
            "[386]\tvalidation_0-mae:589.65615\n",
            "[387]\tvalidation_0-mae:589.53984\n",
            "[388]\tvalidation_0-mae:589.36551\n",
            "[389]\tvalidation_0-mae:589.08891\n",
            "[390]\tvalidation_0-mae:589.00609\n",
            "[391]\tvalidation_0-mae:588.86171\n",
            "[392]\tvalidation_0-mae:588.59798\n",
            "[393]\tvalidation_0-mae:588.58395\n",
            "[394]\tvalidation_0-mae:588.39641\n",
            "[395]\tvalidation_0-mae:588.28227\n",
            "[396]\tvalidation_0-mae:588.22820\n",
            "[397]\tvalidation_0-mae:587.53297\n",
            "[398]\tvalidation_0-mae:587.48888\n",
            "[399]\tvalidation_0-mae:587.51030\n",
            "[400]\tvalidation_0-mae:587.40589\n",
            "[401]\tvalidation_0-mae:587.37194\n",
            "[402]\tvalidation_0-mae:587.29241\n",
            "[403]\tvalidation_0-mae:587.20909\n",
            "[404]\tvalidation_0-mae:586.34115\n",
            "[405]\tvalidation_0-mae:585.75570\n",
            "[406]\tvalidation_0-mae:584.56311\n",
            "[407]\tvalidation_0-mae:584.50565\n",
            "[408]\tvalidation_0-mae:584.46629\n",
            "[409]\tvalidation_0-mae:584.11509\n",
            "[410]\tvalidation_0-mae:584.05131\n",
            "[411]\tvalidation_0-mae:583.96608\n",
            "[412]\tvalidation_0-mae:583.93996\n",
            "[413]\tvalidation_0-mae:583.88789\n",
            "[414]\tvalidation_0-mae:583.78822\n",
            "[415]\tvalidation_0-mae:582.92736\n",
            "[416]\tvalidation_0-mae:582.10030\n",
            "[417]\tvalidation_0-mae:581.68730\n",
            "[418]\tvalidation_0-mae:581.62412\n",
            "[419]\tvalidation_0-mae:581.26680\n",
            "[420]\tvalidation_0-mae:581.23639\n",
            "[421]\tvalidation_0-mae:581.20574\n",
            "[422]\tvalidation_0-mae:580.74799\n",
            "[423]\tvalidation_0-mae:580.57766\n",
            "[424]\tvalidation_0-mae:580.48395\n",
            "[425]\tvalidation_0-mae:580.44224\n",
            "[426]\tvalidation_0-mae:579.92802\n",
            "[427]\tvalidation_0-mae:579.54362\n",
            "[428]\tvalidation_0-mae:579.50620\n",
            "[429]\tvalidation_0-mae:579.39659\n",
            "[430]\tvalidation_0-mae:579.33133\n",
            "[431]\tvalidation_0-mae:579.26103\n",
            "[432]\tvalidation_0-mae:579.23674\n",
            "[433]\tvalidation_0-mae:578.95337\n",
            "[434]\tvalidation_0-mae:578.94710\n",
            "[435]\tvalidation_0-mae:579.00095\n",
            "[436]\tvalidation_0-mae:578.95975\n",
            "[437]\tvalidation_0-mae:578.86779\n",
            "[438]\tvalidation_0-mae:577.93309\n",
            "[439]\tvalidation_0-mae:577.68764\n",
            "[440]\tvalidation_0-mae:577.39108\n",
            "[441]\tvalidation_0-mae:577.38333\n",
            "[442]\tvalidation_0-mae:577.36143\n",
            "[443]\tvalidation_0-mae:577.18162\n",
            "[444]\tvalidation_0-mae:577.03196\n",
            "[445]\tvalidation_0-mae:577.00415\n",
            "[446]\tvalidation_0-mae:576.91288\n",
            "[447]\tvalidation_0-mae:576.82642\n",
            "[448]\tvalidation_0-mae:576.71951\n",
            "[449]\tvalidation_0-mae:576.68545\n",
            "[450]\tvalidation_0-mae:576.57160\n",
            "[451]\tvalidation_0-mae:576.46940\n",
            "[452]\tvalidation_0-mae:576.47611\n",
            "[453]\tvalidation_0-mae:575.43209\n",
            "[454]\tvalidation_0-mae:574.86265\n",
            "[455]\tvalidation_0-mae:574.84178\n",
            "[456]\tvalidation_0-mae:574.09979\n",
            "[457]\tvalidation_0-mae:574.08433\n",
            "[458]\tvalidation_0-mae:573.83950\n",
            "[459]\tvalidation_0-mae:573.84580\n",
            "[460]\tvalidation_0-mae:573.70218\n",
            "[461]\tvalidation_0-mae:572.93466\n",
            "[462]\tvalidation_0-mae:572.89967\n",
            "[463]\tvalidation_0-mae:572.85467\n",
            "[464]\tvalidation_0-mae:572.22150\n",
            "[465]\tvalidation_0-mae:572.20279\n",
            "[466]\tvalidation_0-mae:572.07195\n",
            "[467]\tvalidation_0-mae:572.03579\n",
            "[468]\tvalidation_0-mae:572.06160\n",
            "[469]\tvalidation_0-mae:572.02488\n",
            "[470]\tvalidation_0-mae:572.01717\n",
            "[471]\tvalidation_0-mae:571.95711\n",
            "[472]\tvalidation_0-mae:571.91158\n",
            "[473]\tvalidation_0-mae:571.70394\n",
            "[474]\tvalidation_0-mae:571.58867\n",
            "[475]\tvalidation_0-mae:571.41381\n",
            "[476]\tvalidation_0-mae:570.99971\n",
            "[477]\tvalidation_0-mae:570.87604\n",
            "[478]\tvalidation_0-mae:570.80447\n",
            "[479]\tvalidation_0-mae:570.33837\n",
            "[480]\tvalidation_0-mae:569.54650\n",
            "[481]\tvalidation_0-mae:569.52254\n",
            "[482]\tvalidation_0-mae:569.01489\n",
            "[483]\tvalidation_0-mae:568.89244\n",
            "[484]\tvalidation_0-mae:568.44036\n",
            "[485]\tvalidation_0-mae:568.46088\n",
            "[486]\tvalidation_0-mae:568.45629\n",
            "[487]\tvalidation_0-mae:568.47459\n",
            "[488]\tvalidation_0-mae:568.36878\n",
            "[489]\tvalidation_0-mae:568.31004\n",
            "[490]\tvalidation_0-mae:568.27378\n",
            "[491]\tvalidation_0-mae:568.12306\n",
            "[492]\tvalidation_0-mae:568.02107\n",
            "[493]\tvalidation_0-mae:567.79716\n",
            "[494]\tvalidation_0-mae:567.77591\n",
            "[495]\tvalidation_0-mae:567.47838\n",
            "[496]\tvalidation_0-mae:567.27429\n",
            "[497]\tvalidation_0-mae:567.26297\n",
            "[498]\tvalidation_0-mae:567.17897\n",
            "[499]\tvalidation_0-mae:566.97472\n",
            "[500]\tvalidation_0-mae:566.95162\n",
            "[501]\tvalidation_0-mae:566.69475\n",
            "[502]\tvalidation_0-mae:566.65615\n",
            "[503]\tvalidation_0-mae:566.17406\n",
            "[504]\tvalidation_0-mae:566.09826\n",
            "[505]\tvalidation_0-mae:565.89060\n",
            "[506]\tvalidation_0-mae:565.77027\n",
            "[507]\tvalidation_0-mae:565.36983\n",
            "[508]\tvalidation_0-mae:565.27181\n",
            "[509]\tvalidation_0-mae:565.28756\n",
            "[510]\tvalidation_0-mae:565.18173\n",
            "[511]\tvalidation_0-mae:565.14927\n",
            "[512]\tvalidation_0-mae:564.49776\n",
            "[513]\tvalidation_0-mae:564.49761\n",
            "[514]\tvalidation_0-mae:563.85612\n",
            "[515]\tvalidation_0-mae:563.87349\n",
            "[516]\tvalidation_0-mae:563.87419\n",
            "[517]\tvalidation_0-mae:563.83926\n",
            "[518]\tvalidation_0-mae:563.85903\n",
            "[519]\tvalidation_0-mae:563.70180\n",
            "[520]\tvalidation_0-mae:563.45013\n",
            "[521]\tvalidation_0-mae:563.32517\n",
            "[522]\tvalidation_0-mae:563.28092\n",
            "[523]\tvalidation_0-mae:563.27040\n",
            "[524]\tvalidation_0-mae:563.24031\n",
            "[525]\tvalidation_0-mae:562.78988\n",
            "[526]\tvalidation_0-mae:562.67551\n",
            "[527]\tvalidation_0-mae:562.62390\n",
            "[528]\tvalidation_0-mae:562.34533\n",
            "[529]\tvalidation_0-mae:562.30485\n",
            "[530]\tvalidation_0-mae:562.23872\n",
            "[531]\tvalidation_0-mae:562.19783\n",
            "[532]\tvalidation_0-mae:562.14848\n",
            "[533]\tvalidation_0-mae:562.04630\n",
            "[534]\tvalidation_0-mae:561.95247\n",
            "[535]\tvalidation_0-mae:561.96044\n",
            "[536]\tvalidation_0-mae:561.86663\n",
            "[537]\tvalidation_0-mae:561.25959\n",
            "[538]\tvalidation_0-mae:561.11014\n",
            "[539]\tvalidation_0-mae:561.06246\n",
            "[540]\tvalidation_0-mae:560.86195\n",
            "[541]\tvalidation_0-mae:560.68823\n",
            "[542]\tvalidation_0-mae:560.70491\n",
            "[543]\tvalidation_0-mae:560.72274\n",
            "[544]\tvalidation_0-mae:560.62261\n",
            "[545]\tvalidation_0-mae:560.19477\n",
            "[546]\tvalidation_0-mae:560.00369\n",
            "[547]\tvalidation_0-mae:559.89159\n",
            "[548]\tvalidation_0-mae:559.63485\n",
            "[549]\tvalidation_0-mae:559.61895\n",
            "[550]\tvalidation_0-mae:559.60541\n",
            "[551]\tvalidation_0-mae:559.55711\n",
            "[552]\tvalidation_0-mae:558.88032\n",
            "[553]\tvalidation_0-mae:558.80273\n",
            "[554]\tvalidation_0-mae:558.79125\n",
            "[555]\tvalidation_0-mae:558.77916\n",
            "[556]\tvalidation_0-mae:558.75387\n",
            "[557]\tvalidation_0-mae:558.71388\n",
            "[558]\tvalidation_0-mae:558.53440\n",
            "[559]\tvalidation_0-mae:558.49290\n",
            "[560]\tvalidation_0-mae:558.39049\n",
            "[561]\tvalidation_0-mae:557.68569\n",
            "[562]\tvalidation_0-mae:557.40249\n",
            "[563]\tvalidation_0-mae:557.32990\n",
            "[564]\tvalidation_0-mae:557.06886\n",
            "[565]\tvalidation_0-mae:557.05098\n",
            "[566]\tvalidation_0-mae:557.08726\n",
            "[567]\tvalidation_0-mae:556.94647\n",
            "[568]\tvalidation_0-mae:556.86135\n",
            "[569]\tvalidation_0-mae:556.87561\n",
            "[570]\tvalidation_0-mae:556.83352\n",
            "[571]\tvalidation_0-mae:556.66615\n",
            "[572]\tvalidation_0-mae:556.46218\n",
            "[573]\tvalidation_0-mae:556.23498\n",
            "[574]\tvalidation_0-mae:556.17641\n",
            "[575]\tvalidation_0-mae:556.19341\n",
            "[576]\tvalidation_0-mae:556.18622\n",
            "[577]\tvalidation_0-mae:556.12466\n",
            "[578]\tvalidation_0-mae:555.50538\n",
            "[579]\tvalidation_0-mae:555.45234\n",
            "[580]\tvalidation_0-mae:555.34677\n",
            "[581]\tvalidation_0-mae:554.63193\n",
            "[582]\tvalidation_0-mae:554.58941\n",
            "[583]\tvalidation_0-mae:554.49464\n",
            "[584]\tvalidation_0-mae:554.29039\n",
            "[585]\tvalidation_0-mae:554.16695\n",
            "[586]\tvalidation_0-mae:553.88811\n",
            "[587]\tvalidation_0-mae:553.86139\n",
            "[588]\tvalidation_0-mae:553.40342\n",
            "[589]\tvalidation_0-mae:553.42885\n",
            "[590]\tvalidation_0-mae:553.44403\n",
            "[591]\tvalidation_0-mae:553.34936\n",
            "[592]\tvalidation_0-mae:552.91394\n",
            "[593]\tvalidation_0-mae:552.79463\n",
            "[594]\tvalidation_0-mae:552.66350\n",
            "[595]\tvalidation_0-mae:552.57940\n",
            "[596]\tvalidation_0-mae:552.58298\n",
            "[597]\tvalidation_0-mae:552.61007\n",
            "[598]\tvalidation_0-mae:552.52395\n",
            "[599]\tvalidation_0-mae:552.46393\n",
            "[600]\tvalidation_0-mae:552.26383\n",
            "[601]\tvalidation_0-mae:552.31552\n",
            "[602]\tvalidation_0-mae:552.15815\n",
            "[603]\tvalidation_0-mae:552.12557\n",
            "[604]\tvalidation_0-mae:552.08636\n",
            "[605]\tvalidation_0-mae:552.03906\n",
            "[606]\tvalidation_0-mae:551.99539\n",
            "[607]\tvalidation_0-mae:551.95314\n",
            "[608]\tvalidation_0-mae:551.87115\n",
            "[609]\tvalidation_0-mae:551.82038\n",
            "[610]\tvalidation_0-mae:551.75644\n",
            "[611]\tvalidation_0-mae:551.73800\n",
            "[612]\tvalidation_0-mae:551.11722\n",
            "[613]\tvalidation_0-mae:551.08263\n",
            "[614]\tvalidation_0-mae:550.99534\n",
            "[615]\tvalidation_0-mae:550.97374\n",
            "[616]\tvalidation_0-mae:550.89773\n",
            "[617]\tvalidation_0-mae:550.74647\n",
            "[618]\tvalidation_0-mae:550.39696\n",
            "[619]\tvalidation_0-mae:550.17538\n",
            "[620]\tvalidation_0-mae:550.18169\n",
            "[621]\tvalidation_0-mae:549.87126\n",
            "[622]\tvalidation_0-mae:549.78722\n",
            "[623]\tvalidation_0-mae:549.78280\n",
            "[624]\tvalidation_0-mae:549.81505\n",
            "[625]\tvalidation_0-mae:549.63437\n",
            "[626]\tvalidation_0-mae:549.64132\n",
            "[627]\tvalidation_0-mae:549.58556\n",
            "[628]\tvalidation_0-mae:549.53414\n",
            "[629]\tvalidation_0-mae:549.47374\n",
            "[630]\tvalidation_0-mae:549.39981\n",
            "[631]\tvalidation_0-mae:549.38309\n",
            "[632]\tvalidation_0-mae:549.34469\n",
            "[633]\tvalidation_0-mae:549.39950\n",
            "[634]\tvalidation_0-mae:549.20357\n",
            "[635]\tvalidation_0-mae:549.15058\n",
            "[636]\tvalidation_0-mae:549.06208\n",
            "[637]\tvalidation_0-mae:549.03273\n",
            "[638]\tvalidation_0-mae:549.01685\n",
            "[639]\tvalidation_0-mae:548.99006\n",
            "[640]\tvalidation_0-mae:548.99546\n",
            "[641]\tvalidation_0-mae:549.04920\n",
            "[642]\tvalidation_0-mae:549.09093\n",
            "[643]\tvalidation_0-mae:549.05827\n",
            "[644]\tvalidation_0-mae:549.06224\n",
            "[645]\tvalidation_0-mae:549.01441\n",
            "[646]\tvalidation_0-mae:548.61528\n",
            "[647]\tvalidation_0-mae:548.38247\n",
            "[648]\tvalidation_0-mae:548.36240\n",
            "[649]\tvalidation_0-mae:547.75251\n",
            "[650]\tvalidation_0-mae:547.77227\n",
            "[651]\tvalidation_0-mae:547.69857\n",
            "[652]\tvalidation_0-mae:547.34150\n",
            "[653]\tvalidation_0-mae:547.33174\n",
            "[654]\tvalidation_0-mae:547.30141\n",
            "[655]\tvalidation_0-mae:547.27208\n",
            "[656]\tvalidation_0-mae:547.25227\n",
            "[657]\tvalidation_0-mae:547.16116\n",
            "[658]\tvalidation_0-mae:546.67220\n",
            "[659]\tvalidation_0-mae:546.53739\n",
            "[660]\tvalidation_0-mae:546.50350\n",
            "[661]\tvalidation_0-mae:546.52157\n",
            "[662]\tvalidation_0-mae:546.37809\n",
            "[663]\tvalidation_0-mae:546.28987\n",
            "[664]\tvalidation_0-mae:546.18094\n",
            "[665]\tvalidation_0-mae:545.82776\n",
            "[666]\tvalidation_0-mae:545.80159\n",
            "[667]\tvalidation_0-mae:545.80778\n",
            "[668]\tvalidation_0-mae:545.65606\n",
            "[669]\tvalidation_0-mae:545.61292\n",
            "[670]\tvalidation_0-mae:545.52246\n",
            "[671]\tvalidation_0-mae:545.42208\n",
            "[672]\tvalidation_0-mae:545.30295\n",
            "[673]\tvalidation_0-mae:545.28180\n",
            "[674]\tvalidation_0-mae:545.25493\n",
            "[675]\tvalidation_0-mae:545.19903\n",
            "[676]\tvalidation_0-mae:545.21662\n",
            "[677]\tvalidation_0-mae:545.22596\n",
            "[678]\tvalidation_0-mae:545.10781\n",
            "[679]\tvalidation_0-mae:545.05762\n",
            "[680]\tvalidation_0-mae:545.09410\n",
            "[681]\tvalidation_0-mae:545.06049\n",
            "[682]\tvalidation_0-mae:544.93302\n",
            "[683]\tvalidation_0-mae:544.72730\n",
            "[684]\tvalidation_0-mae:544.69246\n",
            "[685]\tvalidation_0-mae:544.67088\n",
            "[686]\tvalidation_0-mae:544.67008\n",
            "[687]\tvalidation_0-mae:544.55933\n",
            "[688]\tvalidation_0-mae:544.53568\n",
            "[689]\tvalidation_0-mae:544.53240\n",
            "[690]\tvalidation_0-mae:544.35636\n",
            "[691]\tvalidation_0-mae:544.38283\n",
            "[692]\tvalidation_0-mae:543.76285\n",
            "[693]\tvalidation_0-mae:543.74226\n",
            "[694]\tvalidation_0-mae:543.70156\n",
            "[695]\tvalidation_0-mae:543.71951\n",
            "[696]\tvalidation_0-mae:543.65600\n",
            "[697]\tvalidation_0-mae:543.64598\n",
            "[698]\tvalidation_0-mae:543.64697\n",
            "[699]\tvalidation_0-mae:543.58382\n",
            "[700]\tvalidation_0-mae:543.53297\n",
            "[701]\tvalidation_0-mae:543.41557\n",
            "[702]\tvalidation_0-mae:543.13093\n",
            "[703]\tvalidation_0-mae:543.08427\n",
            "[704]\tvalidation_0-mae:543.03197\n",
            "[705]\tvalidation_0-mae:542.96676\n",
            "[706]\tvalidation_0-mae:542.50851\n",
            "[707]\tvalidation_0-mae:542.24399\n",
            "[708]\tvalidation_0-mae:542.23754\n",
            "[709]\tvalidation_0-mae:542.19482\n",
            "[710]\tvalidation_0-mae:542.13701\n",
            "[711]\tvalidation_0-mae:541.83533\n",
            "[712]\tvalidation_0-mae:541.82314\n",
            "[713]\tvalidation_0-mae:541.81175\n",
            "[714]\tvalidation_0-mae:541.76391\n",
            "[715]\tvalidation_0-mae:541.69074\n",
            "[716]\tvalidation_0-mae:541.61163\n",
            "[717]\tvalidation_0-mae:541.55656\n",
            "[718]\tvalidation_0-mae:541.51388\n",
            "[719]\tvalidation_0-mae:541.51088\n",
            "[720]\tvalidation_0-mae:541.50059\n",
            "[721]\tvalidation_0-mae:541.35439\n",
            "[722]\tvalidation_0-mae:541.31453\n",
            "[723]\tvalidation_0-mae:541.33466\n",
            "[724]\tvalidation_0-mae:541.32420\n",
            "[725]\tvalidation_0-mae:541.08130\n",
            "[726]\tvalidation_0-mae:541.08075\n",
            "[727]\tvalidation_0-mae:541.07818\n",
            "[728]\tvalidation_0-mae:540.97935\n",
            "[729]\tvalidation_0-mae:540.95099\n",
            "[730]\tvalidation_0-mae:540.75315\n",
            "[731]\tvalidation_0-mae:540.69319\n",
            "[732]\tvalidation_0-mae:540.52340\n",
            "[733]\tvalidation_0-mae:540.50181\n",
            "[734]\tvalidation_0-mae:540.47777\n",
            "[735]\tvalidation_0-mae:540.50334\n",
            "[736]\tvalidation_0-mae:540.47895\n",
            "[737]\tvalidation_0-mae:540.44068\n",
            "[738]\tvalidation_0-mae:540.39607\n",
            "[739]\tvalidation_0-mae:540.40999\n",
            "[740]\tvalidation_0-mae:540.32898\n",
            "[741]\tvalidation_0-mae:540.30808\n",
            "[742]\tvalidation_0-mae:540.02455\n",
            "[743]\tvalidation_0-mae:540.04670\n",
            "[744]\tvalidation_0-mae:540.02090\n",
            "[745]\tvalidation_0-mae:539.89459\n",
            "[746]\tvalidation_0-mae:539.91852\n",
            "[747]\tvalidation_0-mae:539.88120\n",
            "[748]\tvalidation_0-mae:539.87379\n",
            "[749]\tvalidation_0-mae:539.47567\n",
            "[750]\tvalidation_0-mae:539.24675\n",
            "[751]\tvalidation_0-mae:539.25246\n",
            "[752]\tvalidation_0-mae:539.23329\n",
            "[753]\tvalidation_0-mae:539.19251\n",
            "[754]\tvalidation_0-mae:539.18455\n",
            "[755]\tvalidation_0-mae:539.19619\n",
            "[756]\tvalidation_0-mae:539.19080\n",
            "[757]\tvalidation_0-mae:539.17700\n",
            "[758]\tvalidation_0-mae:539.07717\n",
            "[759]\tvalidation_0-mae:539.12295\n",
            "[760]\tvalidation_0-mae:539.03980\n",
            "[761]\tvalidation_0-mae:539.04435\n",
            "[762]\tvalidation_0-mae:539.09659\n",
            "[763]\tvalidation_0-mae:539.07126\n",
            "[764]\tvalidation_0-mae:538.92211\n",
            "[765]\tvalidation_0-mae:538.55839\n",
            "[766]\tvalidation_0-mae:538.51735\n",
            "[767]\tvalidation_0-mae:538.50745\n",
            "[768]\tvalidation_0-mae:538.50524\n",
            "[769]\tvalidation_0-mae:538.48456\n",
            "[770]\tvalidation_0-mae:538.35550\n",
            "[771]\tvalidation_0-mae:538.25969\n",
            "[772]\tvalidation_0-mae:538.14420\n",
            "[773]\tvalidation_0-mae:537.90588\n",
            "[774]\tvalidation_0-mae:537.89231\n",
            "[775]\tvalidation_0-mae:537.83288\n",
            "[776]\tvalidation_0-mae:537.83081\n",
            "[777]\tvalidation_0-mae:537.83044\n",
            "[778]\tvalidation_0-mae:537.84640\n",
            "[779]\tvalidation_0-mae:537.83406\n",
            "[780]\tvalidation_0-mae:537.45635\n",
            "[781]\tvalidation_0-mae:537.47272\n",
            "[782]\tvalidation_0-mae:537.46432\n",
            "[783]\tvalidation_0-mae:537.47602\n",
            "[784]\tvalidation_0-mae:537.46214\n",
            "[785]\tvalidation_0-mae:537.42022\n",
            "[786]\tvalidation_0-mae:537.37205\n",
            "[787]\tvalidation_0-mae:537.34919\n",
            "[788]\tvalidation_0-mae:537.28945\n",
            "[789]\tvalidation_0-mae:537.27118\n",
            "[790]\tvalidation_0-mae:537.20038\n",
            "[791]\tvalidation_0-mae:537.11342\n",
            "[792]\tvalidation_0-mae:537.09279\n",
            "[793]\tvalidation_0-mae:537.08668\n",
            "[794]\tvalidation_0-mae:537.00546\n",
            "[795]\tvalidation_0-mae:537.02057\n",
            "[796]\tvalidation_0-mae:536.99458\n",
            "[797]\tvalidation_0-mae:536.96462\n",
            "[798]\tvalidation_0-mae:536.86592\n",
            "[799]\tvalidation_0-mae:536.82855\n",
            "[800]\tvalidation_0-mae:536.76732\n",
            "[801]\tvalidation_0-mae:536.71586\n",
            "[802]\tvalidation_0-mae:536.40786\n",
            "[803]\tvalidation_0-mae:536.40395\n",
            "[804]\tvalidation_0-mae:536.38769\n",
            "[805]\tvalidation_0-mae:536.33460\n",
            "[806]\tvalidation_0-mae:536.31956\n",
            "[807]\tvalidation_0-mae:536.33925\n",
            "[808]\tvalidation_0-mae:536.32385\n",
            "[809]\tvalidation_0-mae:536.30316\n",
            "[810]\tvalidation_0-mae:536.28849\n",
            "[811]\tvalidation_0-mae:536.29336\n",
            "[812]\tvalidation_0-mae:536.22671\n",
            "[813]\tvalidation_0-mae:536.11834\n",
            "[814]\tvalidation_0-mae:536.08492\n",
            "[815]\tvalidation_0-mae:536.03182\n",
            "[816]\tvalidation_0-mae:535.96757\n",
            "[817]\tvalidation_0-mae:535.96480\n",
            "[818]\tvalidation_0-mae:535.61832\n",
            "[819]\tvalidation_0-mae:535.60812\n",
            "[820]\tvalidation_0-mae:535.60002\n",
            "[821]\tvalidation_0-mae:535.60334\n",
            "[822]\tvalidation_0-mae:535.36508\n",
            "[823]\tvalidation_0-mae:535.35052\n",
            "[824]\tvalidation_0-mae:535.23041\n",
            "[825]\tvalidation_0-mae:535.22572\n",
            "[826]\tvalidation_0-mae:535.15518\n",
            "[827]\tvalidation_0-mae:535.09810\n",
            "[828]\tvalidation_0-mae:535.05012\n",
            "[829]\tvalidation_0-mae:535.01428\n",
            "[830]\tvalidation_0-mae:535.00885\n",
            "[831]\tvalidation_0-mae:535.02246\n",
            "[832]\tvalidation_0-mae:535.01763\n",
            "[833]\tvalidation_0-mae:535.02403\n",
            "[834]\tvalidation_0-mae:535.01397\n",
            "[835]\tvalidation_0-mae:535.03915\n",
            "[836]\tvalidation_0-mae:534.68442\n",
            "[837]\tvalidation_0-mae:534.24884\n",
            "[838]\tvalidation_0-mae:534.19971\n",
            "[839]\tvalidation_0-mae:534.21681\n",
            "[840]\tvalidation_0-mae:534.18664\n",
            "[841]\tvalidation_0-mae:534.17124\n",
            "[842]\tvalidation_0-mae:534.01370\n",
            "[843]\tvalidation_0-mae:534.00032\n",
            "[844]\tvalidation_0-mae:533.97690\n",
            "[845]\tvalidation_0-mae:533.96625\n",
            "[846]\tvalidation_0-mae:533.90953\n",
            "[847]\tvalidation_0-mae:533.91354\n",
            "[848]\tvalidation_0-mae:533.88052\n",
            "[849]\tvalidation_0-mae:533.76201\n",
            "[850]\tvalidation_0-mae:533.75799\n",
            "[851]\tvalidation_0-mae:533.71677\n",
            "[852]\tvalidation_0-mae:533.70823\n",
            "[853]\tvalidation_0-mae:533.54904\n",
            "[854]\tvalidation_0-mae:533.55185\n",
            "[855]\tvalidation_0-mae:533.50307\n",
            "[856]\tvalidation_0-mae:533.42240\n",
            "[857]\tvalidation_0-mae:533.40792\n",
            "[858]\tvalidation_0-mae:533.40282\n",
            "[859]\tvalidation_0-mae:533.38195\n",
            "[860]\tvalidation_0-mae:533.39812\n",
            "[861]\tvalidation_0-mae:533.38354\n",
            "[862]\tvalidation_0-mae:533.33422\n",
            "[863]\tvalidation_0-mae:533.31064\n",
            "[864]\tvalidation_0-mae:533.27451\n",
            "[865]\tvalidation_0-mae:533.25205\n",
            "[866]\tvalidation_0-mae:533.17440\n",
            "[867]\tvalidation_0-mae:533.00294\n",
            "[868]\tvalidation_0-mae:532.96008\n",
            "[869]\tvalidation_0-mae:532.97151\n",
            "[870]\tvalidation_0-mae:532.90670\n",
            "[871]\tvalidation_0-mae:532.91061\n",
            "[872]\tvalidation_0-mae:532.87226\n",
            "[873]\tvalidation_0-mae:532.83820\n",
            "[874]\tvalidation_0-mae:532.84353\n",
            "[875]\tvalidation_0-mae:532.84993\n",
            "[876]\tvalidation_0-mae:532.80805\n",
            "[877]\tvalidation_0-mae:532.74079\n",
            "[878]\tvalidation_0-mae:532.70855\n",
            "[879]\tvalidation_0-mae:532.71105\n",
            "[880]\tvalidation_0-mae:532.63671\n",
            "[881]\tvalidation_0-mae:532.64068\n",
            "[882]\tvalidation_0-mae:532.61455\n",
            "[883]\tvalidation_0-mae:532.52823\n",
            "[884]\tvalidation_0-mae:532.46977\n",
            "[885]\tvalidation_0-mae:532.45531\n",
            "[886]\tvalidation_0-mae:532.47223\n",
            "[887]\tvalidation_0-mae:532.44598\n",
            "[888]\tvalidation_0-mae:532.36406\n",
            "[889]\tvalidation_0-mae:532.22424\n",
            "[890]\tvalidation_0-mae:531.92582\n",
            "[891]\tvalidation_0-mae:531.94404\n",
            "[892]\tvalidation_0-mae:531.95821\n",
            "[893]\tvalidation_0-mae:531.94362\n",
            "[894]\tvalidation_0-mae:531.94926\n",
            "[895]\tvalidation_0-mae:531.90813\n",
            "[896]\tvalidation_0-mae:531.92350\n",
            "[897]\tvalidation_0-mae:531.81400\n",
            "[898]\tvalidation_0-mae:531.83444\n",
            "[899]\tvalidation_0-mae:531.84995\n",
            "[900]\tvalidation_0-mae:531.82618\n",
            "[901]\tvalidation_0-mae:531.60362\n",
            "[902]\tvalidation_0-mae:531.60829\n",
            "[903]\tvalidation_0-mae:531.48313\n",
            "[904]\tvalidation_0-mae:531.36222\n",
            "[905]\tvalidation_0-mae:531.31441\n",
            "[906]\tvalidation_0-mae:531.27199\n",
            "[907]\tvalidation_0-mae:531.26075\n",
            "[908]\tvalidation_0-mae:531.28482\n",
            "[909]\tvalidation_0-mae:531.19980\n",
            "[910]\tvalidation_0-mae:531.17687\n",
            "[911]\tvalidation_0-mae:531.05177\n",
            "[912]\tvalidation_0-mae:531.03109\n",
            "[913]\tvalidation_0-mae:530.88932\n",
            "[914]\tvalidation_0-mae:530.73539\n",
            "[915]\tvalidation_0-mae:530.68671\n",
            "[916]\tvalidation_0-mae:530.69243\n",
            "[917]\tvalidation_0-mae:530.62786\n",
            "[918]\tvalidation_0-mae:530.61934\n",
            "[919]\tvalidation_0-mae:530.62256\n",
            "[920]\tvalidation_0-mae:530.58369\n",
            "[921]\tvalidation_0-mae:530.31776\n",
            "[922]\tvalidation_0-mae:530.29265\n",
            "[923]\tvalidation_0-mae:530.23513\n",
            "[924]\tvalidation_0-mae:530.21785\n",
            "[925]\tvalidation_0-mae:530.17252\n",
            "[926]\tvalidation_0-mae:530.19603\n",
            "[927]\tvalidation_0-mae:530.10521\n",
            "[928]\tvalidation_0-mae:530.07227\n",
            "[929]\tvalidation_0-mae:529.95477\n",
            "[930]\tvalidation_0-mae:529.92374\n",
            "[931]\tvalidation_0-mae:529.92620\n",
            "[932]\tvalidation_0-mae:529.76991\n",
            "[933]\tvalidation_0-mae:529.73522\n",
            "[934]\tvalidation_0-mae:529.70770\n",
            "[935]\tvalidation_0-mae:529.72611\n",
            "[936]\tvalidation_0-mae:529.66750\n",
            "[937]\tvalidation_0-mae:529.61739\n",
            "[938]\tvalidation_0-mae:529.62663\n",
            "[939]\tvalidation_0-mae:529.56978\n",
            "[940]\tvalidation_0-mae:529.53428\n",
            "[941]\tvalidation_0-mae:529.37505\n",
            "[942]\tvalidation_0-mae:529.33869\n",
            "[943]\tvalidation_0-mae:529.33551\n",
            "[944]\tvalidation_0-mae:529.33775\n",
            "[945]\tvalidation_0-mae:529.33617\n",
            "[946]\tvalidation_0-mae:529.32242\n",
            "[947]\tvalidation_0-mae:529.30447\n",
            "[948]\tvalidation_0-mae:529.30637\n",
            "[949]\tvalidation_0-mae:529.24814\n",
            "[950]\tvalidation_0-mae:529.21827\n",
            "[951]\tvalidation_0-mae:529.22337\n",
            "[952]\tvalidation_0-mae:529.17234\n",
            "[953]\tvalidation_0-mae:529.16489\n",
            "[954]\tvalidation_0-mae:529.16817\n",
            "[955]\tvalidation_0-mae:529.18655\n",
            "[956]\tvalidation_0-mae:529.21733\n",
            "[957]\tvalidation_0-mae:529.21237\n",
            "[958]\tvalidation_0-mae:529.21813\n",
            "[959]\tvalidation_0-mae:529.21511\n",
            "[960]\tvalidation_0-mae:529.06062\n",
            "[961]\tvalidation_0-mae:529.03988\n",
            "[962]\tvalidation_0-mae:529.05292\n",
            "[963]\tvalidation_0-mae:529.01508\n",
            "[964]\tvalidation_0-mae:528.80954\n",
            "[965]\tvalidation_0-mae:528.81622\n",
            "[966]\tvalidation_0-mae:528.70942\n",
            "[967]\tvalidation_0-mae:528.69740\n",
            "[968]\tvalidation_0-mae:528.71050\n",
            "[969]\tvalidation_0-mae:528.71180\n",
            "[970]\tvalidation_0-mae:528.69948\n",
            "[971]\tvalidation_0-mae:528.70586\n",
            "[972]\tvalidation_0-mae:528.67812\n",
            "[973]\tvalidation_0-mae:528.64648\n",
            "[974]\tvalidation_0-mae:528.60777\n",
            "[975]\tvalidation_0-mae:528.49127\n",
            "[976]\tvalidation_0-mae:528.45216\n",
            "[977]\tvalidation_0-mae:528.45113\n",
            "[978]\tvalidation_0-mae:528.46632\n",
            "[979]\tvalidation_0-mae:528.44178\n",
            "[980]\tvalidation_0-mae:528.44136\n",
            "[981]\tvalidation_0-mae:528.43019\n",
            "[982]\tvalidation_0-mae:528.42910\n",
            "[983]\tvalidation_0-mae:528.33080\n",
            "[984]\tvalidation_0-mae:528.30955\n",
            "[985]\tvalidation_0-mae:528.21377\n",
            "[986]\tvalidation_0-mae:528.23757\n",
            "[987]\tvalidation_0-mae:527.99821\n",
            "[988]\tvalidation_0-mae:527.99422\n",
            "[989]\tvalidation_0-mae:527.98243\n",
            "[990]\tvalidation_0-mae:527.99209\n",
            "[991]\tvalidation_0-mae:527.97819\n",
            "[992]\tvalidation_0-mae:527.95147\n",
            "[993]\tvalidation_0-mae:527.93570\n",
            "[994]\tvalidation_0-mae:527.96811\n",
            "[995]\tvalidation_0-mae:527.96998\n",
            "[996]\tvalidation_0-mae:527.95463\n",
            "[997]\tvalidation_0-mae:527.92800\n",
            "[998]\tvalidation_0-mae:527.90235\n",
            "[999]\tvalidation_0-mae:527.91145\n",
            "[1000]\tvalidation_0-mae:527.84244\n",
            "[1001]\tvalidation_0-mae:527.83237\n",
            "[1002]\tvalidation_0-mae:527.83132\n",
            "[1003]\tvalidation_0-mae:527.80300\n",
            "[1004]\tvalidation_0-mae:527.68144\n",
            "[1005]\tvalidation_0-mae:527.68756\n",
            "[1006]\tvalidation_0-mae:527.71253\n",
            "[1007]\tvalidation_0-mae:527.72229\n",
            "[1008]\tvalidation_0-mae:527.72475\n",
            "[1009]\tvalidation_0-mae:527.68376\n",
            "[1010]\tvalidation_0-mae:527.50459\n",
            "[1011]\tvalidation_0-mae:527.50455\n",
            "[1012]\tvalidation_0-mae:527.50916\n",
            "[1013]\tvalidation_0-mae:527.45650\n",
            "[1014]\tvalidation_0-mae:527.46985\n",
            "[1015]\tvalidation_0-mae:527.40315\n",
            "[1016]\tvalidation_0-mae:527.40846\n",
            "[1017]\tvalidation_0-mae:527.37453\n",
            "[1018]\tvalidation_0-mae:527.34165\n",
            "[1019]\tvalidation_0-mae:527.25231\n",
            "[1020]\tvalidation_0-mae:527.19761\n",
            "[1021]\tvalidation_0-mae:527.20018\n",
            "[1022]\tvalidation_0-mae:527.04354\n",
            "[1023]\tvalidation_0-mae:527.08771\n",
            "[1024]\tvalidation_0-mae:527.06374\n",
            "[1025]\tvalidation_0-mae:527.04605\n",
            "[1026]\tvalidation_0-mae:526.92630\n",
            "[1027]\tvalidation_0-mae:526.82948\n",
            "[1028]\tvalidation_0-mae:526.81055\n",
            "[1029]\tvalidation_0-mae:526.74969\n",
            "[1030]\tvalidation_0-mae:526.74046\n",
            "[1031]\tvalidation_0-mae:526.75453\n",
            "[1032]\tvalidation_0-mae:526.72864\n",
            "[1033]\tvalidation_0-mae:526.68330\n",
            "[1034]\tvalidation_0-mae:526.66918\n",
            "[1035]\tvalidation_0-mae:526.65249\n",
            "[1036]\tvalidation_0-mae:526.62145\n",
            "[1037]\tvalidation_0-mae:526.60921\n",
            "[1038]\tvalidation_0-mae:526.57845\n",
            "[1039]\tvalidation_0-mae:526.57779\n",
            "[1040]\tvalidation_0-mae:526.54850\n",
            "[1041]\tvalidation_0-mae:526.56259\n",
            "[1042]\tvalidation_0-mae:526.36948\n",
            "[1043]\tvalidation_0-mae:526.35506\n",
            "[1044]\tvalidation_0-mae:526.32229\n",
            "[1045]\tvalidation_0-mae:526.30467\n",
            "[1046]\tvalidation_0-mae:526.28114\n",
            "[1047]\tvalidation_0-mae:526.26772\n",
            "[1048]\tvalidation_0-mae:526.29902\n",
            "[1049]\tvalidation_0-mae:526.25649\n",
            "[1050]\tvalidation_0-mae:526.20782\n",
            "[1051]\tvalidation_0-mae:526.01494\n",
            "[1052]\tvalidation_0-mae:525.99895\n",
            "[1053]\tvalidation_0-mae:525.99207\n",
            "[1054]\tvalidation_0-mae:525.96086\n",
            "[1055]\tvalidation_0-mae:525.94590\n",
            "[1056]\tvalidation_0-mae:525.94803\n",
            "[1057]\tvalidation_0-mae:525.95413\n",
            "[1058]\tvalidation_0-mae:525.94364\n",
            "[1059]\tvalidation_0-mae:525.94219\n",
            "[1060]\tvalidation_0-mae:525.79361\n",
            "[1061]\tvalidation_0-mae:525.79616\n",
            "[1062]\tvalidation_0-mae:525.80319\n",
            "[1063]\tvalidation_0-mae:525.78325\n",
            "[1064]\tvalidation_0-mae:525.78918\n",
            "[1065]\tvalidation_0-mae:525.79950\n",
            "[1066]\tvalidation_0-mae:525.80712\n",
            "[1067]\tvalidation_0-mae:525.79538\n",
            "[1068]\tvalidation_0-mae:525.76985\n",
            "[1069]\tvalidation_0-mae:525.76931\n",
            "[1070]\tvalidation_0-mae:525.73767\n",
            "[1071]\tvalidation_0-mae:525.73645\n",
            "[1072]\tvalidation_0-mae:525.74845\n",
            "[1073]\tvalidation_0-mae:525.71923\n",
            "[1074]\tvalidation_0-mae:525.72596\n",
            "[1075]\tvalidation_0-mae:525.72718\n",
            "[1076]\tvalidation_0-mae:525.72552\n",
            "[1077]\tvalidation_0-mae:525.68560\n",
            "[1078]\tvalidation_0-mae:525.67003\n",
            "[1079]\tvalidation_0-mae:525.59812\n",
            "[1080]\tvalidation_0-mae:525.51777\n",
            "[1081]\tvalidation_0-mae:525.54135\n",
            "[1082]\tvalidation_0-mae:525.54873\n",
            "[1083]\tvalidation_0-mae:525.53787\n",
            "[1084]\tvalidation_0-mae:525.55116\n",
            "[1085]\tvalidation_0-mae:525.55184\n",
            "[1086]\tvalidation_0-mae:525.40757\n",
            "[1087]\tvalidation_0-mae:525.42928\n",
            "[1088]\tvalidation_0-mae:525.36075\n",
            "[1089]\tvalidation_0-mae:525.37167\n",
            "[1090]\tvalidation_0-mae:525.26594\n",
            "[1091]\tvalidation_0-mae:525.25423\n",
            "[1092]\tvalidation_0-mae:525.22787\n",
            "[1093]\tvalidation_0-mae:525.08217\n",
            "[1094]\tvalidation_0-mae:525.08071\n",
            "[1095]\tvalidation_0-mae:525.10306\n",
            "[1096]\tvalidation_0-mae:525.11272\n",
            "[1097]\tvalidation_0-mae:525.11691\n",
            "[1098]\tvalidation_0-mae:525.12378\n",
            "[1099]\tvalidation_0-mae:525.11703\n",
            "[1100]\tvalidation_0-mae:525.10175\n",
            "[1101]\tvalidation_0-mae:525.06248\n",
            "[1102]\tvalidation_0-mae:525.02612\n",
            "[1103]\tvalidation_0-mae:525.03378\n",
            "[1104]\tvalidation_0-mae:525.00999\n",
            "[1105]\tvalidation_0-mae:524.98693\n",
            "[1106]\tvalidation_0-mae:524.91587\n",
            "[1107]\tvalidation_0-mae:524.90901\n",
            "[1108]\tvalidation_0-mae:524.90547\n",
            "[1109]\tvalidation_0-mae:524.85909\n",
            "[1110]\tvalidation_0-mae:524.84042\n",
            "[1111]\tvalidation_0-mae:524.83571\n",
            "[1112]\tvalidation_0-mae:524.79947\n",
            "[1113]\tvalidation_0-mae:524.77114\n",
            "[1114]\tvalidation_0-mae:524.77272\n",
            "[1115]\tvalidation_0-mae:524.71108\n",
            "[1116]\tvalidation_0-mae:524.70084\n",
            "[1117]\tvalidation_0-mae:524.69820\n",
            "[1118]\tvalidation_0-mae:524.69126\n",
            "[1119]\tvalidation_0-mae:524.65320\n",
            "[1120]\tvalidation_0-mae:524.68247\n",
            "[1121]\tvalidation_0-mae:524.65831\n",
            "[1122]\tvalidation_0-mae:524.56182\n",
            "[1123]\tvalidation_0-mae:524.55884\n",
            "[1124]\tvalidation_0-mae:524.56506\n",
            "[1125]\tvalidation_0-mae:524.56481\n",
            "[1126]\tvalidation_0-mae:524.59308\n",
            "[1127]\tvalidation_0-mae:524.57589\n",
            "[1128]\tvalidation_0-mae:524.57639\n",
            "[1129]\tvalidation_0-mae:524.55882\n",
            "[1130]\tvalidation_0-mae:524.56961\n",
            "[1131]\tvalidation_0-mae:524.56106\n",
            "[1132]\tvalidation_0-mae:524.46692\n",
            "[1133]\tvalidation_0-mae:524.47225\n",
            "[1134]\tvalidation_0-mae:524.45845\n",
            "[1135]\tvalidation_0-mae:524.45020\n",
            "[1136]\tvalidation_0-mae:524.43854\n",
            "[1137]\tvalidation_0-mae:524.43816\n",
            "[1138]\tvalidation_0-mae:524.42525\n",
            "[1139]\tvalidation_0-mae:524.44685\n",
            "[1140]\tvalidation_0-mae:524.45143\n",
            "[1141]\tvalidation_0-mae:524.44470\n",
            "[1142]\tvalidation_0-mae:524.41623\n",
            "[1143]\tvalidation_0-mae:524.38341\n",
            "[1144]\tvalidation_0-mae:524.35703\n",
            "[1145]\tvalidation_0-mae:524.35107\n",
            "[1146]\tvalidation_0-mae:524.34391\n",
            "[1147]\tvalidation_0-mae:524.21836\n",
            "[1148]\tvalidation_0-mae:524.13494\n",
            "[1149]\tvalidation_0-mae:524.12282\n",
            "[1150]\tvalidation_0-mae:523.98112\n",
            "[1151]\tvalidation_0-mae:523.99937\n",
            "[1152]\tvalidation_0-mae:523.99620\n",
            "[1153]\tvalidation_0-mae:523.97746\n",
            "[1154]\tvalidation_0-mae:523.97482\n",
            "[1155]\tvalidation_0-mae:523.95780\n",
            "[1156]\tvalidation_0-mae:523.95233\n",
            "[1157]\tvalidation_0-mae:523.98092\n",
            "[1158]\tvalidation_0-mae:523.88897\n",
            "[1159]\tvalidation_0-mae:523.90525\n",
            "[1160]\tvalidation_0-mae:523.89036\n",
            "[1161]\tvalidation_0-mae:523.86787\n",
            "[1162]\tvalidation_0-mae:523.86572\n",
            "[1163]\tvalidation_0-mae:523.87446\n",
            "[1164]\tvalidation_0-mae:523.88641\n",
            "[1165]\tvalidation_0-mae:523.83968\n",
            "[1166]\tvalidation_0-mae:523.78645\n",
            "[1167]\tvalidation_0-mae:523.79290\n",
            "[1168]\tvalidation_0-mae:523.78209\n",
            "[1169]\tvalidation_0-mae:523.74832\n",
            "[1170]\tvalidation_0-mae:523.71556\n",
            "[1171]\tvalidation_0-mae:523.73468\n",
            "[1172]\tvalidation_0-mae:523.73182\n",
            "[1173]\tvalidation_0-mae:523.72330\n",
            "[1174]\tvalidation_0-mae:523.59059\n",
            "[1175]\tvalidation_0-mae:523.58512\n",
            "[1176]\tvalidation_0-mae:523.55550\n",
            "[1177]\tvalidation_0-mae:523.42675\n",
            "[1178]\tvalidation_0-mae:523.41610\n",
            "[1179]\tvalidation_0-mae:523.39152\n",
            "[1180]\tvalidation_0-mae:523.26552\n",
            "[1181]\tvalidation_0-mae:523.22686\n",
            "[1182]\tvalidation_0-mae:523.23430\n",
            "[1183]\tvalidation_0-mae:523.24396\n",
            "[1184]\tvalidation_0-mae:523.25885\n",
            "[1185]\tvalidation_0-mae:523.22573\n",
            "[1186]\tvalidation_0-mae:523.13329\n",
            "[1187]\tvalidation_0-mae:523.11949\n",
            "[1188]\tvalidation_0-mae:523.11210\n",
            "[1189]\tvalidation_0-mae:523.12418\n",
            "[1190]\tvalidation_0-mae:523.10935\n",
            "[1191]\tvalidation_0-mae:523.10568\n",
            "[1192]\tvalidation_0-mae:523.01105\n",
            "[1193]\tvalidation_0-mae:523.01572\n",
            "[1194]\tvalidation_0-mae:522.93618\n",
            "[1195]\tvalidation_0-mae:522.92101\n",
            "[1196]\tvalidation_0-mae:522.76062\n",
            "[1197]\tvalidation_0-mae:522.72582\n",
            "[1198]\tvalidation_0-mae:522.73984\n",
            "[1199]\tvalidation_0-mae:522.73750\n",
            "[1200]\tvalidation_0-mae:522.71949\n",
            "[1201]\tvalidation_0-mae:522.74983\n",
            "[1202]\tvalidation_0-mae:522.76055\n",
            "[1203]\tvalidation_0-mae:522.64077\n",
            "[1204]\tvalidation_0-mae:522.57979\n",
            "[1205]\tvalidation_0-mae:522.58940\n",
            "[1206]\tvalidation_0-mae:522.58237\n",
            "[1207]\tvalidation_0-mae:522.54424\n",
            "[1208]\tvalidation_0-mae:522.55754\n",
            "[1209]\tvalidation_0-mae:522.53627\n",
            "[1210]\tvalidation_0-mae:522.49972\n",
            "[1211]\tvalidation_0-mae:522.43644\n",
            "[1212]\tvalidation_0-mae:522.43028\n",
            "[1213]\tvalidation_0-mae:522.43193\n",
            "[1214]\tvalidation_0-mae:522.42131\n",
            "[1215]\tvalidation_0-mae:522.43142\n",
            "[1216]\tvalidation_0-mae:522.43449\n",
            "[1217]\tvalidation_0-mae:522.42392\n",
            "[1218]\tvalidation_0-mae:522.46564\n",
            "[1219]\tvalidation_0-mae:522.40940\n",
            "[1220]\tvalidation_0-mae:522.20623\n",
            "[1221]\tvalidation_0-mae:522.19648\n",
            "[1222]\tvalidation_0-mae:522.19103\n",
            "[1223]\tvalidation_0-mae:522.19583\n",
            "[1224]\tvalidation_0-mae:522.19459\n",
            "[1225]\tvalidation_0-mae:522.19152\n",
            "[1226]\tvalidation_0-mae:522.18392\n",
            "[1227]\tvalidation_0-mae:522.16859\n",
            "[1228]\tvalidation_0-mae:522.07481\n",
            "[1229]\tvalidation_0-mae:522.08722\n",
            "[1230]\tvalidation_0-mae:522.06862\n",
            "[1231]\tvalidation_0-mae:522.07609\n",
            "[1232]\tvalidation_0-mae:522.09421\n",
            "[1233]\tvalidation_0-mae:522.08654\n",
            "[1234]\tvalidation_0-mae:522.06713\n",
            "[1235]\tvalidation_0-mae:522.05475\n",
            "[1236]\tvalidation_0-mae:522.04350\n",
            "[1237]\tvalidation_0-mae:522.02893\n",
            "[1238]\tvalidation_0-mae:522.05996\n",
            "[1239]\tvalidation_0-mae:522.07543\n",
            "[1240]\tvalidation_0-mae:521.97938\n",
            "[1241]\tvalidation_0-mae:521.97353\n",
            "[1242]\tvalidation_0-mae:521.93535\n",
            "[1243]\tvalidation_0-mae:521.80592\n",
            "[1244]\tvalidation_0-mae:521.81862\n",
            "[1245]\tvalidation_0-mae:521.83102\n",
            "[1246]\tvalidation_0-mae:521.81287\n",
            "[1247]\tvalidation_0-mae:521.81878\n",
            "[1248]\tvalidation_0-mae:521.82051\n",
            "[1249]\tvalidation_0-mae:521.81986\n",
            "[1250]\tvalidation_0-mae:521.83710\n",
            "[1251]\tvalidation_0-mae:521.83672\n",
            "[1252]\tvalidation_0-mae:521.83687\n",
            "[1253]\tvalidation_0-mae:521.86117\n",
            "[1254]\tvalidation_0-mae:521.85924\n",
            "[1255]\tvalidation_0-mae:521.87146\n",
            "[1256]\tvalidation_0-mae:521.83045\n",
            "[1257]\tvalidation_0-mae:521.84780\n",
            "[1258]\tvalidation_0-mae:521.83848\n",
            "[1259]\tvalidation_0-mae:521.82558\n",
            "[1260]\tvalidation_0-mae:521.77609\n",
            "[1261]\tvalidation_0-mae:521.71519\n",
            "[1262]\tvalidation_0-mae:521.72574\n",
            "[1263]\tvalidation_0-mae:521.72492\n",
            "[1264]\tvalidation_0-mae:521.70889\n",
            "[1265]\tvalidation_0-mae:521.67432\n",
            "[1266]\tvalidation_0-mae:521.65447\n",
            "[1267]\tvalidation_0-mae:521.64551\n",
            "[1268]\tvalidation_0-mae:521.65148\n",
            "[1269]\tvalidation_0-mae:521.65264\n",
            "[1270]\tvalidation_0-mae:521.63202\n",
            "[1271]\tvalidation_0-mae:521.63470\n",
            "[1272]\tvalidation_0-mae:521.63168\n",
            "[1273]\tvalidation_0-mae:521.63041\n",
            "[1274]\tvalidation_0-mae:521.64179\n",
            "[1275]\tvalidation_0-mae:521.60103\n",
            "[1276]\tvalidation_0-mae:521.60186\n",
            "[1277]\tvalidation_0-mae:521.58676\n",
            "[1278]\tvalidation_0-mae:521.59334\n",
            "[1279]\tvalidation_0-mae:521.61978\n",
            "[1280]\tvalidation_0-mae:521.53887\n",
            "[1281]\tvalidation_0-mae:521.48397\n",
            "[1282]\tvalidation_0-mae:521.46152\n",
            "[1283]\tvalidation_0-mae:521.44843\n",
            "[1284]\tvalidation_0-mae:521.45215\n",
            "[1285]\tvalidation_0-mae:521.42807\n",
            "[1286]\tvalidation_0-mae:521.35063\n",
            "[1287]\tvalidation_0-mae:521.33772\n",
            "[1288]\tvalidation_0-mae:521.35252\n",
            "[1289]\tvalidation_0-mae:521.35989\n",
            "[1290]\tvalidation_0-mae:521.34845\n",
            "[1291]\tvalidation_0-mae:521.17039\n",
            "[1292]\tvalidation_0-mae:521.15421\n",
            "[1293]\tvalidation_0-mae:521.14790\n",
            "[1294]\tvalidation_0-mae:521.10109\n",
            "[1295]\tvalidation_0-mae:521.09149\n",
            "[1296]\tvalidation_0-mae:521.06876\n",
            "[1297]\tvalidation_0-mae:521.06044\n",
            "[1298]\tvalidation_0-mae:521.05400\n",
            "[1299]\tvalidation_0-mae:521.05495\n",
            "[1300]\tvalidation_0-mae:521.07225\n",
            "[1301]\tvalidation_0-mae:521.06404\n",
            "[1302]\tvalidation_0-mae:521.07401\n",
            "[1303]\tvalidation_0-mae:521.07257\n",
            "[1304]\tvalidation_0-mae:521.09128\n",
            "[1305]\tvalidation_0-mae:521.04082\n",
            "[1306]\tvalidation_0-mae:521.01147\n",
            "[1307]\tvalidation_0-mae:521.01926\n",
            "[1308]\tvalidation_0-mae:520.98674\n",
            "[1309]\tvalidation_0-mae:520.93931\n",
            "[1310]\tvalidation_0-mae:520.91126\n",
            "[1311]\tvalidation_0-mae:520.89012\n",
            "[1312]\tvalidation_0-mae:520.87532\n",
            "[1313]\tvalidation_0-mae:520.86304\n",
            "[1314]\tvalidation_0-mae:520.87534\n",
            "[1315]\tvalidation_0-mae:520.81316\n",
            "[1316]\tvalidation_0-mae:520.79192\n",
            "[1317]\tvalidation_0-mae:520.79515\n",
            "[1318]\tvalidation_0-mae:520.77546\n",
            "[1319]\tvalidation_0-mae:520.77735\n",
            "[1320]\tvalidation_0-mae:520.78161\n",
            "[1321]\tvalidation_0-mae:520.79017\n",
            "[1322]\tvalidation_0-mae:520.75139\n",
            "[1323]\tvalidation_0-mae:520.74270\n",
            "[1324]\tvalidation_0-mae:520.76276\n",
            "[1325]\tvalidation_0-mae:520.76296\n",
            "[1326]\tvalidation_0-mae:520.77089\n",
            "[1327]\tvalidation_0-mae:520.79888\n",
            "[1328]\tvalidation_0-mae:520.79733\n",
            "[1329]\tvalidation_0-mae:520.82006\n",
            "[1330]\tvalidation_0-mae:520.81791\n",
            "[1331]\tvalidation_0-mae:520.84383\n",
            "[1332]\tvalidation_0-mae:520.75720\n",
            "[1333]\tvalidation_0-mae:520.73838\n",
            "[1334]\tvalidation_0-mae:520.71754\n",
            "[1335]\tvalidation_0-mae:520.69424\n",
            "[1336]\tvalidation_0-mae:520.70036\n",
            "[1337]\tvalidation_0-mae:520.68711\n",
            "[1338]\tvalidation_0-mae:520.71067\n",
            "[1339]\tvalidation_0-mae:520.69476\n",
            "[1340]\tvalidation_0-mae:520.68263\n",
            "[1341]\tvalidation_0-mae:520.65431\n",
            "[1342]\tvalidation_0-mae:520.66010\n",
            "[1343]\tvalidation_0-mae:520.65728\n",
            "[1344]\tvalidation_0-mae:520.55024\n",
            "[1345]\tvalidation_0-mae:520.57240\n",
            "[1346]\tvalidation_0-mae:520.55322\n",
            "[1347]\tvalidation_0-mae:520.52957\n",
            "[1348]\tvalidation_0-mae:520.52459\n",
            "[1349]\tvalidation_0-mae:520.50651\n",
            "[1350]\tvalidation_0-mae:520.48125\n",
            "[1351]\tvalidation_0-mae:520.50404\n",
            "[1352]\tvalidation_0-mae:520.49990\n",
            "[1353]\tvalidation_0-mae:520.50714\n",
            "[1354]\tvalidation_0-mae:520.49442\n",
            "[1355]\tvalidation_0-mae:520.48942\n",
            "[1356]\tvalidation_0-mae:520.49086\n",
            "[1357]\tvalidation_0-mae:520.46903\n",
            "[1358]\tvalidation_0-mae:520.46772\n",
            "[1359]\tvalidation_0-mae:520.46702\n",
            "[1360]\tvalidation_0-mae:520.46825\n",
            "[1361]\tvalidation_0-mae:520.48539\n",
            "[1362]\tvalidation_0-mae:520.47565\n",
            "[1363]\tvalidation_0-mae:520.43291\n",
            "[1364]\tvalidation_0-mae:520.40841\n",
            "[1365]\tvalidation_0-mae:520.43015\n",
            "[1366]\tvalidation_0-mae:520.41817\n",
            "[1367]\tvalidation_0-mae:520.41932\n",
            "[1368]\tvalidation_0-mae:520.38202\n",
            "[1369]\tvalidation_0-mae:520.35386\n",
            "[1370]\tvalidation_0-mae:520.35989\n",
            "[1371]\tvalidation_0-mae:520.34218\n",
            "[1372]\tvalidation_0-mae:520.32474\n",
            "[1373]\tvalidation_0-mae:520.32524\n",
            "[1374]\tvalidation_0-mae:520.32443\n",
            "[1375]\tvalidation_0-mae:520.32128\n",
            "[1376]\tvalidation_0-mae:520.32267\n",
            "[1377]\tvalidation_0-mae:520.32609\n",
            "[1378]\tvalidation_0-mae:520.33471\n",
            "[1379]\tvalidation_0-mae:520.32865\n",
            "[1380]\tvalidation_0-mae:520.31566\n",
            "[1381]\tvalidation_0-mae:520.27081\n",
            "[1382]\tvalidation_0-mae:520.29319\n",
            "[1383]\tvalidation_0-mae:520.30623\n",
            "[1384]\tvalidation_0-mae:520.30975\n",
            "[1385]\tvalidation_0-mae:520.28506\n",
            "[1386]\tvalidation_0-mae:520.28206\n",
            "[1387]\tvalidation_0-mae:520.27315\n",
            "[1388]\tvalidation_0-mae:520.25834\n",
            "[1389]\tvalidation_0-mae:520.19298\n",
            "[1390]\tvalidation_0-mae:520.18604\n",
            "[1391]\tvalidation_0-mae:520.15567\n",
            "[1392]\tvalidation_0-mae:520.15665\n",
            "[1393]\tvalidation_0-mae:520.14309\n",
            "[1394]\tvalidation_0-mae:520.08164\n",
            "[1395]\tvalidation_0-mae:520.04045\n",
            "[1396]\tvalidation_0-mae:519.87466\n",
            "[1397]\tvalidation_0-mae:519.87959\n",
            "[1398]\tvalidation_0-mae:519.85943\n",
            "[1399]\tvalidation_0-mae:519.83650\n",
            "[1400]\tvalidation_0-mae:519.82475\n",
            "[1401]\tvalidation_0-mae:519.82913\n",
            "[1402]\tvalidation_0-mae:519.82573\n",
            "[1403]\tvalidation_0-mae:519.84492\n",
            "[1404]\tvalidation_0-mae:519.86470\n",
            "[1405]\tvalidation_0-mae:519.85676\n",
            "[1406]\tvalidation_0-mae:519.83467\n",
            "[1407]\tvalidation_0-mae:519.83593\n",
            "[1408]\tvalidation_0-mae:519.82933\n",
            "[1409]\tvalidation_0-mae:519.82135\n",
            "[1410]\tvalidation_0-mae:519.82530\n",
            "[1411]\tvalidation_0-mae:519.82096\n",
            "[1412]\tvalidation_0-mae:519.83412\n",
            "[1413]\tvalidation_0-mae:519.81710\n",
            "[1414]\tvalidation_0-mae:519.81948\n",
            "[1415]\tvalidation_0-mae:519.80796\n",
            "[1416]\tvalidation_0-mae:519.80071\n",
            "[1417]\tvalidation_0-mae:519.79817\n",
            "[1418]\tvalidation_0-mae:519.70543\n",
            "[1419]\tvalidation_0-mae:519.72135\n",
            "[1420]\tvalidation_0-mae:519.71893\n",
            "[1421]\tvalidation_0-mae:519.72292\n",
            "[1422]\tvalidation_0-mae:519.73466\n",
            "[1423]\tvalidation_0-mae:519.73342\n",
            "[1424]\tvalidation_0-mae:519.75422\n",
            "[1425]\tvalidation_0-mae:519.69978\n",
            "[1426]\tvalidation_0-mae:519.68610\n",
            "[1427]\tvalidation_0-mae:519.67512\n",
            "[1428]\tvalidation_0-mae:519.67908\n",
            "[1429]\tvalidation_0-mae:519.67037\n",
            "[1430]\tvalidation_0-mae:519.67492\n",
            "[1431]\tvalidation_0-mae:519.53801\n",
            "[1432]\tvalidation_0-mae:519.45685\n",
            "[1433]\tvalidation_0-mae:519.40996\n",
            "[1434]\tvalidation_0-mae:519.38715\n",
            "[1435]\tvalidation_0-mae:519.38796\n",
            "[1436]\tvalidation_0-mae:519.37725\n",
            "[1437]\tvalidation_0-mae:519.31629\n",
            "[1438]\tvalidation_0-mae:519.31557\n",
            "[1439]\tvalidation_0-mae:519.30193\n",
            "[1440]\tvalidation_0-mae:519.30421\n",
            "[1441]\tvalidation_0-mae:519.29701\n",
            "[1442]\tvalidation_0-mae:519.31043\n",
            "[1443]\tvalidation_0-mae:519.32054\n",
            "[1444]\tvalidation_0-mae:519.33628\n",
            "[1445]\tvalidation_0-mae:519.31560\n",
            "[1446]\tvalidation_0-mae:519.32572\n",
            "[1447]\tvalidation_0-mae:519.32633\n",
            "[1448]\tvalidation_0-mae:519.32712\n",
            "[1449]\tvalidation_0-mae:519.29876\n",
            "[1450]\tvalidation_0-mae:519.29163\n",
            "[1451]\tvalidation_0-mae:519.31486\n",
            "[1452]\tvalidation_0-mae:519.25215\n",
            "[1453]\tvalidation_0-mae:519.25208\n",
            "[1454]\tvalidation_0-mae:519.17187\n",
            "[1455]\tvalidation_0-mae:519.15929\n",
            "[1456]\tvalidation_0-mae:519.14688\n",
            "[1457]\tvalidation_0-mae:519.16645\n",
            "[1458]\tvalidation_0-mae:519.17800\n",
            "[1459]\tvalidation_0-mae:519.15500\n",
            "[1460]\tvalidation_0-mae:519.14721\n",
            "[1461]\tvalidation_0-mae:519.12818\n",
            "[1462]\tvalidation_0-mae:519.11823\n",
            "[1463]\tvalidation_0-mae:519.09959\n",
            "[1464]\tvalidation_0-mae:519.11345\n",
            "[1465]\tvalidation_0-mae:519.11428\n",
            "[1466]\tvalidation_0-mae:519.12311\n",
            "[1467]\tvalidation_0-mae:519.10701\n",
            "[1468]\tvalidation_0-mae:519.05392\n",
            "[1469]\tvalidation_0-mae:519.04062\n",
            "[1470]\tvalidation_0-mae:519.04146\n",
            "[1471]\tvalidation_0-mae:519.04683\n",
            "[1472]\tvalidation_0-mae:519.07564\n",
            "[1473]\tvalidation_0-mae:519.06878\n",
            "[1474]\tvalidation_0-mae:519.07111\n",
            "[1475]\tvalidation_0-mae:519.04848\n",
            "[1476]\tvalidation_0-mae:519.03288\n",
            "[1477]\tvalidation_0-mae:519.03259\n",
            "[1478]\tvalidation_0-mae:519.00120\n",
            "[1479]\tvalidation_0-mae:518.99498\n",
            "[1480]\tvalidation_0-mae:518.99189\n",
            "[1481]\tvalidation_0-mae:519.01678\n",
            "[1482]\tvalidation_0-mae:519.03334\n",
            "[1483]\tvalidation_0-mae:519.02691\n",
            "[1484]\tvalidation_0-mae:519.00648\n",
            "[1485]\tvalidation_0-mae:518.98897\n",
            "[1486]\tvalidation_0-mae:518.99795\n",
            "[1487]\tvalidation_0-mae:519.00230\n",
            "[1488]\tvalidation_0-mae:518.97856\n",
            "[1489]\tvalidation_0-mae:518.99129\n",
            "[1490]\tvalidation_0-mae:518.97124\n",
            "[1491]\tvalidation_0-mae:518.97060\n",
            "[1492]\tvalidation_0-mae:518.96216\n",
            "[1493]\tvalidation_0-mae:518.95319\n",
            "[1494]\tvalidation_0-mae:518.95369\n",
            "[1495]\tvalidation_0-mae:518.93436\n",
            "[1496]\tvalidation_0-mae:518.96021\n",
            "[1497]\tvalidation_0-mae:518.95892\n",
            "[1498]\tvalidation_0-mae:518.95230\n",
            "[1499]\tvalidation_0-mae:518.92514\n",
            "[1500]\tvalidation_0-mae:518.92222\n",
            "[1501]\tvalidation_0-mae:518.92834\n",
            "[1502]\tvalidation_0-mae:518.93392\n",
            "[1503]\tvalidation_0-mae:518.94412\n",
            "[1504]\tvalidation_0-mae:518.96838\n",
            "[1505]\tvalidation_0-mae:518.97741\n",
            "[1506]\tvalidation_0-mae:518.97133\n",
            "[1507]\tvalidation_0-mae:518.98332\n",
            "[1508]\tvalidation_0-mae:519.00490\n",
            "[1509]\tvalidation_0-mae:519.00744\n",
            "[1510]\tvalidation_0-mae:519.04014\n",
            "[1511]\tvalidation_0-mae:519.03907\n",
            "[1512]\tvalidation_0-mae:519.02529\n",
            "[1513]\tvalidation_0-mae:519.02436\n",
            "[1514]\tvalidation_0-mae:519.02164\n",
            "[1515]\tvalidation_0-mae:519.01667\n",
            "[1516]\tvalidation_0-mae:519.01338\n",
            "[1517]\tvalidation_0-mae:518.98858\n",
            "[1518]\tvalidation_0-mae:518.94639\n",
            "[1519]\tvalidation_0-mae:518.94821\n",
            "[1520]\tvalidation_0-mae:518.93001\n",
            "[1521]\tvalidation_0-mae:518.93082\n",
            "[1522]\tvalidation_0-mae:518.92779\n",
            "[1523]\tvalidation_0-mae:518.94188\n",
            "[1524]\tvalidation_0-mae:518.95546\n",
            "[1525]\tvalidation_0-mae:518.95569\n",
            "[1526]\tvalidation_0-mae:518.96036\n",
            "[1527]\tvalidation_0-mae:518.94992\n",
            "[1528]\tvalidation_0-mae:518.94371\n",
            "[1529]\tvalidation_0-mae:518.91610\n",
            "[1530]\tvalidation_0-mae:518.92025\n",
            "[1531]\tvalidation_0-mae:518.90235\n",
            "[1532]\tvalidation_0-mae:518.91813\n",
            "[1533]\tvalidation_0-mae:518.91506\n",
            "[1534]\tvalidation_0-mae:518.91757\n",
            "[1535]\tvalidation_0-mae:518.86217\n",
            "[1536]\tvalidation_0-mae:518.85484\n",
            "[1537]\tvalidation_0-mae:518.85855\n",
            "[1538]\tvalidation_0-mae:518.84365\n",
            "[1539]\tvalidation_0-mae:518.81782\n",
            "[1540]\tvalidation_0-mae:518.80683\n",
            "[1541]\tvalidation_0-mae:518.79252\n",
            "[1542]\tvalidation_0-mae:518.81072\n",
            "[1543]\tvalidation_0-mae:518.79648\n",
            "[1544]\tvalidation_0-mae:518.78825\n",
            "[1545]\tvalidation_0-mae:518.79050\n",
            "[1546]\tvalidation_0-mae:518.80882\n",
            "[1547]\tvalidation_0-mae:518.82417\n",
            "[1548]\tvalidation_0-mae:518.79706\n",
            "[1549]\tvalidation_0-mae:518.78915\n",
            "[1550]\tvalidation_0-mae:518.77130\n",
            "[1551]\tvalidation_0-mae:518.76370\n",
            "[1552]\tvalidation_0-mae:518.73126\n",
            "[1553]\tvalidation_0-mae:518.71507\n",
            "[1554]\tvalidation_0-mae:518.70378\n",
            "[1555]\tvalidation_0-mae:518.68128\n",
            "[1556]\tvalidation_0-mae:518.66295\n",
            "[1557]\tvalidation_0-mae:518.66548\n",
            "[1558]\tvalidation_0-mae:518.65310\n",
            "[1559]\tvalidation_0-mae:518.66836\n",
            "[1560]\tvalidation_0-mae:518.56798\n",
            "[1561]\tvalidation_0-mae:518.52797\n",
            "[1562]\tvalidation_0-mae:518.52489\n",
            "[1563]\tvalidation_0-mae:518.52608\n",
            "[1564]\tvalidation_0-mae:518.50875\n",
            "[1565]\tvalidation_0-mae:518.50024\n",
            "[1566]\tvalidation_0-mae:518.48418\n",
            "[1567]\tvalidation_0-mae:518.47264\n",
            "[1568]\tvalidation_0-mae:518.47643\n",
            "[1569]\tvalidation_0-mae:518.42957\n",
            "[1570]\tvalidation_0-mae:518.42283\n",
            "[1571]\tvalidation_0-mae:518.43880\n",
            "[1572]\tvalidation_0-mae:518.45802\n",
            "[1573]\tvalidation_0-mae:518.45811\n",
            "[1574]\tvalidation_0-mae:518.36745\n",
            "[1575]\tvalidation_0-mae:518.35239\n",
            "[1576]\tvalidation_0-mae:518.33813\n",
            "[1577]\tvalidation_0-mae:518.31561\n",
            "[1578]\tvalidation_0-mae:518.31808\n",
            "[1579]\tvalidation_0-mae:518.26715\n",
            "[1580]\tvalidation_0-mae:518.24426\n",
            "[1581]\tvalidation_0-mae:518.24819\n",
            "[1582]\tvalidation_0-mae:518.23804\n",
            "[1583]\tvalidation_0-mae:518.24849\n",
            "[1584]\tvalidation_0-mae:518.22179\n",
            "[1585]\tvalidation_0-mae:518.17139\n",
            "[1586]\tvalidation_0-mae:518.18494\n",
            "[1587]\tvalidation_0-mae:518.16676\n",
            "[1588]\tvalidation_0-mae:518.17764\n",
            "[1589]\tvalidation_0-mae:518.18809\n",
            "[1590]\tvalidation_0-mae:518.15834\n",
            "[1591]\tvalidation_0-mae:518.16727\n",
            "[1592]\tvalidation_0-mae:518.17267\n",
            "[1593]\tvalidation_0-mae:518.17657\n",
            "[1594]\tvalidation_0-mae:518.14580\n",
            "[1595]\tvalidation_0-mae:518.14235\n",
            "[1596]\tvalidation_0-mae:518.12544\n",
            "[1597]\tvalidation_0-mae:518.13104\n",
            "[1598]\tvalidation_0-mae:518.12958\n",
            "[1599]\tvalidation_0-mae:518.14794\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025/07/07 21:41:34 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n",
            "2025/07/07 21:41:34 WARNING mlflow.utils.autologging_utils: Encountered unexpected error during xgboost autologging: INTERNAL_ERROR: Response: {'error': 'unsupported endpoint, please contact support@dagshub.com'}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Validation – WMAE:571.62, MAE:518.15, RMSE:1721.68, R²:0.9941\n",
            "✅ Written xgb_submission.csv\n",
            "✅ Pipeline saved to 'xgb_model_pipeline.pkl'\n",
            "🏃 View run xgb_final at: https://dagshub.com/nkhar21/ML_Final_Project.mlflow/#/experiments/0/runs/63ec8410d6944556bdc70f1c55d68513\n",
            "🧪 View experiment at: https://dagshub.com/nkhar21/ML_Final_Project.mlflow/#/experiments/0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Predict and generate sub"
      ],
      "metadata": {
        "id": "1u-jHv_P2VYV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_test  = clamp.transform(scaler.transform       (test_df))\n",
        "test_preds = xgb_model.predict(X_test)\n",
        "\n",
        "# 5) Create submission DataFrame with Id format: Store_Dept_Date\n",
        "submission = pd.DataFrame({\n",
        "    \"Id\": test[\"Store\"].astype(str) + \"_\" +\n",
        "          test[\"Dept\"].astype(str) + \"_\" +\n",
        "          test[\"Date\"].astype(str),\n",
        "    \"Weekly_Sales\": test_preds\n",
        "})\n",
        "\n",
        "# Save to CSV\n",
        "submission.to_csv(\"submission_XGB.csv\", index=False)\n",
        "\n",
        "# Preview\n",
        "print(submission.head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oL5M9o08yVT9",
        "outputId": "10b4da25-6504-4080-b5de-0c51a66c7e31"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "               Id  Weekly_Sales\n",
            "0  1_1_2012-11-02  21761.939453\n",
            "1  1_1_2012-11-09  22235.800781\n",
            "2  1_1_2012-11-16  21867.765625\n",
            "3  1_1_2012-11-23  21721.035156\n",
            "4  1_1_2012-11-30  21982.865234\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "import mlflow\n",
        "\n",
        "mlflow.set_experiment(\"XGBoost_Training\")\n",
        "with mlflow.start_run(run_name=\"XGBoost_preprocessor_run\") as run:\n",
        "    run_id = run.info.run_id\n",
        "    print(\"MLflow Run ID:\", run_id)\n",
        "\n",
        "    # 1) Log your custom params as before\n",
        "    bm = preprocessor_pipeline.named_steps[\"merge\"]\n",
        "    mlflow.log_param(\"BaseMerger.feature_store_rows\", len(bm.feature_store))\n",
        "    mvf = preprocessor_pipeline.named_steps[\"fillna\"]\n",
        "    mlflow.log_param(\"MissingValueFiller.markdown_cols\", len(mvf.markdown_cols))\n",
        "    mlflow.log_param(\"MissingValueFiller.mean_cols\",     len(mvf.mean_cols))\n",
        "    ce = preprocessor_pipeline.named_steps[\"label_encode\"]\n",
        "    mlflow.log_param(\"CategoricalEncoder.type_mapping\",    str(ce.type_mapping))\n",
        "    mlflow.log_param(\"CategoricalEncoder.holiday_mapping\", str(ce.holiday_mapping))\n",
        "    fa = preprocessor_pipeline.named_steps[\"feature_add\"]\n",
        "    mlflow.log_param(\"FeatureAdder.superbowl_dates\",     len(fa.superbowl))\n",
        "    mlflow.log_param(\"FeatureAdder.thanksgiving_dates\",  len(fa.thanksgiving))\n",
        "    lag = preprocessor_pipeline.named_steps[\"lags\"]\n",
        "    mlflow.log_param(\"LagFeatureTransformer.lags\",            \",\".join(map(str, lag.lags)))\n",
        "    mlflow.log_param(\"LagFeatureTransformer.rolling_windows\", \",\".join(map(str, lag.rolling_windows)))\n",
        "    mlflow.log_param(\"LagFeatureTransformer.drop_na\",         lag.drop_na)\n",
        "\n",
        "    # 2) Serialize to disk\n",
        "    with open(\"preprocess_pipeline.pkl\", \"wb\") as f:\n",
        "        pickle.dump(preprocessor_pipeline, f)\n",
        "\n",
        "    # 3) Log it as a generic artifact\n",
        "    mlflow.log_artifact(\"preprocess_pipeline.pkl\", artifact_path=\"pipelines\")\n",
        "\n",
        "    print(\"✅ Pickled & logged preprocessor as an artifact\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nlJzPfMkaa2y",
        "outputId": "4a397d57-a5d8-41c5-e618-3c411bd9a993"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MLflow Run ID: 3c94a96859224c08988b1ff0df319dd5\n",
            "✅ Pickled & logged preprocessor as an artifact\n",
            "🏃 View run XGBoost_preprocessor_run at: https://dagshub.com/nkhar21/ML_Final_Project.mlflow/#/experiments/0/runs/3c94a96859224c08988b1ff0df319dd5\n",
            "🧪 View experiment at: https://dagshub.com/nkhar21/ML_Final_Project.mlflow/#/experiments/0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EEl_nZ2rgiPw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Predict and generate submission"
      ],
      "metadata": {
        "id": "IrUltkhecxwK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "import mlflow\n",
        "import mlflow.xgboost\n",
        "import mlflow.sklearn\n",
        "import pandas as pd\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "\n",
        "# 1) Split off a small hold‐out for early stopping\n",
        "X_tr, X_val, y_tr, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
        "\n",
        "# 2) Configure MLflow\n",
        "mlflow.set_experiment(\"XGBoost_Training\")\n",
        "with mlflow.start_run(run_name=\"xgboost_regressor\") as run:\n",
        "    run_id = run.info.run_id\n",
        "    print(\"🧪 MLflow Run ID:\", run_id)\n",
        "\n",
        "    # 3) Fit with early stopping\n",
        "    regressor_pipeline.fit(\n",
        "        X_tr, y_tr,\n",
        "        regressor__eval_set=[(X_val, y_val)],\n",
        "        regressor__early_stopping_rounds=50,\n",
        "        regressor__eval_metric=\"mae\",\n",
        "        regressor__verbose=False\n",
        "    )\n",
        "\n",
        "\n",
        "    # 4) Log validation MAE\n",
        "    val_preds = regressor_pipeline.predict(X_val)\n",
        "    val_mae = mean_absolute_error(y_val, val_preds)\n",
        "    mlflow.log_metric(\"val_mae\", val_mae)\n",
        "    print(\"✅ Validation MAE:\", val_mae)\n",
        "\n",
        "    # 5) Create submission\n",
        "    test_preds = regressor_pipeline.predict(X_test)\n",
        "    submission = pd.DataFrame({\n",
        "        \"Id\": test[\"Store\"].astype(str) + \"_\" +\n",
        "              test[\"Dept\"].astype(str) + \"_\" +\n",
        "              test[\"Date\"].astype(str),\n",
        "        \"Weekly_Sales\": test_preds\n",
        "    })\n",
        "    submission.to_csv(\"xgb_submission.csv\", index=False)\n",
        "    print(\"✅ Wrote xgb_submission.csv\")\n",
        "\n",
        "    # 6) Log the entire fitted pipeline\n",
        "    mlflow.sklearn.log_model(regressor_pipeline, \"xgb_model_pipeline\")\n",
        "    print(\"✅ Pipeline logged as `xgb_model_pipeline`\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 445
        },
        "id": "lEslqSXAcwCe",
        "outputId": "9e0d9ff8-853d-4424-d99f-78cebe9ea069"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:urllib3.connectionpool:Retrying (Retry(total=6, connect=7, read=6, redirect=7, status=7)) after connection broken by 'RemoteDisconnected('Remote end closed connection without response')': /nkhar21/ML_Final_Project.mlflow/api/2.0/mlflow/experiments/get-by-name?experiment_name=XGBoost_Training\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🧪 MLflow Run ID: d3cf3d2059ff4ddda8960cbe174873a1\n",
            "🏃 View run xgboost_regressor at: https://dagshub.com/nkhar21/ML_Final_Project.mlflow/#/experiments/0/runs/d3cf3d2059ff4ddda8960cbe174873a1\n",
            "🧪 View experiment at: https://dagshub.com/nkhar21/ML_Final_Project.mlflow/#/experiments/0\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "XGBModel.fit() got an unexpected keyword argument 'early_stopping_rounds'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-28-4214329887.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;31m# 3) Fit with early stopping\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     regressor_pipeline.fit(\n\u001b[0m\u001b[1;32m     19\u001b[0m         \u001b[0mX_tr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_tr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mregressor__eval_set\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1387\u001b[0m                 )\n\u001b[1;32m   1388\u001b[0m             ):\n\u001b[0;32m-> 1389\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfit_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1390\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1391\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, **params)\u001b[0m\n\u001b[1;32m    660\u001b[0m                     \u001b[0mall_params\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    661\u001b[0m                 )\n\u001b[0;32m--> 662\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_final_estimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mlast_step_params\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"fit\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    663\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    664\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/xgboost/core.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    724\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    725\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 726\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    727\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    728\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: XGBModel.fit() got an unexpected keyword argument 'early_stopping_rounds'"
          ]
        }
      ]
    }
  ]
}