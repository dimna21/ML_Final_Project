# -*- coding: utf-8 -*-
"""model_experiment_NBEATS.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1T6h8Wuh4cCkhstvKr5LJhhproyUvgSm2
"""

from google.colab import drive
drive.mount('/content/drive')

!pip install neuralforecast
import pandas as pd
import numpy as np
features = pd.read_csv('/content/drive/MyDrive/ML_Final_Project/features.csv')
stores = pd.read_csv('/content/drive/MyDrive/ML_Final_Project/stores.csv')
train = pd.read_csv('/content/drive/MyDrive/ML_Final_Project/train.csv')
test = pd.read_csv('/content/drive/MyDrive/ML_Final_Project/test.csv')
features.columns, stores.columns, train.columns, test.columns

"""# wandb setup"""

!pip install wandb
import wandb
wandb.login()
WANDB_API_KEY  = 'f8a227b42dc881e037b25911fa86b8a491fc0581'

"""# preprocessor"""

from sklearn.base import BaseEstimator, TransformerMixin

class Preprocessor(BaseEstimator, TransformerMixin):
    """
    Turns train/test tables into [unique_id, ds, y, IsHoliday] for NeuralForecast N-BEATS.
    Uses only Store, Dept, Date, Weekly_Sales (or zeros if missing).
    Drops all exogenous features.
    """
    def fit(self, X, y=None):
        return self

    def transform(self, X):
        df = X.copy()
        # parse dates
        df['Date'] = pd.to_datetime(df['Date'])
        # set y: use Weekly_Sales if present, else zeros
        if 'Weekly_Sales' in df.columns:
            df['y'] = df['Weekly_Sales']
        else:
            df['y'] = 0.0
        # build a unique series ID per Store-Dept
        df['unique_id'] = df['Dept'].astype(str) + '_' + df['Store'].astype(str)
        # rename for NeuralForecast
        df = df.rename(columns={'Date': 'ds'})
        # select only the needed columns
        return df[['unique_id', 'ds', 'y', 'IsHoliday']]

"""# date splitter"""

import pandas as pd

def split_sales_data(df: pd.DataFrame, split_date : pd.Timestamp = pd.Timestamp('2012-02-15')):
    """
    Splits sales data into train and test sets based on a provided date,
    including the 'Weekly_Sales' column in both dataframes.

    Parameters:
    - df (pd.DataFrame): df to be split.
    - split_date (pd.Timestamp): The cutoff date for train-test split.

    Returns:
    - df_train, df_test: Tuple of training and test dataframes including 'Weekly_Sales'.
    """
    # Ensure Date is in datetime format
    df = df.copy()
    df['Date'] = pd.to_datetime(df['Date'])

    # Split the data based on date
    train_mask = df['Date'] < split_date
    test_mask = df['Date'] >= split_date

    df_train = df[train_mask].copy()
    df_test = df[test_mask].copy()

    return df_train, df_test

"""# preprocees and split for training"""

train_df, val_df = split_sales_data(train)

preprocessor = Preprocessor()
train_df = preprocessor.fit_transform(train_df)
val_df = preprocessor.transform(val_df)

w_train = train_df["IsHoliday"].map({True: 5, False: 1}).values
w_val = val_df["IsHoliday"].map({True: 5, False: 1}).values
train_df = train_df.drop('IsHoliday', axis=1)
val_df = val_df.drop('IsHoliday', axis=1)

X_train = train_df.drop('y', axis=1)
y_train = train_df['y']
X_val = val_df.drop('y', axis=1)
y_val = val_df['y']
train_df.shape, train_df.columns, val_df.shape, val_df.columns

"""# helper fucntions for training"""

import torch
import pandas as pd
import wandb
from neuralforecast.models import NBEATS
from neuralforecast import NeuralForecast
from sklearn.base import BaseEstimator, TransformerMixin
from sklearn.pipeline import Pipeline

# Your W&B‑adapted helpers from before:
import numpy as np
import matplotlib.pyplot as plt
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score


def evaluate_and_plot(model, X, y_true, w, split):
    """
    1) Predict & compute metrics (WMAE, MAE, RMSE, R2)
    2) Log metrics and diagnostic plots to W&B.
    """
    # 1) get the raw predictions DataFrame
    preds_df = model.predict(X)

    # 2) extract the forecast column (it's the last one)
    y_pred = preds_df.iloc[:, -1].to_numpy()

    # now y_true, y_pred, w are all 1-d numeric arrays
    # Compute metrics
    metrics = {
        "WMAE": wmae(y_true, y_pred, w),
        "MAE": mean_absolute_error(y_true, y_pred),
        "RMSE": np.sqrt(mean_squared_error(y_true, y_pred)),
        "R2": r2_score(y_true, y_pred),
    }
    wandb.log({f"{split}_{m}": v for m, v in metrics.items()})
    print(f"[{split.upper()}] " + "  ".join(f"{k}={v:.4f}" for k,v in metrics.items()))

    # Plot 1: Actual vs Predicted
    fig, ax = plt.subplots(figsize=(6,6))
    mask = np.isfinite(y_true) & np.isfinite(y_pred)
    if mask.any():
        ax.scatter(y_true[mask], y_pred[mask], alpha=0.3, s=10)
        mn, mx = y_true[mask].min(), y_true[mask].max()
        ax.plot([mn,mx], [mn,mx], 'r--', linewidth=1)
    ax.set_xlabel("Actual Weekly Sales")
    ax.set_ylabel("Predicted Weekly Sales")
    ax.set_title(f"{split.capitalize()} Actual vs Predicted")
    wandb.log({f"{split}_actual_vs_pred": wandb.Image(fig)})
    plt.close(fig)

    # Plot 2: Residuals vs Predicted
    residuals = y_true - y_pred
    fig, ax = plt.subplots(figsize=(6,4))
    mask = np.isfinite(y_pred) & np.isfinite(residuals)
    if mask.any():
        ax.scatter(y_pred[mask], residuals[mask], alpha=0.3, s=10)
        ax.hlines(0, y_pred[mask].min(), y_pred[mask].max(),
                  colors='r', linestyles='--')
    ax.set_xlabel("Predicted Weekly Sales")
    ax.set_ylabel("Residuals (Actual – Pred)")
    ax.set_title(f"{split.capitalize()} Residuals vs Predicted")
    wandb.log({f"{split}_residuals": wandb.Image(fig)})
    plt.close(fig)

    # Plot 3: WMAE‑weighted error distribution
    fig, ax = plt.subplots(figsize=(6,4))
    mask = np.isfinite(y_true) & np.isfinite(y_pred) & np.isfinite(w)
    if mask.any():
        weighted_errors = w[mask] * np.abs(y_true[mask] - y_pred[mask])
        if weighted_errors.size:
            ax.hist(weighted_errors, bins=50, density=True, alpha=0.7)
            ax.set_xlabel("Weighted Absolute Error")
            ax.set_ylabel("Density")
            ax.set_title(f"{split.capitalize()} WMAE Error Dist.")
            wandb.log({f"{split}_wmae_error_dist": wandb.Image(fig)})
    plt.close(fig)

    return metrics


def plot_overfitting(train_metrics, val_metrics):
    """
    Plot bar chart comparing train vs val for each metric (WMAE, MAE, RMSE, R²)
    and log the figure to W&B.
    """
    metrics = list(train_metrics.keys())
    train_vals = [train_metrics[m] for m in metrics]
    val_vals   = [val_metrics[m]   for m in metrics]

    # Separate R2 from the rest for plotting on secondary axis
    primary_metrics = [m for m in metrics if m != "R2"]
    secondary_metrics = ["R2"]

    x_primary = np.arange(len(primary_metrics))
    x_secondary = [len(primary_metrics)]  # position for R2 on x-axis

    fig, ax1 = plt.subplots(figsize=(10, 5))
    # Primary metrics (WMAE, MAE, RMSE)
    train_primary = [train_metrics[m] for m in primary_metrics]
    val_primary   = [val_metrics[m]   for m in primary_metrics]
    ax1.bar(x_primary - 0.2, train_primary, width=0.4, label='Train', color='skyblue')
    ax1.bar(x_primary + 0.2, val_primary,   width=0.4, label='Val',   color='orange')
    ax1.set_ylabel("Error Metrics")
    ax1.set_xticks(list(x_primary) + x_secondary)
    ax1.set_xticklabels(primary_metrics + secondary_metrics)
    ax1.legend(loc="upper left")

    # R² on secondary y-axis
    ax2 = ax1.twinx()
    train_r2 = train_metrics.get("R2")
    val_r2   = val_metrics.get("R2")
    if train_r2 is not None and val_r2 is not None:
        ax2.bar(x_secondary[0] - 0.2, [train_r2], width=0.4, label='Train R²', color='green')
        ax2.bar(x_secondary[0] + 0.2, [val_r2],   width=0.4, label='Val R²',   color='red')
        ax2.set_ylabel("R² Score")
        ax2.set_ylim(0, 1)

    # Combine legends
    handles1, labels1 = ax1.get_legend_handles_labels()
    handles2, labels2 = ax2.get_legend_handles_labels()
    ax1.legend(handles1 + handles2, labels1 + labels2, loc='upper left')

    plt.title("Train vs Val Metrics (Primary & R²)")
    plt.tight_layout()
    # Log overfitting metrics figure to W&B
    wandb.log({"train_vs_val_metrics": wandb.Image(fig)})
    plt.close(fig)

def wmae(y_true, y_pred, w):
    """Weighted MAE (holidays count 5×) with NaN handling."""
    # Create mask for finite values
    mask = np.isfinite(y_true) & np.isfinite(y_pred) & np.isfinite(w)

    if not mask.any():
        print("⚠️ All values are NaN/infinite in WMAE calculation")
        return np.nan

    # Filter to finite values only
    y_true_clean = y_true[mask]
    y_pred_clean = y_pred[mask]
    w_clean = w[mask]

    if w_clean.sum() == 0:
        print("⚠️ Sum of weights is zero in WMAE calculation")
        return np.nan

    return (w_clean * np.abs(y_true_clean - y_pred_clean)).sum() / w_clean.sum()

"""# NBEATS model subclass"""

# N-BEATS subclass
class N_Beats(NBEATS):
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.optimizer = torch.optim.AdamW(self.parameters(), lr=1e-3)
        self.scheduler = torch.optim.lr_scheduler.StepLR(self.optimizer, step_size=10, gamma=0.9)
    def configure_optimizers(self):
        return {
            "optimizer": self.optimizer,
            "lr_scheduler": {
                "scheduler": self.scheduler,
                "interval": "epoch",
                "frequency": 1
            }
        }

"""# training 1 - test 1"""

class NBEATSModel(BaseEstimator, TransformerMixin):
    def __init__(self):
        device = "gpu" if torch.cuda.is_available() else "cpu"
        model = N_Beats(
            input_size=20, h=24, max_steps=1500, batch_size=64,
            stack_types=["identity","trend","seasonality"],
            n_blocks=[1,1,1], random_seed=42,
            accelerator=device, devices=1,
            logger=True, enable_progress_bar=True, enable_model_summary=True
        )
        self.nf = NeuralForecast(models=[model], freq="W-FRI")
    def fit(self, X, y=None):
        df = pd.DataFrame({"unique_id":X["unique_id"],"ds":X["ds"],"y":y.values})
        df.sort_values(["unique_id","ds"], inplace=True)
        self.nf.fit(df)
        return self
    def predict(self, X=None):
        real = pd.DataFrame({"unique_id":X["unique_id"],"ds":X["ds"],"y":X["y"]})
        preds = self.nf.predict()
        merged = real.merge(preds, on=["unique_id","ds"], how="left").fillna(0)
        return merged

import wandb

# 1) Start a new W&B run
wandb.init(
    project="ML_Final",
    name="NBeats_test_1",
    config={
        "input_size": 20,
        "horizon": 24,
        "max_steps": 1500,
        "batch_size": 64,
        "stack_types": ["identity", "trend", "seasonality"],
        "n_blocks": [1, 1, 1],
        "optimizer": "AdamW",
        "lr_scheduler_step": 10,
        "lr_scheduler_gamma": 0.9
    },
    reinit=True
)

# 2) Instantiate and fit the model
model = NBEATSModel()
model.fit(X_train, y_train)

# Pull out the trained estimator to get its loss_history
est = model.nf.models[0]

# Log final training + validation loss (if available)
try:
    wandb.log({
        "train_loss": est.loss_history["train"][-1],
        "val_loss":   est.loss_history["val"][-1]
    })
except Exception as e:
    print("⚠️ Could not log loss_history:", e)

train_metrics = evaluate_and_plot(model, X_train.assign(y=y_train), y_train.values, w_train, split="train")
val_metrics = evaluate_and_plot(model, X_val.assign(y=y_val), y_val.values, w_val, split="val")

plot_overfitting(train_metrics, val_metrics)

wandb.finish()

"""# training 2 - input size, horiozon 1"""

# N-BEATS subclass
class N_Beats(NBEATS):
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.optimizer = torch.optim.AdamW(self.parameters(), lr=1e-3)
        self.scheduler = torch.optim.lr_scheduler.StepLR(self.optimizer, step_size=10, gamma=0.9)
    def configure_optimizers(self):
        return {
            "optimizer": self.optimizer,
            "lr_scheduler": {
                "scheduler": self.scheduler,
                "interval": "epoch",
                "frequency": 1
            }
        }

import pandas as pd
import torch
from sklearn.base import BaseEstimator, TransformerMixin
from neuralforecast import NeuralForecast

class NBEATSModel(BaseEstimator, TransformerMixin):
    def __init__(self, input_size: int, h: int):
        device = "gpu" if torch.cuda.is_available() else "cpu"
        # instantiate your LightningModule with the given window & horizon
        model = N_Beats(
            input_size   = input_size,
            h            = h,
            max_steps    = 1500,
            batch_size   = 64,
            stack_types  = ["identity","trend","seasonality"],
            n_blocks     = [1,1,1],
            random_seed  = 42,
            accelerator  = device,
            devices      = 1,
            logger       = True,
            enable_progress_bar  = True,
            enable_model_summary = True
        )
        self.nf = NeuralForecast(models=[model], freq="W-FRI")

    def fit(self, X, y=None):
        df = pd.DataFrame({
            "unique_id": X["unique_id"],
            "ds":        pd.to_datetime(X["ds"]),
            "y":         y.values
        })
        df.sort_values(["unique_id","ds"], inplace=True)
        self.nf.fit(df)
        return self

    def predict(self, X=None):
        real  = pd.DataFrame({
            "unique_id": X["unique_id"],
            "ds":        X["ds"],
            "y":         X["y"]
        })
        preds = self.nf.predict()
        return real.merge(preds, on=["unique_id","ds"], how="left").fillna(0)

import wandb

# your list of (input_size, horizon) pairs to try
iz_h = [
    (4,4), (8,4), (16,4), (52,4),
    (4,8), (8,8), (16,8), (52,8)
]

for input_size, h in iz_h:
    print("==================================================================")
    print(f"=========== input size = {input_size}, horizon = {h} =========== ")
    print("==================================================================")
    # 1) start a new run
    run = wandb.init(
        project="ML_Final",
        name=f"NBeats_{input_size}in_{h}h",
        config={
            "input_size": input_size,
            "h":          h,
            "max_steps":  1500,
            "batch_size": 64,
            "stack_types": ["identity","trend","seasonality"],
            "n_blocks":   [1,1,1],
        },
        reinit=True
    )

    # 2) build & train
    model = NBEATSModel(input_size=input_size, h=h)
    model.fit(X_train, y_train)

    # 3) evaluate
    train_m = evaluate_and_plot(
        model,
        X_train.assign(y=y_train),
        y_train.values,
        w_train,
        split="train"
    )
    val_m = evaluate_and_plot(
        model,
        X_val.assign(y=y_val),
        y_val.values,
        w_val,
        split="val"
    )
    plot_overfitting(train_m, val_m)

    # 4) log a summary metric for easy comparison
    wandb.log({
        "val_WMAE": val_m["WMAE"],
        "val_MAE":  val_m["MAE"],
        "val_RMSE": val_m["RMSE"],
        "val_R2":   val_m["R2"]
    })

    # 5) finish
    run.finish()
    print(f"Done run input_size={input_size}, h={h} → val_WMAE={val_m['WMAE']:.2f}")

"""# training 3 - tune input size, horizon 2"""

import wandb

# your list of (input_size, horizon) pairs to try
iz_h = [
    (16,16), (52,16), (16,24), (52,24),
    (16,32), (52,32), (16,40), (52,40)
]

for input_size, h in iz_h:
    print("==================================================================")
    print(f"=========== input size = {input_size}, horizon = {h} =========== ")
    print("==================================================================")
    # 1) start a new run
    run = wandb.init(
        project="ML_Final",
        name=f"NBeats_{input_size}in_{h}h",
        config={
            "input_size": input_size,
            "h":          h,
            "max_steps":  1500,
            "batch_size": 64,
            "stack_types": ["identity","trend","seasonality"],
            "n_blocks":   [1,1,1],
        },
        reinit=True,
        settings=wandb.Settings(_disable_stats=True)
    )

    # 2) build & train
    model = NBEATSModel(input_size=input_size, h=h)
    model.fit(X_train, y_train)

    # 3) evaluate
    train_m = evaluate_and_plot(
        model,
        X_train.assign(y=y_train),
        y_train.values,
        w_train,
        split="train"
    )
    val_m = evaluate_and_plot(
        model,
        X_val.assign(y=y_val),
        y_val.values,
        w_val,
        split="val"
    )
    plot_overfitting(train_m, val_m)

    # 4) log a summary metric for easy comparison
    wandb.log({
        "val_WMAE": val_m["WMAE"],
        "val_MAE":  val_m["MAE"],
        "val_RMSE": val_m["RMSE"],
        "val_R2":   val_m["R2"]
    })

    # 5) finish
    run.finish()
    print(f"Done run input_size={input_size}, h={h} → val_WMAE={val_m['WMAE']:.2f}")

"""# training 3 - tune n blocks"""

import pandas as pd
import torch
from sklearn.base import BaseEstimator, TransformerMixin
from neuralforecast import NeuralForecast

class NBEATSModel(BaseEstimator, TransformerMixin):
    def __init__(self, n_blocks: list):
        device = "gpu" if torch.cuda.is_available() else "cpu"
        # instantiate your LightningModule with the given window & horizon
        model = N_Beats(
            input_size   = 52,
            h            = 40,
            max_steps    = 1500,
            batch_size   = 64,
            stack_types  = ["identity", "trend", "seasonality"],
            n_blocks     = n_blocks,  # <-- TUNABLE
            random_seed  = 42,
            accelerator  = device,
            devices      = 1,
            logger       = True,
            enable_progress_bar  = True,
            enable_model_summary = True
        )
        self.nf = NeuralForecast(models=[model], freq="W-FRI")

    def fit(self, X, y=None):
        df = pd.DataFrame({
            "unique_id": X["unique_id"],
            "ds":        pd.to_datetime(X["ds"]),
            "y":         y.values
        })
        df.sort_values(["unique_id", "ds"], inplace=True)
        self.nf.fit(df)
        return self

    def predict(self, X=None):
        real = pd.DataFrame({
            "unique_id": X["unique_id"],
            "ds":        X["ds"],
            "y":         X["y"]
        })
        preds = self.nf.predict()
        return real.merge(preds, on=["unique_id", "ds"], how="left").fillna(0)

import wandb
wandb.termlog = lambda *args, **kwargs: None  # Suppress termlog output

# your list of (input_size, horizon) pairs to try
n_blocks = [ [2,2,2], [3, 3, 3], [2, 3, 4], [4, 4, 4], [3, 4, 5] ]

for blocks in n_blocks:
    print("==================================================================")
    print(f"=========== n_blocks = {blocks} =========== ")
    print("==================================================================")
    # 1) start a new run
    run = wandb.init(
        project="ML_Final",
        name=f"NBeats_{blocks}_n_blocks",
        config={
            "input_size": 52,
            "h":          40,
            "max_steps":  1500,
            "batch_size": 64,
            "stack_types": ["identity","trend","seasonality"],
            "n_blocks":   blocks,
        },
        reinit=True,
        settings=wandb.Settings(_disable_stats=True)
    )

    # 2) build & train
    model = NBEATSModel(n_blocks=blocks)
    model.fit(X_train, y_train)

    # 3) evaluate
    train_m = evaluate_and_plot(
        model,
        X_train.assign(y=y_train),
        y_train.values,
        w_train,
        split="train"
    )

    val_m = evaluate_and_plot(
        model,
        X_val.assign(y=y_val),
        y_val.values,
        w_val,
        split="val"
    )
    plot_overfitting(train_m, val_m)

    # 4) log a summary metric for easy comparison
    wandb.log({
        "val_WMAE": val_m["WMAE"],
        "val_MAE":  val_m["MAE"],
        "val_RMSE": val_m["RMSE"],
        "val_R2":   val_m["R2"]
    })

    # 5) finish
    run.finish()
    print(f"Done run input_size={input_size}, h={h} → val_WMAE={val_m['WMAE']:.2f}")

"""# trianing 4 - tune batch size, learnig rate"""

import pandas as pd
import torch
from sklearn.base import BaseEstimator, TransformerMixin
from neuralforecast import NeuralForecast
from neuralforecast.models import NBEATS as N_Beats


class NBEATSModel(BaseEstimator, TransformerMixin):
    def __init__(self, input_size : int = 52, h:int = 40, n_blocks: list = [3,3,3],
                 max_steps :int = 1500, batch_size:int = 64,
                 stack_types = ["identity", "trend", "seasonality"]):
        device = "gpu" if torch.cuda.is_available() else "cpu"
        # instantiate your LightningModule with the given window & horizon
        model = N_Beats(
            input_size   = input_size,
            h            = h,
            max_steps    = max_steps,
            batch_size   = batch_size,
            stack_types  = stack_types,
            n_blocks     = n_blocks,  # <-- TUNABLE
            random_seed  = 42,
            accelerator  = device,
            devices      = 1,
            logger       = True,
            enable_progress_bar  = True,
            enable_model_summary = True
        )
        self.nf = NeuralForecast(models=[model], freq="W-FRI")

    def fit(self, X, y=None):
        df = pd.DataFrame({
            "unique_id": X["unique_id"],
            "ds":        pd.to_datetime(X["ds"]),
            "y":         y.values
        })
        df.sort_values(["unique_id", "ds"], inplace=True)
        self.nf.fit(df)
        return self

    def predict(self, X=None):
        real = pd.DataFrame({
            "unique_id": X["unique_id"],
            "ds":        X["ds"],
            "y":         X["y"]
        })
        preds = self.nf.predict()
        return real.merge(preds, on=["unique_id", "ds"], how="left").fillna(0)

import wandb
wandb.termlog = lambda *args, **kwargs: None  # suppress clutter

# List of batch sizes to experiment with
batch_sizes = [16, 32, 64, 128, 256]

for bs in batch_sizes:
    print("==================================================================")
    print(f"=========== batch_size = {bs} =========== ")
    print("==================================================================")
    # 1) start a new run
    run = wandb.init(
        project="ML_Final",
        name=f"NBeats_bs_{bs}",
        config={
            "input_size": 52,
            "h":          40,
            "max_steps":  1500,
            "batch_size": bs,
            "stack_types": ["identity","trend","seasonality"],
            "n_blocks":   [3,3,3],
        },
        reinit=True,
        settings=wandb.Settings(_disable_stats=True)
    )

    # 2) build & train
    model = NBEATSModel(
        input_size=52,
        h=40,
        n_blocks=[3,3,3],
        max_steps=1500,
        batch_size=bs,
        stack_types=["identity","trend","seasonality"],
    )
    model.fit(X_train, y_train)

    # 3) evaluate
    train_m = evaluate_and_plot(
        model,
        X_train.assign(y=y_train),
        y_train.values,
        w_train,
        split="train"
    )

    val_m = evaluate_and_plot(
        model,
        X_val.assign(y=y_val),
        y_val.values,
        w_val,
        split="val"
    )
    plot_overfitting(train_m, val_m)

    # 4) log summary
    wandb.log({
        "val_WMAE": val_m["WMAE"],
        "val_MAE":  val_m["MAE"],
        "val_RMSE": val_m["RMSE"],
        "val_R2":   val_m["R2"]
    })

    # 5) finish
    run.finish()
    print(f"Done run batch_size={bs} → val_WMAE={val_m['WMAE']:.2f}")

"""# training 5 - tune stack types"""

import wandb
wandb.termlog = lambda *args, **kwargs: None  # suppress clutter

# Different stack configurations to try
stack_configs = [
    ["identity"],
    ["trend"],
    ["seasonality"],
    ["identity", "trend"],
    ["identity", "seasonality"],
    ["trend", "seasonality"],
    ["identity", "trend", "seasonality"],  # default
]

for stacks in stack_configs:
    print("==================================================================")
    print(f"=========== stack_types = {stacks} =========== ")
    print("==================================================================")

    run = wandb.init(
        project="ML_Final",
        name=f"NBeats_stacks_{'+'.join(stacks)}",
        config={
            "input_size":  52,
            "h":           40,
            "max_steps":   1500,
            "batch_size":  64,
            "stack_types": stacks,
            "n_blocks":    [3, 3, 3],
        },
        reinit=True,
        settings=wandb.Settings(_disable_stats=True)
    )

    # build & train with this stack configuration
    model = NBEATSModel(
        input_size=52,
        h=40,
        n_blocks=[3,3,3],
        max_steps=1500,
        batch_size=64,
        stack_types=stacks,
    )
    model.fit(X_train, y_train)

    val_m = evaluate_and_plot(
        model,
        X_val.assign(y=y_val),
        y_val.values,
        w_val,
        split="val"
    )
    plot_overfitting(train_m, val_m)

    # log summary metrics
    wandb.log({
        "val_WMAE": val_m["WMAE"],
        "val_MAE":  val_m["MAE"],
        "val_RMSE": val_m["RMSE"],
        "val_R2":   val_m["R2"]
    })

    run.finish()
    print(f"Done run stacks={stacks} → val_WMAE={val_m['WMAE']:.2f}")

"""# trianing 6 - tune n_harmonics/polynomicals"""

import pandas as pd
import torch
from sklearn.base import BaseEstimator, TransformerMixin
from neuralforecast import NeuralForecast
from neuralforecast.models import NBEATS as N_Beats

class NBEATSModel(BaseEstimator, TransformerMixin):
    def __init__(
        self,
        input_size: int = 52,
        h: int = 40,
        n_blocks: list = [3, 3, 3],
        max_steps: int = 1500,
        batch_size: int = 64,
        stack_types = ["identity", "trend", "seasonality"],
        n_polynomials: int = 2,
        n_harmonics: int   = 2,
    ):
        device = "gpu" if torch.cuda.is_available() else "cpu"

        model = N_Beats(
            input_size       = input_size,
            h                = h,
            max_steps        = max_steps,
            batch_size       = batch_size,
            stack_types      = stack_types,
            n_blocks         = n_blocks,
            n_polynomials    = n_polynomials,    # <-- tunable
            n_harmonics      = n_harmonics,      # <-- tunable
            random_seed      = 42,
            accelerator      = device,
            devices          = 1,
            logger           = True,
            enable_progress_bar  = True,
            enable_model_summary = True
        )
        self.nf = NeuralForecast(models=[model], freq="W-FRI")

    def fit(self, X, y=None):
        df = pd.DataFrame({
            "unique_id": X["unique_id"],
            "ds":        pd.to_datetime(X["ds"]),
            "y":         y.values
        })
        df.sort_values(["unique_id", "ds"], inplace=True)
        self.nf.fit(df)
        return self

    def predict(self, X=None):
        real = pd.DataFrame({
            "unique_id": X["unique_id"],
            "ds":        X["ds"],
            "y":         X["y"]
        })
        preds = self.nf.predict()
        return real.merge(preds, on=["unique_id", "ds"], how="left").fillna(0)

import wandb
wandb.termlog = lambda *args, **kwargs: None  # suppress clutter

# small grid of polynomial degrees and harmonic counts
poly_list = [1, 3]
harm_list = [1, 3]

for p in poly_list:
    for h in harm_list:
        print("="*70)
        print(f"===== n_polynomials={p}, n_harmonics={h} =====")
        print("="*70)

        run = wandb.init(
            project="ML_Final",
            name=f"NBeats_poly{p}_harm{h}",
            config={
                "input_size":     52,
                "h":              40,
                "max_steps":      1500,
                "batch_size":     64,
                "stack_types":    ["identity","trend","seasonality"],
                "n_blocks":       [3,3,3],
                "n_polynomials":  p,
                "n_harmonics":    h,
            },
            reinit=True,
            settings=wandb.Settings(_disable_stats=True)
        )

        # build & train with these basis settings
        model = NBEATSModel(
            input_size     = 52,
            h              = 40,
            n_blocks       = [3,3,3],
            max_steps      = 1500,
            batch_size     = 64,
            stack_types    = ["identity","trend","seasonality"],
            n_polynomials  = p,
            n_harmonics    = h,
        )
        model.fit(X_train, y_train)

        # evaluate
        train_m = evaluate_and_plot(
            model,
            X_train.assign(y=y_train),
            y_train.values,
            w_train,
            split="train"
        )
        val_m = evaluate_and_plot(
            model,
            X_val.assign(y=y_val),
            y_val.values,
            w_val,
            split="val"
        )
        plot_overfitting(train_m, val_m)

        # log summary metrics
        wandb.log({
            "val_WMAE": val_m["WMAE"],
            "val_MAE":  val_m["MAE"],
            "val_RMSE": val_m["RMSE"],
            "val_R2":   val_m["R2"]
        })

        run.finish()
        print(f"Done run p={p}, h={h} → val_WMAE={val_m['WMAE']:.2f}")

"""# trianing 7 - tune lr max_steps"""

import torch
from neuralforecast.models import NBEATS

class N_Beats(NBEATS):
    def __init__(
        self,
        *args,
        learning_rate: float = 1e-3,
        step_size: int     = 10,
        gamma: float       = 0.9,
        **kwargs
    ):
        super().__init__(*args, **kwargs)
        # Use the provided learning rate
        self.optimizer = torch.optim.AdamW(self.parameters(), lr=learning_rate)
        # Scheduler with provided step_size & gamma
        self.scheduler = torch.optim.lr_scheduler.StepLR(
            self.optimizer, step_size=step_size, gamma=gamma
        )

    def configure_optimizers(self):
        return {
            "optimizer": self.optimizer,
            "lr_scheduler": {
                "scheduler": self.scheduler,
                "interval": "epoch",
                "frequency": 1
            }
        }

import pandas as pd
import torch
from sklearn.base import BaseEstimator, TransformerMixin
from neuralforecast import NeuralForecast

class NBEATSModel(BaseEstimator, TransformerMixin):
    def __init__(
        self,
        input_size: int = 52,
        h: int = 40,
        n_blocks: list = [3, 3, 3],
        max_steps: int = 1500,
        batch_size: int = 64,
        stack_types = ["identity", "trend", "seasonality"],
        n_polynomials: int = 2,
        n_harmonics: int = 2,
        learning_rate: float = 1e-3,
        step_size: int = 10,
        gamma: float = 0.9,
    ):
        device = "gpu" if torch.cuda.is_available() else "cpu"

        model = N_Beats(
            input_size       = input_size,
            h                = h,
            max_steps        = max_steps,
            batch_size       = batch_size,
            stack_types      = stack_types,
            n_blocks         = n_blocks,
            n_polynomials    = n_polynomials,
            n_harmonics      = n_harmonics,
            learning_rate    = learning_rate,   # passed to subclass
            step_size        = step_size,
            gamma            = gamma,
            random_seed      = 42,
            accelerator      = device,
            devices          = 1,
            logger           = True,
            enable_progress_bar  = True,
            enable_model_summary = True
        )
        self.nf = NeuralForecast(models=[model], freq="W-FRI")

    def fit(self, X, y=None):
        df = pd.DataFrame({
            "unique_id": X["unique_id"],
            "ds":        pd.to_datetime(X["ds"]),
            "y":         y.values
        })
        df.sort_values(["unique_id", "ds"], inplace=True)
        self.nf.fit(df)
        return self

    def predict(self, X=None):
        real = pd.DataFrame({
            "unique_id": X["unique_id"],
            "ds":        X["ds"],
            "y":         X["y"]
        })
        preds = self.nf.predict()
        return real.merge(preds, on=["unique_id", "ds"], how="left").fillna(0)

import wandb
wandb.termlog = lambda *args, **kwargs: None  # suppress clutter

lr_ms = [(1e-2, 500), (5e-3, 1000), (5e-3, 1500), (2e-3, 2000), (1e-3, 2000)]
for lr, ms in lr_ms:
        print("="*60)
        print(f"===== learning_rate={lr}, max_steps={ms} =====")
        print("="*60)

        run = wandb.init(
            project="ML_Final",
            name=f"NBeats_lr{lr}_steps{ms}",
            config={
                "input_size":    52,
                "h":             40,
                "max_steps":     ms,
                "batch_size":    64,
                "stack_types":   ["identity","trend","seasonality"],
                "n_blocks":      [3,3,3],
                "n_polynomials": 2,
                "n_harmonics":   2,
                "learning_rate": lr,
                "step_size":     10,
                "gamma":         0.9
            },
            reinit=True,
            settings=wandb.Settings(_disable_stats=True)
        )

        # build & train
        model = NBEATSModel(
            input_size     = 52,
            h              = 40,
            n_blocks       = [3,3,3],
            max_steps      = ms,
            batch_size     = 64,
            stack_types    = ["identity","trend","seasonality"],
            n_polynomials  = 2,
            n_harmonics    = 2,
            learning_rate  = lr,
            step_size      = 10,
            gamma          = 0.9,
        )
        model.fit(X_train, y_train)

        # evaluate
        train_m = evaluate_and_plot(
            model,
            X_train.assign(y=y_train),
            y_train.values,
            w_train,
            split="train"
        )
        val_m = evaluate_and_plot(
            model,
            X_val.assign(y=y_val),
            y_val.values,
            w_val,
            split="val"
        )
        plot_overfitting(train_m, val_m)

        # log summary metrics
        wandb.log({
            "val_WMAE": val_m["WMAE"],
            "val_MAE":  val_m["MAE"],
            "val_RMSE": val_m["RMSE"],
            "val_R2":   val_m["R2"]
        })

        run.finish()
        print(f"Done run lr={lr}, steps={ms} → val_WMAE={val_m['WMAE']:.2f}")

"""# predict on best model"""

import pandas as pd
import torch
from sklearn.base import BaseEstimator, TransformerMixin
from neuralforecast import NeuralForecast
from neuralforecast.models import NBEATS as N_Beats

class NBEATSModel(BaseEstimator, TransformerMixin):
    def __init__(
        self,
        input_size: int = 52,
        h: int = 40,
        n_blocks: list = [3, 3, 3],
        max_steps: int = 2000,
        batch_size: int = 64,
        stack_types = ["identity", "trend", "seasonality"],
        n_polynomials: int = 2,
        n_harmonics: int = 2,
        learning_rate: float = 0.001,
        step_size: int = 10,
        gamma: float = 0.9,
    ):
        device = "gpu" if torch.cuda.is_available() else "cpu"

        model = N_Beats(
            input_size          = input_size,
            h                   = h,
            max_steps           = max_steps,
            batch_size          = batch_size,
            stack_types         = stack_types,
            n_blocks            = n_blocks,
            n_polynomials       = n_polynomials,
            n_harmonics         = n_harmonics,
            learning_rate       = learning_rate,
            #gamma               = gamma,
            #random_seed         = 42,
            accelerator         = device,
            devices             = 1,
            logger              = True,
            enable_progress_bar = True,
            enable_model_summary= True
        )
        self.nf = NeuralForecast(models=[model], freq="W-FRI")

    def fit(self, X, y=None):
        df = pd.DataFrame({
            "unique_id": X["unique_id"],
            "ds":        pd.to_datetime(X["ds"]),
            "y":         y.values
        })
        df.sort_values(["unique_id", "ds"], inplace=True)
        self.nf.fit(df)
        return self

    def predict(self, X=None):
        real = pd.DataFrame({
            "unique_id": X["unique_id"],
            "ds":        X["ds"],
            "y":         X["y"]
        })
        preds = self.nf.predict()
        return real.merge(preds, on=["unique_id", "ds"], how="left").fillna(0)

preprocessor = Preprocessor()
train_df = preprocessor.fit_transform(train)
test_df = preprocessor.transform(test)

train_df = train_df.drop('IsHoliday', axis=1)
X_test = test_df.drop('IsHoliday', axis=1)

X_train = train_df.drop('y', axis=1)
y_train = train_df['y']

X_train.shape, y_train.shape, X_train.columns, X_test.columns, X_test.shape

# Cell X: Generate submission properly

def generate_submission(model, test_features, output_filename):
    """
    model: your trained NBEATSModel
    test_features: DataFrame with columns ['unique_id','ds','y'] (y may be dummy)
    full_test: the original Kaggle test DataFrame with Store, Dept, Date
    """
    preds_df = model.predict(test_features)

    # Grab only the forecast column (last column)
    y_pred = preds_df.iloc[:, -1].reset_index(drop=True)

    submission = pd.DataFrame({
        "Id": test["Store"].astype(str)
              + "_" + test["Dept"].astype(str)
              + "_" + test["Date"].astype(str),
        "Weekly_Sales": y_pred
    })

    submission.to_csv(output_filename, index=False)
    print(f"✅ Wrote {output_filename}")

best_model = NBEATSModel()
best_model.fit(X_train, y_train)
generate_submission(best_model, X_test, "NBEATS_BEST_MODEL.csv")

"""# plot hyperparam effects on best model

# input size
"""

# Cell 2: Sweep input_size → separate W&B run per config, collect results

import wandb

input_sizes = [4, 12, 26, 52]
h = 40
max_steps = 2000
learning_rate = 0.001

results = []

for input_size in input_sizes:
    print("\n" + "="*60)
    print(f"Training with input_size = {input_size}")
    print("="*60)

    # 1) New W&B run per input_size
    run = wandb.init(
        project="ML_Final",
        name=f"NBeats_input{input_size}",
        config={
            "input_size":     input_size,
            "h":              h,
            "max_steps":      max_steps,
            "learning_rate":  learning_rate,
            "batch_size":     64,
            "n_blocks":       [3,3,3],
            "stack_types":    ["identity","trend","seasonality"],
            "n_polynomials":  2,
            "n_harmonics":    2,
        },
        reinit=True,
        settings=wandb.Settings(_disable_stats=True)
    )

    # 2) Build & train
    model = NBEATSModel(
        input_size     = input_size,
        h              = h,
        n_blocks       = [3,3,3],
        max_steps      = max_steps,
        batch_size     = 64,
        stack_types    = ["identity","trend","seasonality"],
        n_polynomials  = 2,
        n_harmonics    = 2,
        learning_rate  = learning_rate
    )
    model.fit(X_train, y_train)

    # 3) Evaluate on validation
    val_m = evaluate_and_plot(
        model,
        X_val.assign(y=y_val),
        y_val.values,
        w_val,
        split="val"
    )
    val_wmae = val_m["WMAE"]
    results.append((input_size, val_wmae))

    # 4) Log summary metric
    wandb.log({"val_WMAE": val_wmae})
    run.finish()
    print(f"→ Done input_size={input_size}, val_WMAE={val_wmae:.2f}")

# Cell 3: Bar chart in Colab comparing validation WMAE vs input_size

import matplotlib.pyplot as plt

# Unzip results
sizes, wmaes = zip(*results)

fig, ax = plt.subplots(figsize=(6,4))
ax.bar([str(s) for s in sizes], wmaes, width=0.6)
ax.set_xlabel("Input Size (weeks)")
ax.set_ylabel("Validation WMAE")
ax.set_title("Validation WMAE by Input Size")
plt.show()

"""# horizon"""

# Cell 4: Sweep over horizon h with fixed input_size, max_steps, learning_rate

import wandb

# Sweep horizons to try (in weeks)
horizons = [4, 8, 16, 40]
input_size = 52
max_steps = 2000
learning_rate = 0.001

h_results = []

for h in horizons:
    print("\n" + "="*60)
    print(f"Training with horizon h = {h}")
    print("="*60)

    run = wandb.init(
        project="ML_Final",
        name=f"NBeats_h{h}",
        config={
            "input_size":     input_size,
            "h":              h,
            "max_steps":      max_steps,
            "learning_rate":  learning_rate,
            "batch_size":     64,
            "n_blocks":       [3,3,3],
            "stack_types":    ["identity","trend","seasonality"],
            "n_polynomials":  2,
            "n_harmonics":    2,
        },
        reinit=True,
        settings=wandb.Settings(_disable_stats=True)
    )

    # build & train
    model = NBEATSModel(
        input_size     = input_size,
        h              = h,
        n_blocks       = [3,3,3],
        max_steps      = max_steps,
        batch_size     = 64,
        stack_types    = ["identity","trend","seasonality"],
        n_polynomials  = 2,
        n_harmonics    = 2,
        learning_rate  = learning_rate
    )
    model.fit(X_train, y_train)

    # evaluate on validation
    val_m = evaluate_and_plot(
        model,
        X_val.assign(y=y_val),
        y_val.values,
        w_val,
        split="val"
    )
    val_wmae = val_m["WMAE"]
    h_results.append((h, val_wmae))

    # log summary metric
    wandb.log({"val_WMAE": val_wmae})
    run.finish()
    print(f"→ Done h={h}, val_WMAE={val_wmae:.2f}")

# Cell 5: Bar chart in Colab comparing Validation WMAE vs horizon h

import matplotlib.pyplot as plt

hs, wmaes_h = zip(*h_results)

fig, ax = plt.subplots(figsize=(6,4))
ax.bar([str(h) for h in hs], wmaes_h, width=0.6)
ax.set_xlabel("Horizon (weeks)")
ax.set_ylabel("Validation WMAE")
ax.set_title("Validation WMAE by Forecast Horizon")
plt.show()