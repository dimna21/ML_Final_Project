{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "8gJcpOJtnETq"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "features = pd.read_csv(\"/content/drive/MyDrive/ML_Final_Project/features.csv\")\n",
        "train = pd.read_csv(\"/content/drive/MyDrive/ML_Final_Project/train.csv\")\n",
        "stores = pd.read_csv(\"/content/drive/MyDrive/ML_Final_Project/stores.csv\")\n",
        "test = pd.read_csv(\"/content/drive/MyDrive/ML_Final_Project/test.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nE1z2HYrAPpf"
      },
      "outputs": [],
      "source": [
        "!pip install dagshub\n",
        "!pip install mlflow\n",
        "import dagshub\n",
        "import mlflow"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dagshub.init(repo_owner='dimna21', repo_name='ML_Final_Project', mlflow=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 65
        },
        "id": "bILt9JaXBLIx",
        "outputId": "ac4518cf-e5f3-48ba-fd78-8361d85cffd8"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Accessing as dimna21\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Accessing as dimna21\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Initialized MLflow to track repo \u001b[32m\"dimna21/ML_Final_Project\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Initialized MLflow to track repo <span style=\"color: #008000; text-decoration-color: #008000\">\"dimna21/ML_Final_Project\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Repository dimna21/ML_Final_Project initialized!\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Repository dimna21/ML_Final_Project initialized!\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "1NzxglB_2ZlF"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from prophet import Prophet\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "import mlflow\n",
        "import dagshub\n",
        "import warnings\n",
        "from tqdm import tqdm\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "\n",
        "class BaseMerger(BaseEstimator, TransformerMixin):\n",
        "\n",
        "    def __init__(self, features, stores):\n",
        "        self.feature_store = features.merge(stores, how='inner', on='Store')\n",
        "        self.feature_store['Date'] = pd.to_datetime(self.feature_store['Date'])\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        X = X.copy()\n",
        "        X['Date'] = pd.to_datetime(X['Date'])\n",
        "        merged = X.merge(self.feature_store, how='inner', on=['Store', 'Date', 'IsHoliday'])\n",
        "        merged = merged.sort_values(by=['Date', 'Store', 'Dept']).reset_index(drop=True)\n",
        "        return merged\n",
        "\n",
        "class FeatureAdder(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self):\n",
        "        self.superbowl = pd.to_datetime(['2010-02-12', '2011-02-11', '2012-02-10', '2013-02-08'])\n",
        "        self.labor_day = pd.to_datetime(['2010-09-10', '2011-09-09', '2012-09-07', '2013-09-06'])\n",
        "        self.thanksgiving = pd.to_datetime(['2010-11-26', '2011-11-25', '2012-11-23', '2013-11-29'])\n",
        "        self.christmas = pd.to_datetime(['2010-12-31', '2011-12-30', '2012-12-28', '2013-12-27'])\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        X = X.copy()\n",
        "\n",
        "        # Convert temperature to Celsius\n",
        "        if 'Temperature' in X.columns:\n",
        "            X['Temperature'] = (X['Temperature'] - 32) * (5.0 / 9.0)\n",
        "\n",
        "        # Basic date parts\n",
        "        X['Day'] = X['Date'].dt.day\n",
        "        X['Month'] = X['Date'].dt.month\n",
        "        X['Year'] = X['Date'].dt.year\n",
        "\n",
        "        # Extract ISO week and year for holiday matching\n",
        "        X['Week'] = X['Date'].dt.isocalendar().week\n",
        "        X['YearNum'] = X['Date'].dt.year\n",
        "\n",
        "        # Helper to flag if a date is in same ISO week/year as a known holiday\n",
        "        def is_holiday_week(date_series, holidays):\n",
        "            holiday_weeks = set((d.isocalendar().week, d.year) for d in holidays)\n",
        "            return date_series.apply(lambda d: (d.isocalendar().week, d.year) in holiday_weeks if pd.notnull(d) else False).astype(int)\n",
        "\n",
        "        X['SuperbowlWeek'] = is_holiday_week(X['Date'], self.superbowl)\n",
        "        X['LaborDayWeek'] = is_holiday_week(X['Date'], self.labor_day)\n",
        "        X['ThanksgivingWeek'] = is_holiday_week(X['Date'], self.thanksgiving)\n",
        "        X['ChristmasWeek'] = is_holiday_week(X['Date'], self.christmas)\n",
        "\n",
        "        # Calculate days to Thanksgiving and Christmas (using Nov 24 and Dec 24 as anchor dates)\n",
        "        thanksgiving_dates = pd.to_datetime(X['Year'].astype(str) + \"-11-24\")\n",
        "        christmas_dates = pd.to_datetime(X['Year'].astype(str) + \"-12-24\")\n",
        "\n",
        "        X['Days_to_Thanksgiving'] = (thanksgiving_dates - X['Date']).dt.days\n",
        "        X['Days_to_Christmas'] = (christmas_dates - X['Date']).dt.days\n",
        "\n",
        "        # Clean up helper cols\n",
        "        X = X.drop(columns=['Week', 'YearNum'])\n",
        "\n",
        "        return X\n",
        "\n",
        "class MissingValueFiller(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self):\n",
        "        self.markdown_cols = ['MarkDown1', 'MarkDown2', 'MarkDown3', 'MarkDown4', 'MarkDown5']\n",
        "        self.mean_cols = ['CPI', 'Unemployment']\n",
        "        self.mean_values = {}\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        for col in self.mean_cols:\n",
        "            if col in X.columns:\n",
        "                self.mean_values[col] = X[col].mean()\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        X = X.copy()\n",
        "\n",
        "        # Fill markdowns with 0\n",
        "        for col in self.markdown_cols:\n",
        "            if col in X.columns:\n",
        "                X[col] = X[col].fillna(0.0)\n",
        "\n",
        "        # Fill CPI and Unemployment with learned mean\n",
        "        for col in self.mean_cols:\n",
        "            if col in X.columns and col in self.mean_values:\n",
        "                X[col] = X[col].fillna(self.mean_values[col])\n",
        "\n",
        "        return X\n",
        "\n",
        "class CategoricalEncoder(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self):\n",
        "        self.type_mapping = {'A': 3, 'B': 2, 'C': 1}\n",
        "        self.holiday_mapping = {False: 0, True: 1}\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        X = X.copy()\n",
        "\n",
        "        if 'Type' in X.columns:\n",
        "            X['Type'] = X['Type'].map(self.type_mapping)\n",
        "\n",
        "        if 'IsHoliday' in X.columns:\n",
        "            X['IsHoliday'] = X['IsHoliday'].map(self.holiday_mapping)\n",
        "\n",
        "        return X\n",
        "\n",
        "class StoreAggregator(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self):\n",
        "        self.timeseries = {}\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        self.timeseries = {}\n",
        "        for store in X['Store'].unique():\n",
        "            self.aggregate_store_info(store, X)\n",
        "        return self.timeseries\n",
        "\n",
        "    def aggregate_store_info(self, store_id, X):\n",
        "        store_data = X[X['Store'] == store_id].copy()\n",
        "\n",
        "        # Check if Weekly_Sales exists (train data) or not (test data)\n",
        "        has_weekly_sales = 'Weekly_Sales' in store_data.columns\n",
        "\n",
        "        if has_weekly_sales:\n",
        "            sum_columns = ['Weekly_Sales', 'MarkDown1', 'MarkDown2', 'MarkDown3', 'MarkDown4', 'MarkDown5']\n",
        "        else:\n",
        "            sum_columns = ['MarkDown1', 'MarkDown2', 'MarkDown3', 'MarkDown4', 'MarkDown5']\n",
        "\n",
        "        first_columns = ['IsHoliday', 'Temperature', 'Fuel_Price', 'CPI', 'Unemployment',\n",
        "                        'Type', 'Size', 'Day', 'Month', 'Year', 'SuperbowlWeek',\n",
        "                        'LaborDayWeek', 'ThanksgivingWeek', 'ChristmasWeek',\n",
        "                        'Days_to_Thanksgiving', 'Days_to_Christmas']\n",
        "\n",
        "        agg_dict = {}\n",
        "\n",
        "        # Add sum columns that exist in the data\n",
        "        for col in sum_columns:\n",
        "            if col in store_data.columns:\n",
        "                agg_dict[col] = 'sum'\n",
        "\n",
        "        # Add first columns that exist in the data\n",
        "        for col in first_columns:\n",
        "            if col in store_data.columns:\n",
        "                agg_dict[col] = 'first'\n",
        "\n",
        "        aggregated = store_data.groupby(['Date', 'Store']).agg(agg_dict).reset_index()\n",
        "        aggregated = aggregated.sort_values('Date').reset_index(drop=True)\n",
        "\n",
        "        # Calculate department proportions only if Weekly_Sales exists\n",
        "        if has_weekly_sales:\n",
        "            dept_proportions = self.calculate_dept_proportions(store_data)\n",
        "        else:\n",
        "            dept_proportions = None\n",
        "\n",
        "        self.timeseries[store_id] = (aggregated, dept_proportions)\n",
        "        return aggregated\n",
        "\n",
        "    def calculate_dept_proportions(self, store_data):\n",
        "        dept_totals = store_data.groupby('Dept')['Weekly_Sales'].sum()\n",
        "        store_total = store_data['Weekly_Sales'].sum()\n",
        "\n",
        "        if store_total == 0:\n",
        "            num_depts = len(dept_totals)\n",
        "            return {dept: 1.0/num_depts for dept in dept_totals.index}\n",
        "\n",
        "        dept_proportions = (dept_totals / store_total).to_dict()\n",
        "        return dept_proportions"
      ],
      "metadata": {
        "id": "a9GK7Z9sBX0w"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "Hvafta6j_exc"
      },
      "outputs": [],
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "# Pipeline with store aggregation\n",
        "pipeline = Pipeline([\n",
        "   ('merge', BaseMerger(features, stores)),\n",
        "   ('feature_add', FeatureAdder()),\n",
        "   ('value_fill', MissingValueFiller()),\n",
        "   ('cat_encoder', CategoricalEncoder()),\n",
        "   ('store_agg', StoreAggregator())\n",
        "])\n",
        "\n",
        "# Transform training data\n",
        "train_aggregated = pipeline.fit_transform(train)\n",
        "test_aggregated = pipeline.transform(test)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "from prophet import Prophet\n",
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Features to use as regressors\n",
        "'''\n",
        "ls = ['IsHoliday', 'Type', 'Size', 'SuperbowlWeek',\n",
        "     'LaborDayWeek', 'ThanksgivingWeek', 'ChristmasWeek',\n",
        "     'Days_to_Thanksgiving', 'Days_to_Christmas']\n",
        "'''\n",
        "ls = ['IsHoliday', 'SuperbowlWeek', 'LaborDayWeek', 'ThanksgivingWeek', 'ChristmasWeek']\n",
        "\n",
        "store_models = {}\n",
        "\n",
        "for store_id in tqdm(train_aggregated.keys(), desc=\"Training Prophet models\"):\n",
        "   store_ts, dept_props = train_aggregated[store_id]\n",
        "   try:\n",
        "       # Prepare data for Prophet\n",
        "       df_prophet = store_ts[['Date', 'Weekly_Sales']].copy()\n",
        "       df_prophet.columns = ['ds', 'y']\n",
        "       df_prophet['ds'] = pd.to_datetime(df_prophet['ds'])\n",
        "\n",
        "       # Add regressors that exist in the data\n",
        "       available_regressors = []\n",
        "       for regressor in ls:\n",
        "           if regressor in store_ts.columns:\n",
        "               df_prophet[regressor] = store_ts[regressor].values\n",
        "               available_regressors.append(regressor)\n",
        "\n",
        "       # Sort by date\n",
        "       df_prophet = df_prophet.sort_values('ds').reset_index(drop=True)\n",
        "\n",
        "       # Initialize Prophet model\n",
        "       model = Prophet(\n",
        "           yearly_seasonality=True,\n",
        "           weekly_seasonality=False,\n",
        "           daily_seasonality=False,\n",
        "           changepoint_prior_scale=0.01,\n",
        "           seasonality_prior_scale=10.0,\n",
        "           holidays_prior_scale=15.0,\n",
        "           mcmc_samples=0,\n",
        "           interval_width=0.8,\n",
        "           growth='flat'\n",
        "       )\n",
        "\n",
        "       # Add US holidays\n",
        "       model.add_country_holidays(country_name='US')\n",
        "\n",
        "       # Add available regressors\n",
        "       for regressor in available_regressors:\n",
        "           model.add_regressor(regressor)\n",
        "\n",
        "       # Fit the model\n",
        "       model.fit(df_prophet)\n",
        "\n",
        "       # Store the fitted model with department proportions and available regressors\n",
        "       store_models[store_id] = (model, dept_props, available_regressors)\n",
        "\n",
        "   except Exception as e:\n",
        "       print(f\"Failed to train Prophet model for Store {store_id}: {e}\")\n",
        "       continue\n",
        "\n",
        "print(f\"Successfully trained {len(store_models)} Prophet store models\")"
      ],
      "metadata": {
        "id": "CIel5_DSJiuS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import mlflow\n",
        "import dagshub\n",
        "import pickle\n",
        "import os\n",
        "from datetime import datetime\n",
        "\n",
        "# Set up MLflow experiment\n",
        "mlflow.set_experiment(\"Prophet_Store_Models_Minimal_Features\")\n",
        "\n",
        "# Start MLflow run\n",
        "with mlflow.start_run(run_name=f\"Prophet_Store_Training_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"):\n",
        "\n",
        "    # Log experiment parameters\n",
        "    mlflow.log_param(\"model_type\", \"Prophet\")\n",
        "    mlflow.log_param(\"approach\", \"store_level_aggregated\")\n",
        "    mlflow.log_param(\"regressors\", \", \".join(ls))\n",
        "    mlflow.log_param(\"num_regressors\", len(ls))\n",
        "    mlflow.log_param(\"total_stores\", len(train_aggregated))\n",
        "    mlflow.log_param(\"successful_models\", len(store_models))\n",
        "    mlflow.log_param(\"failed_models\", len(train_aggregated) - len(store_models))\n",
        "\n",
        "    # Log Prophet model parameters\n",
        "    mlflow.log_param(\"yearly_seasonality\", True)\n",
        "    mlflow.log_param(\"weekly_seasonality\", False)\n",
        "    mlflow.log_param(\"daily_seasonality\", False)\n",
        "    mlflow.log_param(\"changepoint_prior_scale\", 0.05)\n",
        "    mlflow.log_param(\"seasonality_prior_scale\", 10.0)\n",
        "    mlflow.log_param(\"holidays_prior_scale\", 10.0)\n",
        "    mlflow.log_param(\"mcmc_samples\", 0)\n",
        "    mlflow.log_param(\"interval_width\", 0.8)\n",
        "    mlflow.log_param(\"growth\", \"linear\")\n",
        "    mlflow.log_param(\"country_holidays\", \"US\")\n",
        "\n",
        "    # Log metrics\n",
        "    mlflow.log_metric(\"model_success_rate\", len(store_models) / len(train_aggregated))\n",
        "\n",
        "    # Log individual store model info\n",
        "    store_info = []\n",
        "    total_training_periods = 0\n",
        "    total_departments = 0\n",
        "\n",
        "\n",
        "    # Log aggregate metrics\n",
        "    mlflow.log_metric(\"avg_training_periods_per_store\", total_training_periods / len(store_models))\n",
        "    mlflow.log_metric(\"avg_departments_per_store\", total_departments / len(store_models))\n",
        "    mlflow.log_metric(\"total_departments\", total_departments)\n",
        "\n",
        "    # Save store information as artifact\n",
        "    store_info_df = pd.DataFrame(store_info)\n",
        "    store_info_path = \"/content/drive/MyDrive/ML_Final_Project/prophet_store_info.csv\"\n",
        "    store_info_df.to_csv(store_info_path, index=False)\n",
        "    mlflow.log_artifact(store_info_path)\n",
        "\n",
        "    # Save the complete store_models dictionary as pickle\n",
        "    models_path = \"/content/drive/MyDrive/ML_Final_Project/prophet_store_models.pkl\"\n",
        "    with open(models_path, 'wb') as f:\n",
        "        pickle.dump(store_models, f)\n",
        "    mlflow.log_artifact(models_path)\n",
        "\n",
        "    # Log feature list as artifact\n",
        "    features_path = \"/content/drive/MyDrive/ML_Final_Project/prophet_features_used.txt\"\n",
        "    with open(features_path, 'w') as f:\n",
        "        f.write(\"Features used in Prophet models:\\n\")\n",
        "        for feature in ls:\n",
        "            f.write(f\"- {feature}\\n\")\n",
        "    mlflow.log_artifact(features_path)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G5XoNa_BJuZB",
        "outputId": "4bd4fbf0-f078-4e34-9731-803e1e6982b3"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìä LOGGED TO MLFLOW:\n",
            "Successful models: 45/45\n",
            "Success rate: 100.00%\n",
            "Average training periods per store: 0.0\n",
            "Average departments per store: 0.0\n",
            "Features used: IsHoliday, SuperbowlWeek, LaborDayWeek, ThanksgivingWeek, ChristmasWeek\n",
            "\n",
            "üìÅ ARTIFACTS LOGGED:\n",
            "- Complete models: prophet_store_models.pkl (1.5 MB)\n",
            "- Store information: prophet_store_info.csv\n",
            "- Features used: prophet_features_used.txt\n",
            "üèÉ View run Prophet_Store_Training_20250729_134714 at: https://dagshub.com/dimna21/ML_Final_Project.mlflow/#/experiments/11/runs/d7f9664319674bdfad7e120cc7582565\n",
            "üß™ View experiment at: https://dagshub.com/dimna21/ML_Final_Project.mlflow/#/experiments/11\n",
            "‚úÖ MLflow logging completed!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "max_train_date = pd.to_datetime(train['Date'].max())\n",
        "\n",
        "def get_forecast(idx):\n",
        "   sample = test.iloc[idx]\n",
        "   store = sample['Store']\n",
        "   dept = sample['Dept']\n",
        "   pred_date = pd.to_datetime(sample['Date'])\n",
        "\n",
        "\n",
        "   try:\n",
        "       store_model, dept_props, available_regressors = store_models[store]\n",
        "\n",
        "       # Get test data for this store\n",
        "       if store not in test_aggregated:\n",
        "           return 1000.0\n",
        "\n",
        "       df, _ = test_aggregated[store]\n",
        "       df = df.copy()\n",
        "       df['Date'] = pd.to_datetime(df['Date'])\n",
        "       df = df.sort_values('Date')\n",
        "\n",
        "       # Get all test dates up to and including the prediction date\n",
        "       relevant_data = df[df['Date'] <= pred_date].copy()\n",
        "\n",
        "       if len(relevant_data) == 0:\n",
        "           return 1000.0\n",
        "\n",
        "       # Prepare future dataframe for Prophet\n",
        "       future_data = {\n",
        "           'ds': relevant_data['Date'].tolist()\n",
        "       }\n",
        "\n",
        "       # Add available regressors\n",
        "       for regressor in available_regressors:\n",
        "           if regressor in relevant_data.columns:\n",
        "               future_data[regressor] = relevant_data[regressor].tolist()\n",
        "\n",
        "       future_df = pd.DataFrame(future_data)\n",
        "\n",
        "       # Make Prophet forecast\n",
        "       forecast = store_model.predict(future_df)\n",
        "       forecast['ds'] = pd.to_datetime(forecast['ds'])\n",
        "\n",
        "       # Find the prediction for our target date\n",
        "       target_forecast = forecast[forecast['ds'] == pred_date]\n",
        "\n",
        "       if len(target_forecast) > 0:\n",
        "           store_prediction = max(0, target_forecast['yhat'].iloc[0])\n",
        "\n",
        "           # Apply department proportion\n",
        "           if dept_props and dept in dept_props.keys():\n",
        "               return store_prediction * dept_props[dept]\n",
        "           elif dept_props:\n",
        "               return store_prediction / len(dept_props)\n",
        "           else:\n",
        "               return store_prediction\n",
        "       else:\n",
        "           return 1000.0  # Fallback if date not found\n",
        "\n",
        "   except Exception as e:\n",
        "       print(f\"Forecast failed for Store {store}, Dept {dept}: {e}\")\n",
        "       return 1000.0  # Fallback for any errors"
      ],
      "metadata": {
        "id": "HNy0oItkJxOg"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "from datetime import datetime\n",
        "\n",
        "def process_store_batch(store_data_tuple):\n",
        "    \"\"\"Process all predictions for a single store at once\"\"\"\n",
        "    store_id, store_test_data = store_data_tuple\n",
        "\n",
        "    # Check if store model exists\n",
        "    if store_id not in store_models:\n",
        "        # Return fallback predictions for all rows\n",
        "        results = []\n",
        "        for _, row in store_test_data.iterrows():\n",
        "            results.append({\n",
        "                'Id': f\"{row['Store']}_{row['Dept']}_{pd.to_datetime(row['Date']).strftime('%Y-%m-%d')}\",\n",
        "                'Weekly_Sales': 1000.0\n",
        "            })\n",
        "        return results\n",
        "\n",
        "    try:\n",
        "        store_model, dept_props, available_regressors = store_models[store_id]\n",
        "\n",
        "        # Get test aggregated data for this store\n",
        "        if store_id not in test_aggregated:\n",
        "            # Fallback for missing test data\n",
        "            results = []\n",
        "            for _, row in store_test_data.iterrows():\n",
        "                results.append({\n",
        "                    'Id': f\"{row['Store']}_{row['Dept']}_{pd.to_datetime(row['Date']).strftime('%Y-%m-%d')}\",\n",
        "                    'Weekly_Sales': 1000.0\n",
        "                })\n",
        "            return results\n",
        "\n",
        "        df, _ = test_aggregated[store_id]\n",
        "        df = df.copy()\n",
        "        df['Date'] = pd.to_datetime(df['Date'])\n",
        "        df = df.sort_values('Date')\n",
        "\n",
        "        # Get all unique dates needed for this store\n",
        "        store_test_data['Date'] = pd.to_datetime(store_test_data['Date'])\n",
        "        unique_dates = sorted(store_test_data['Date'].unique())\n",
        "\n",
        "        # Create future dataframe for ALL dates at once\n",
        "        future_data = {'ds': unique_dates}\n",
        "\n",
        "        # Add regressors for all dates\n",
        "        for regressor in available_regressors:\n",
        "            if regressor in df.columns:\n",
        "                # Map regressor values to each unique date\n",
        "                regressor_values = []\n",
        "                for date in unique_dates:\n",
        "                    # Find the regressor value for this date\n",
        "                    date_data = df[df['Date'] <= date]\n",
        "                    if len(date_data) > 0:\n",
        "                        regressor_values.append(date_data[regressor].iloc[-1])\n",
        "                    else:\n",
        "                        regressor_values.append(0)  # Fallback\n",
        "\n",
        "                future_data[regressor] = regressor_values\n",
        "\n",
        "        future_df = pd.DataFrame(future_data)\n",
        "\n",
        "        # SINGLE Prophet prediction call for ALL dates\n",
        "        forecast = store_model.predict(future_df)\n",
        "        forecast['ds'] = pd.to_datetime(forecast['ds'])\n",
        "\n",
        "        # Create a mapping of date -> prediction\n",
        "        date_to_prediction = dict(zip(forecast['ds'], forecast['yhat']))\n",
        "\n",
        "        # Process all test rows for this store\n",
        "        results = []\n",
        "        for _, row in store_test_data.iterrows():\n",
        "            pred_date = pd.to_datetime(row['Date'])\n",
        "            dept = row['Dept']\n",
        "\n",
        "            # Get store-level prediction\n",
        "            if pred_date in date_to_prediction:\n",
        "                store_prediction = max(0, date_to_prediction[pred_date])\n",
        "\n",
        "                # Apply department proportion\n",
        "                if dept_props and dept in dept_props:\n",
        "                    final_prediction = store_prediction * dept_props[dept]\n",
        "                elif dept_props:\n",
        "                    final_prediction = store_prediction / len(dept_props)\n",
        "                else:\n",
        "                    final_prediction = store_prediction\n",
        "            else:\n",
        "                final_prediction = 1000.0\n",
        "\n",
        "            results.append({\n",
        "                'Id': f\"{row['Store']}_{row['Dept']}_{pred_date.strftime('%Y-%m-%d')}\",\n",
        "                'Weekly_Sales': max(0, final_prediction)\n",
        "            })\n",
        "\n",
        "        return results\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Batch processing failed for Store {store_id}: {e}\")\n",
        "        results = []\n",
        "        for _, row in store_test_data.iterrows():\n",
        "            results.append({\n",
        "                'Id': f\"{row['Store']}_{row['Dept']}_{pd.to_datetime(row['Date']).strftime('%Y-%m-%d')}\",\n",
        "                'Weekly_Sales': 1000.0\n",
        "            })\n",
        "        return results\n",
        "\n",
        "test_by_store = list(test.groupby('Store'))\n",
        "\n",
        "\n",
        "# Process stores in parallel\n",
        "submission_data = []\n",
        "with ThreadPoolExecutor(max_workers=4) as executor:\n",
        "    futures = [executor.submit(process_store_batch, store_data) for store_data in test_by_store]\n",
        "\n",
        "    for future in tqdm(as_completed(futures), total=len(futures)):\n",
        "        batch_results = future.result()\n",
        "        submission_data.extend(batch_results)\n",
        "\n",
        "# Save the results\n",
        "submission_df = pd.DataFrame(submission_data)\n",
        "submission_df = submission_df.sort_values('Id')\n",
        "submission_df.to_csv(\"/content/drive/MyDrive/ML_Final_Project/Prophet_Store_Submission_2.csv\", index=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QZzrP-g0Jxp7",
        "outputId": "722f2824-1dbb-4c73-b649-fcbae742e973"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 45/45 [00:21<00:00,  2.09it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sEn1Sk_WXf6I"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}