# -*- coding: utf-8 -*-
"""model_experiment_patchtst.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Kz1s_4zp4kGnqdTWBclfXdRG1AxgGNgA

# import tables / neuralforecast / wandb
"""

from google.colab import drive
drive.mount('/content/drive')

!pip install neuralforecast
!pip install wandb
import wandb
wandb.login()
WANDB_API_KEY  = 'f8a227b42dc881e037b25911fa86b8a491fc0581'

import pandas as pd
import numpy as np

train = pd.read_csv('/content/drive/MyDrive/train.csv')
test = pd.read_csv('/content/drive/MyDrive/test.csv')
train.columns, test.columns

"""# split function"""

def split_sales_data(df: pd.DataFrame, split_date : pd.Timestamp = pd.Timestamp('2012-02-15')):
    """
    Splits sales data into train and test sets based on a provided date,
    including the 'Weekly_Sales' column in both dataframes.

    Parameters:
    - df (pd.DataFrame): df to be split.
    - split_date (pd.Timestamp): The cutoff date for train-test split.

    Returns:
    - df_train, df_test: Tuple of training and test dataframes including 'Weekly_Sales'.
    """
    # Ensure Date is in datetime format
    df = df.copy()
    df['Date'] = pd.to_datetime(df['Date'])

    # Split the data based on date
    train_mask = df['Date'] < split_date
    test_mask = df['Date'] >= split_date

    df_train = df[train_mask].copy()
    df_test = df[test_mask].copy()

    return df_train, df_test

"""# nf formatter function"""

def prepare_df_for_nf(df: pd.DataFrame, is_train: bool = True) -> pd.DataFrame:
    df_pre = df.copy()
    # Convert Date to datetime and rename to ds
    df_pre['Date'] = pd.to_datetime(df_pre['Date'])
    df_pre.rename(columns={'Date': 'ds'}, inplace=True)

    # Rename Weekly_Sales to y in training; create dummy y in test
    if is_train:
        df_pre.rename(columns={'Weekly_Sales': 'y'}, inplace=True)
    else:
        df_pre['y'] = np.nan

    # Build unique_id per Store-Dept
    df_pre['unique_id'] = (
        df_pre['Dept'].astype(str) + '_' + df_pre['Store'].astype(str)
    )

    return df_pre

"""# preprocessing data / imports"""

from neuralforecast import NeuralForecast
from neuralforecast.models import PatchTST
from neuralforecast.losses.pytorch import MAE
import pandas as pd
import numpy as np
from sklearn.metrics import mean_absolute_error, mean_squared_error

train_df, val_df = split_sales_data(train)

train_final = prepare_df_for_nf(train_df, is_train=True)
val_final = prepare_df_for_nf(val_df, is_train=False)
test_final = prepare_df_for_nf(test, is_train=False)

y_train = train_final['y']
y_val = val_final['Weekly_Sales']
val_final = val_final.drop(columns=['Weekly_Sales'])
w_val = val_final["IsHoliday"].map({True: 5, False: 1}).values

print(f"\nDate ranges:")
print(f"Train: {train_final['ds'].min()} to {train_final['ds'].max()}")
print(f"Val: {val_final['ds'].min()} to {val_final['ds'].max()}")
print(f"Test: {test_final['ds'].min()} to {test_final['ds'].max()}")
train_final.shape, train_final.columns, val_final.shape, val_final.columns,  test_final.shape, test_final.columns,

"""# shifter"""

def shift_ds_by_days(df: pd.DataFrame, days: int) -> pd.DataFrame:
    """
    Returns a copy of `df` where the 'ds' column has been shifted by `days`.
    Positive `days` moves dates forward; negative moves them backward.
    """
    df_shifted = df.copy()
    df_shifted['ds'] = df_shifted['ds'] + pd.Timedelta(days=days)
    return df_shifted

"""# store/dept check across train/val/test"""

# Cell 4: Analyze ID Consistency (Keep All Validation/Test IDs)
train_ids = set(train_final['unique_id'])
val_ids = set(val_final['unique_id'])
test_ids = set(test_final['unique_id'])

print("=== Unique ID Analysis ===")
print(f"Train unique_ids: {len(train_ids)}")
print(f"Val unique_ids: {len(val_ids)}")
print(f"Test unique_ids: {len(test_ids)}")

# Check for missing IDs
missing_in_val = train_ids - val_ids
missing_in_test = train_ids - test_ids
extra_in_val = val_ids - train_ids
extra_in_test = test_ids - train_ids

print(f"\nID consistency:")
print(f"Missing in val (from train): {len(missing_in_val)}")
print(f"Missing in test (from train): {len(missing_in_test)}")
print(f"Extra in val (not in train): {len(extra_in_val)}")
print(f"Extra in test (not in train): {len(extra_in_test)}")

# Analyze the missing combinations
print(f"\nüîç Analysis of missing store-dept combinations:")
if extra_in_val:
    print(f"New store-dept combinations in validation: {list(extra_in_val)[:5]}...")
if extra_in_test:
    print(f"New store-dept combinations in test: {list(extra_in_test)[:5]}...")

# Check if all stores exist across datasets
def extract_store_from_unique_id(unique_ids):
    """Extract store numbers from unique_id format 'dept_store'"""
    return {uid.split('_')[1] for uid in unique_ids}

train_stores = extract_store_from_unique_id(train_ids)
val_stores = extract_store_from_unique_id(val_ids)
test_stores = extract_store_from_unique_id(test_ids)

print(f"\nüè™ Store-level analysis:")
print(f"Train stores: {len(train_stores)}")
print(f"Val stores: {len(val_stores)}")
print(f"Test stores: {len(test_stores)}")
print(f"Stores missing in val: {len(train_stores - val_stores)}")
print(f"Stores missing in test: {len(train_stores - test_stores)}")

"""# SIMPLE MODEL TRAIN"""

# Cell 7: Calculate Forecast Horizon and Model Parameters
forecast_horizon = len(val_final['ds'].unique())
input_size = min(forecast_horizon * 2, 104)  # Cap at ~2 years of weekly data

print("=== Model Configuration ===")
print(f"Forecast horizon: {forecast_horizon} weeks")
print(f"Input size: {input_size} weeks")
print(f"Input/Output ratio: {input_size/forecast_horizon:.1f}x")

# Verify we have enough history
min_history_per_id = train_final.groupby('unique_id').size().min()
print(f"Minimum history per unique_id: {min_history_per_id} weeks")
if min_history_per_id < input_size:
    print(f"‚ö†Ô∏è  Some series have less history ({min_history_per_id}) than input_size ({input_size})")
    print("Model will handle this with padding")

# Cell 8: Initialize PatchTST Model
patchtst = PatchTST(
    # Forecasting parameters
    h=forecast_horizon,
    input_size=input_size,

    # Architecture parameters
    patch_len=16,  # Length of each patch
    stride=8,      # Stride between patches (patch_len/2)
    encoder_layers=3,  # Number of transformer layers
    n_heads=8,     # Number of attention heads
    hidden_size=128,  # Hidden dimension

    # Training parameters
    loss=MAE(),
    learning_rate=1e-3,
    max_steps=1000,
    val_check_steps=100,
    early_stop_patience_steps=0,  # Disabled since we use val_size=0
    batch_size=32,

    # Regularization
    dropout=0.1,
    fc_dropout=0.1,
    head_dropout=0.1,

    # Normalization and scaling
    scaler_type='robust',
    revin=True,  # Reversible Instance Normalization

    # Other parameters
    enable_progress_bar=True,
    random_seed=42
)

print("PatchTST model initialized:")
print(f"  ‚úÖ Forecast horizon: {patchtst.h} weeks")
print(f"  ‚úÖ Input size: {patchtst.input_size} weeks")
print(f"  ‚úÖ Patch length: {16}")
print(f"  ‚úÖ Model type: Univariate (no exogenous variables)")

# Cell 9: Train PatchTST Model
print("=== Training PatchTST Model ===")
nf = NeuralForecast(models=[patchtst], freq='W')

print("Starting training...")
print("This may take several minutes depending on data size and hardware.")

# Train the model
nf.fit(
    df=train_final,
    val_size=0  # Using external validation
)
print("‚úÖ Training completed!")

# Cell 10: Generate Validation Predictions (All IDs)
print("=== Generating Validation Predictions ===")

# Generate predictions for ALL validation data
val_predictions = nf.predict(df=train_final)
val_predictions = shift_ds_by_days(val_predictions, 5)

print(f"Predictions generated!")
print(f"Predictions shape: {val_predictions.shape}")
print(f"Validation data shape: {val_final.shape}")

# Check if we have predictions for all validation IDs
val_pred_ids = set(val_predictions['unique_id'])
val_target_ids = set(val_final['unique_id'])
missing_pred_ids = val_target_ids - val_pred_ids

if missing_pred_ids:
    print(f"‚ö†Ô∏è  Missing predictions for {len(missing_pred_ids)} IDs: {list(missing_pred_ids)[:5]}...")
    print("These will be handled with store-level fallbacks")
else:
    print("‚úÖ Predictions generated for all validation IDs")

# Now you can merge against val_final on the aligned 'ds':
val_results = (
    val_predictions[['unique_id','ds','PatchTST']]
      .merge(
          val_final[['unique_id','ds','y']],
          on=['unique_id','ds'],
          how='inner'
      )
)
print("After shift, merged shape:", val_results.shape)
val_results.head(), val_results.shape, y_val.shape

# Cell 12: Evaluate Model Performance
print("=== Model Performance Evaluation ===")

# 1) Build a tiny DF of your true validation targets:
val_true = val_final[['unique_id', 'ds']].copy()
val_true['y'] = y_val  # y_val from before dropping Weekly_Sales

# 2) Merge your (shifted) predictions with the true targets:
val_results = (
    val_predictions[['unique_id', 'ds', 'PatchTST']]
      .merge(val_true, on=['unique_id', 'ds'], how='inner')
)

print(f"After shift & merge, evaluation data shape: {val_results.shape}")

if not val_results.empty:
    # 3) Compute MAE and RMSE
    mae  = mean_absolute_error(val_results['y'], val_results['PatchTST'])
    rmse = np.sqrt(mean_squared_error(val_results['y'], val_results['PatchTST']))

    print(f"\nüéØ === Validation Performance ===")
    print(f"MAE:  {mae:,.2f}")
    print(f"RMSE: {rmse:,.2f}")

    # 5) Quick stats on the distributions
    print(f"\nüìä === Prediction Statistics ===")
    print(f"Actual   sales ‚Äî mean: {val_results['y'].mean():,.2f}, std: {val_results['y'].std():,.2f}")
    print(f"Predicted sales ‚Äî mean: {val_results['PatchTST'].mean():,.2f}, std: {val_results['PatchTST'].std():,.2f}")

    # 6) (Optional) If you still need to measure fallback-only performance:
    if 'missing_pred_ids' in globals() and missing_pred_ids:
        fb = val_results[val_results['unique_id'].isin(missing_pred_ids)]
        if not fb.empty:
            fb_mae = mean_absolute_error(fb['y'], fb['PatchTST'])
            print(f"\nüîÑ Fallback-only MAE ({len(fb)} series): {fb_mae:,.2f}")

"""# TRAINING

# helper functions
"""

import os
import matplotlib.pyplot as plt
from sklearn.metrics import mean_absolute_error, mean_squared_error

def log_patchtst_params(model):
    cfg = {}
    for attr in [
        "h","input_size","patch_len","stride",
        "encoder_layers","n_heads","hidden_size",
        "learning_rate","max_steps","val_check_steps",
        "early_stop_patience_steps","batch_size",
        "dropout","fc_dropout","head_dropout",
        "scaler_type","revin","enable_progress_bar","random_seed"
    ]:
        val = getattr(model, attr, None)
        if val is not None:
            cfg[attr] = val

    loss = getattr(model, "loss", None)
    if loss is not None:
        cfg["loss"] = loss.__class__.__name__

    # **DEBUG LINE**‚Äîthis should appear in your notebook output
    print("‚ñ∂ Logging PatchTST params to W&B:", cfg)

    wandb.config.update(cfg, allow_val_change=True)



def wmae(y_true, y_pred, w):
    """Weighted MAE (holidays count 5√ó) with NaN handling."""
    mask = np.isfinite(y_true) & np.isfinite(y_pred) & np.isfinite(w)
    if not mask.any():
        print("‚ö†Ô∏è All values are NaN/infinite in WMAE calculation")
        return np.nan
    y_true_clean = y_true[mask]
    y_pred_clean = y_pred[mask]
    w_clean = w[mask]
    if w_clean.sum() == 0:
        print("‚ö†Ô∏è Sum of weights is zero in WMAE calculation")
        return np.nan
    return (w_clean * np.abs(y_true_clean - y_pred_clean)).sum() / w_clean.sum()

def evaluate_and_log_patchtst(
    nf,
    train_df: pd.DataFrame,
    val_df: pd.DataFrame,
    y_val: pd.Series,
    w_val: np.ndarray,
    shift_days: int = 5,
    split: str = "val"
):
    """
    1) Forecast with nf.predict on train_df
    2) Shift the ds in predictions by shift_days
    3) Align forecasts to val_df by unique_id and position
    4) Compute MAE, RMSE, WMAE
    5) Log metrics + diagnostic plots to W&B
    """
    # 1) Forecast
    preds_all = nf.predict(df=train_df)

    # 2) Shift the 'ds' column
    preds_all['ds'] = preds_all['ds'] + pd.Timedelta(days=shift_days)

    # 3) Align by unique_id + position
    records = []
    for uid in preds_all['unique_id'].unique():
        p = preds_all[preds_all['unique_id'] == uid].sort_values('ds').reset_index(drop=True)
        v = val_df[val_df['unique_id'] == uid].sort_values('ds').reset_index(drop=True)
        if p.empty or v.empty:
            continue
        n = min(len(p), len(v))

        # mask over val_df for this uid
        mask = (val_df['unique_id'] == uid).values
        y_arr = y_val.values[mask][:n]
        w_arr = w_val[mask][:n]
        ds_arr = v['ds'].iloc[:n].values
        y_pred_arr = p['PatchTST'].iloc[:n].values

        records.append(pd.DataFrame({
            'unique_id': uid,
            'ds':         ds_arr,
            'y_true':     y_arr,
            'y_pred':     y_pred_arr,
            'w':          w_arr
        }))

    if not records:
        raise ValueError("No overlapping series for validation after shift.")
    results = pd.concat(records, ignore_index=True)

    # 4) Metrics
    mae_val  = mean_absolute_error(results['y_true'], results['y_pred'])
    rmse_val = np.sqrt(mean_squared_error(results['y_true'], results['y_pred']))
    wmae_val = wmae(results['y_true'], results['y_pred'], results['w'])

    # log metrics
    wandb.log({
        f"{split}/MAE":  mae_val,
        f"{split}/RMSE": rmse_val,
        f"{split}/WMAE": wmae_val
    })
    print(f"[{split.upper()}] MAE={mae_val:.4f}  RMSE={rmse_val:.4f}  WMAE={wmae_val:.4f}")

    mask_finite = np.isfinite(results['y_true']) & np.isfinite(results['y_pred'])

    # 5a) Actual vs Predicted
    fig, ax = plt.subplots(figsize=(6,6))
    ax.scatter(results.loc[mask_finite,'y_true'],
               results.loc[mask_finite,'y_pred'],
               alpha=0.3, s=10)
    mn, mx = results.loc[mask_finite, ['y_true','y_pred']].min().min(), \
             results.loc[mask_finite, ['y_true','y_pred']].max().max()
    ax.plot([mn,mx],[mn,mx],'r--', linewidth=1)
    ax.set(xlabel="Actual", ylabel="Predicted", title=f"{split.capitalize()} Actual vs Pred")
    wandb.log({f"{split}/actual_vs_pred": wandb.Image(fig)})
    plt.close(fig)

    # 5b) Residuals vs Predicted
    fig, ax = plt.subplots(figsize=(6,4))
    residuals = results['y_true'] - results['y_pred']
    ax.scatter(results.loc[mask_finite,'y_pred'], residuals[mask_finite],
               alpha=0.3, s=10)
    ax.hlines(0,
              results.loc[mask_finite,'y_pred'].min(),
              results.loc[mask_finite,'y_pred'].max(),
              colors='r', linestyles='--')
    ax.set(xlabel="Predicted", ylabel="Residuals", title=f"{split.capitalize()} Residuals")
    wandb.log({f"{split}/residuals": wandb.Image(fig)})
    plt.close(fig)

    # 5c) WMAE Error Distribution
    fig, ax = plt.subplots(figsize=(6,4))
    errors = results['w'] * np.abs(results['y_true'] - results['y_pred'])
    ax.hist(errors, bins=50, density=True, alpha=0.7)
    ax.set(xlabel="Weighted Abs Error", ylabel="Density", title=f"{split.capitalize()} WMAE Dist")
    wandb.log({f"{split}/wmae_error_dist": wandb.Image(fig)})
    plt.close(fig)

    return {"MAE": mae_val, "RMSE": rmse_val, "WMAE": wmae_val}


forecast_horizon = len(val_final['ds'].unique())
input_size = min(forecast_horizon * 2, 104)  # Cap at ~2 years of weekly data

print(f"Forecast horizon: {forecast_horizon} weeks")
print(f"Input size: {input_size} weeks")
print(f"Input/Output ratio: {input_size/forecast_horizon:.1f}x")

# Verify we have enough history
min_history_per_id = train_final.groupby('unique_id').size().min()
print(f"Minimum history per unique_id: {min_history_per_id} weeks")
if min_history_per_id < input_size:
    print(f"‚ö†Ô∏è  Some series have less history ({min_history_per_id}) than input_size ({input_size})")
    print("Model will handle this with padding")

"""# training 1 - tune input_size 1"""

import wandb
from neuralforecast import NeuralForecast
from neuralforecast.models import PatchTST
from neuralforecast.losses.pytorch import MAE

# sweep over these input sizes
input_sizes = [8, 32, 52, 74, 104]
val_results_per_size = {}

for inp_size in input_sizes:
    run_name = f"Trianing_PatchTST_input{inp_size}"
    wandb.init(
        project="ML_Final",
        name=run_name,
        reinit=True,
        config={"input_size": inp_size, "horizon": forecast_horizon}
    )

    # 1) Instantiate PatchTST with this input size
    patchtst = PatchTST(
        h=forecast_horizon,
        input_size=inp_size,

        # architecture (keep these fixed)
        patch_len=16,
        stride=8,
        encoder_layers=3,
        n_heads=8,
        hidden_size=128,

        # training
        loss=MAE(),
        learning_rate=1e-3,
        max_steps=1000,
        val_check_steps=100,
        early_stop_patience_steps=0,
        batch_size=32,

        # regularization
        dropout=0.1,
        fc_dropout=0.1,
        head_dropout=0.1,

        # scaling / norm
        scaler_type='robust',
        revin=True,

        # misc
        enable_progress_bar=True,
        random_seed=42
    )

    # 2) Log its hyper‚Äêparameters to W&B
    log_patchtst_params(patchtst)

    # 3) Train
    nf = NeuralForecast(models=[patchtst], freq='W')
    nf.fit(df=train_final, val_size=0)

    # 4) Evaluate (includes a 5-day shift internally)
    metrics = evaluate_and_log_patchtst(
        nf=nf,
        train_df=train_final,
        val_df=val_final,
        y_val=y_val,
        w_val=w_val,
        shift_days=5,
        split="val"
    )
    val_results_per_size[inp_size] = metrics["WMAE"]

    wandb.finish()

print("WMAE by input_size:", val_results_per_size)

input_sizes = list(val_results_per_size.keys())
wmae_values = list(val_results_per_size.values())

plt.figure(figsize=(8, 6))
plt.bar([str(s) for s in input_sizes], wmae_values)
plt.xlabel("Input Size")
plt.ylabel("WMAE")
plt.title("WMAE vs. Input Size for DLinear Model")
plt.show()

"""# training 2 - tune input_size 2"""

import wandb
from neuralforecast import NeuralForecast
from neuralforecast.models import PatchTST
from neuralforecast.losses.pytorch import MAE

# sweep over these input sizes
input_sizes = [38, 46, 58]
val_results_per_size = {}

for inp_size in input_sizes:
    run_name = f"Trianing_PatchTST_input{inp_size}"
    wandb.init(
        project="ML_Final",
        name=run_name,
        reinit=True,
        config={"input_size": inp_size, "horizon": forecast_horizon}
    )

    # 1) Instantiate PatchTST with this input size
    patchtst = PatchTST(
        h=forecast_horizon,
        input_size=inp_size,

        # architecture (keep these fixed)
        patch_len=16,
        stride=8,
        encoder_layers=3,
        n_heads=8,
        hidden_size=128,

        # training
        loss=MAE(),
        learning_rate=1e-3,
        max_steps=1000,
        val_check_steps=100,
        early_stop_patience_steps=0,
        batch_size=32,

        # regularization
        dropout=0.1,
        fc_dropout=0.1,
        head_dropout=0.1,

        # scaling / norm
        scaler_type='robust',
        revin=True,

        # misc
        enable_progress_bar=True,
        random_seed=42
    )

    # 2) Log its hyper‚Äêparameters to W&B
    log_patchtst_params(patchtst)

    # 3) Train
    nf = NeuralForecast(models=[patchtst], freq='W')
    nf.fit(df=train_final, val_size=0)

    # 4) Evaluate (includes a 5-day shift internally)
    metrics = evaluate_and_log_patchtst(
        nf=nf,
        train_df=train_final,
        val_df=val_final,
        y_val=y_val,
        w_val=w_val,
        shift_days=5,
        split="val"
    )
    val_results_per_size[inp_size] = metrics["WMAE"]

    wandb.finish()

print("WMAE by input_size:", val_results_per_size)

input_sizes = list(val_results_per_size.keys())
wmae_values = list(val_results_per_size.values())

plt.figure(figsize=(8, 6))
plt.bar([str(s) for s in input_sizes], wmae_values)
plt.xlabel("Input Size")
plt.ylabel("WMAE")
plt.title("WMAE vs. Input Size for PatchTST Model")
plt.show()

"""# training 3 - tune input_size 3"""

# the values you‚Äôre sweeping over
input_sizes = [56, 60, 64]
val_results_per_size = {}

# fixed hyper-parameters
HORIZON = forecast_horizon
PATCH_LEN = 16
STRIDE = 8
ENCODER_LAYERS = 3
N_HEADS = 8
HIDDEN_SIZE = 128

LR = 1e-3
MAX_STEPS = 1000
VAL_CHECK_STEPS = 100
EARLY_STOP = 0
BATCH_SIZE = 32

DROPOUT = 0.1
FC_DROPOUT = 0.1
HEAD_DROPOUT = 0.1

SCALER = 'robust'
REVIN = True

PROGRESS = True
SEED = 42

LOSS_NAME = "MAE"

for inp_size in input_sizes:
    run_name = f"PatchTST_input{inp_size}"
    # 1) start W&B with the full, hardcoded config
    wandb.init(
        project="ML_Final",
        name=run_name,
        reinit=True,
        config={
            "horizon": HORIZON,
            "input_size": inp_size,
            "patch_len": PATCH_LEN,
            "stride": STRIDE,
            "encoder_layers": ENCODER_LAYERS,
            "n_heads": N_HEADS,
            "hidden_size": HIDDEN_SIZE,
            "learning_rate": LR,
            "max_steps": MAX_STEPS,
            "val_check_steps": VAL_CHECK_STEPS,
            "early_stop_patience_steps": EARLY_STOP,
            "batch_size": BATCH_SIZE,
            "dropout": DROPOUT,
            "fc_dropout": FC_DROPOUT,
            "head_dropout": HEAD_DROPOUT,
            "scaler_type": SCALER,
            "revin": REVIN,
            "enable_progress_bar": PROGRESS,
            "random_seed": SEED,
            "loss": LOSS_NAME,
        }
    )

    # 2) instantiate your model exactly with those same numbers
    patchtst = PatchTST(
        h=HORIZON,
        input_size=inp_size,
        patch_len=PATCH_LEN,
        stride=STRIDE,
        encoder_layers=ENCODER_LAYERS,
        n_heads=N_HEADS,
        hidden_size=HIDDEN_SIZE,

        loss=MAE(),
        learning_rate=LR,
        max_steps=MAX_STEPS,
        val_check_steps=VAL_CHECK_STEPS,
        early_stop_patience_steps=EARLY_STOP,
        batch_size=BATCH_SIZE,

        dropout=DROPOUT,
        fc_dropout=FC_DROPOUT,
        head_dropout=HEAD_DROPOUT,

        scaler_type=SCALER,
        revin=REVIN,

        enable_progress_bar=PROGRESS,
        random_seed=SEED
    )

    # 3) train
    nf = NeuralForecast(models=[patchtst], freq='W')
    nf.fit(df=train_final, val_size=0)

    # 4) evaluate (+ 5-day shift internally) and log MAE/RMSE/WMAE & plots
    metrics = evaluate_and_log_patchtst(
        nf=nf,
        train_df=train_final,
        val_df=val_final,
        y_val=y_val,
        w_val=w_val,
        shift_days=5,
        split="val"
    )
    val_results_per_size[inp_size] = metrics["WMAE"]

    wandb.finish()

# 5) Finally, plot your sweep results
plt.figure(figsize=(8, 6))
plt.bar([str(s) for s in val_results_per_size], val_results_per_size.values())
plt.xlabel("Input Size")
plt.ylabel("WMAE")
plt.title("WMAE vs. Input Size for PatchTST")
plt.show()

"""# training 4 - patch_len, stride"""

import wandb
import matplotlib.pyplot as plt
from neuralforecast import NeuralForecast
from neuralforecast.models import PatchTST
from neuralforecast.losses.pytorch import MAE

# 1) Define the sweep grid of (patch_len, stride) pairs.
#    Note: stride must be < patch_len. A common rule: stride = patch_len // 2.
patch_lens = [8, 16, 32]
strides    = [4, 8, 16]

combos = [(pl, st) for pl in patch_lens for st in strides if st < pl]

# 2) Fixed hyperparams
HORIZON     = forecast_horizon
INPUT_SIZE  = 58 # best one
ENCODER_LAYERS = 3
N_HEADS     = 8
HIDDEN_SIZE = 128

LR              = 1e-3
MAX_STEPS       = 1000
VAL_CHECK       = 100
EARLY_STOP      = 0
BATCH_SIZE      = 32

DROPOUT     = 0.1
FC_DROPOUT  = 0.1
HEAD_DROPOUT= 0.1

SCALER_TYPE = 'robust'
REVIN       = True

PROGRESS    = True
SEED        = 42
LOSS_NAME   = "MAE"

results = {}

for (patch_len, stride) in combos:
    run_name = f"PatchTST_pl{patch_len}_st{stride}"
    # 3) Start new W&B run with ALL config hard‚Äêcoded
    wandb.init(
        project="ML_Final",
        name=run_name,
        reinit=True,
        config={
            "horizon": HORIZON,
            "input_size": INPUT_SIZE,
            "patch_len": patch_len,
            "stride": stride,
            "encoder_layers": ENCODER_LAYERS,
            "n_heads": N_HEADS,
            "hidden_size": HIDDEN_SIZE,
            "learning_rate": LR,
            "max_steps": MAX_STEPS,
            "val_check_steps": VAL_CHECK,
            "early_stop_patience_steps": EARLY_STOP,
            "batch_size": BATCH_SIZE,
            "dropout": DROPOUT,
            "fc_dropout": FC_DROPOUT,
            "head_dropout": HEAD_DROPOUT,
            "scaler_type": SCALER_TYPE,
            "revin": REVIN,
            "enable_progress_bar": PROGRESS,
            "random_seed": SEED,
            "loss": LOSS_NAME,
        }
    )

    # 4) Instantiate the model with this (patch_len, stride)
    model = PatchTST(
        h=HORIZON,
        input_size=INPUT_SIZE,
        patch_len=patch_len,
        stride=stride,
        encoder_layers=ENCODER_LAYERS,
        n_heads=N_HEADS,
        hidden_size=HIDDEN_SIZE,

        loss=MAE(),
        learning_rate=LR,
        max_steps=MAX_STEPS,
        val_check_steps=VAL_CHECK,
        early_stop_patience_steps=EARLY_STOP,
        batch_size=BATCH_SIZE,

        dropout=DROPOUT,
        fc_dropout=FC_DROPOUT,
        head_dropout=HEAD_DROPOUT,

        scaler_type=SCALER_TYPE,
        revin=REVIN,

        enable_progress_bar=PROGRESS,
        random_seed=SEED
    )

    # 5) Train
    nf = NeuralForecast(models=[model], freq='W')
    nf.fit(df=train_final, val_size=0)

    # 6) Evaluate (+ 5-day shift internally) and log
    metrics = evaluate_and_log_patchtst(
        nf=nf,
        train_df=train_final,
        val_df=val_final,
        y_val=y_val,
        w_val=w_val,
        shift_days=5,
        split="val"
    )
    results[(patch_len, stride)] = metrics["WMAE"]

    wandb.finish()

# 7) Plot the WMAE for each (patch_len, stride)
labels = [f"{pl}/{st}" for (pl,st) in results]
values = [results[key]    for key in results]

plt.figure(figsize=(10,6))
plt.bar(labels, values)
plt.xlabel("patch_len / stride")
plt.ylabel("WMAE")
plt.title("PatchTST: WMAE vs. patch_len & stride")
plt.show()

"""# training 5 - patch_len, stride"""

import wandb
import matplotlib.pyplot as plt
from neuralforecast import NeuralForecast
from neuralforecast.models import PatchTST
from neuralforecast.losses.pytorch import MAE

# 1) Define the sweep grid of (patch_len, stride) pairs.
#    Note: stride must be < patch_len. A common rule: stride = patch_len // 2.
combos = [(24, 12), (32, 20), (40, 20), (48, 24)]

# 2) Fixed hyperparams
HORIZON     = forecast_horizon
INPUT_SIZE  = 58 # best one
ENCODER_LAYERS = 3
N_HEADS     = 8
HIDDEN_SIZE = 128

LR              = 1e-3
MAX_STEPS       = 1000
VAL_CHECK       = 100
EARLY_STOP      = 0
BATCH_SIZE      = 32

DROPOUT     = 0.1
FC_DROPOUT  = 0.1
HEAD_DROPOUT= 0.1

SCALER_TYPE = 'robust'
REVIN       = True

PROGRESS    = True
SEED        = 42
LOSS_NAME   = "MAE"

results = {}

for (patch_len, stride) in combos:
    run_name = f"PatchTST_pl{patch_len}_st{stride}"
    # 3) Start new W&B run with ALL config hard‚Äêcoded
    wandb.init(
        project="ML_Final",
        name=run_name,
        reinit=True,
        config={
            "horizon": HORIZON,
            "input_size": INPUT_SIZE,
            "patch_len": patch_len,
            "stride": stride,
            "encoder_layers": ENCODER_LAYERS,
            "n_heads": N_HEADS,
            "hidden_size": HIDDEN_SIZE,
            "learning_rate": LR,
            "max_steps": MAX_STEPS,
            "val_check_steps": VAL_CHECK,
            "early_stop_patience_steps": EARLY_STOP,
            "batch_size": BATCH_SIZE,
            "dropout": DROPOUT,
            "fc_dropout": FC_DROPOUT,
            "head_dropout": HEAD_DROPOUT,
            "scaler_type": SCALER_TYPE,
            "revin": REVIN,
            "enable_progress_bar": PROGRESS,
            "random_seed": SEED,
            "loss": LOSS_NAME,
        }
    )

    # 4) Instantiate the model with this (patch_len, stride)
    model = PatchTST(
        h=HORIZON,
        input_size=INPUT_SIZE,
        patch_len=patch_len,
        stride=stride,
        encoder_layers=ENCODER_LAYERS,
        n_heads=N_HEADS,
        hidden_size=HIDDEN_SIZE,

        loss=MAE(),
        learning_rate=LR,
        max_steps=MAX_STEPS,
        val_check_steps=VAL_CHECK,
        early_stop_patience_steps=EARLY_STOP,
        batch_size=BATCH_SIZE,

        dropout=DROPOUT,
        fc_dropout=FC_DROPOUT,
        head_dropout=HEAD_DROPOUT,

        scaler_type=SCALER_TYPE,
        revin=REVIN,

        enable_progress_bar=PROGRESS,
        random_seed=SEED
    )

    # 5) Train
    nf = NeuralForecast(models=[model], freq='W')
    nf.fit(df=train_final, val_size=0)

    # 6) Evaluate (+ 5-day shift internally) and log
    metrics = evaluate_and_log_patchtst(
        nf=nf,
        train_df=train_final,
        val_df=val_final,
        y_val=y_val,
        w_val=w_val,
        shift_days=5,
        split="val"
    )
    results[(patch_len, stride)] = metrics["WMAE"]

    wandb.finish()

# 7) Plot the WMAE for each (patch_len, stride)
labels = [f"{pl}/{st}" for (pl,st) in results]
values = [results[key]    for key in results]

plt.figure(figsize=(10,6))
plt.bar(labels, values)
plt.xlabel("patch_len / stride")
plt.ylabel("WMAE")
plt.title("PatchTST: WMAE vs. patch_len & stride")
plt.show()

"""# training 6 - tune encoder layers"""

import wandb
import matplotlib.pyplot as plt
from neuralforecast import NeuralForecast
from neuralforecast.models import PatchTST
from neuralforecast.losses.pytorch import MAE

# 1) Sweep over different numbers of encoder layers
encoder_layer_values = [1, 2, 4, 5]
results = {}

# 2) Fixed hyper-parameters (best so far)
HORIZON      = forecast_horizon
INPUT_SIZE   = 58
PATCH_LEN    = 40
STRIDE       = 20
N_HEADS      = 8
HIDDEN_SIZE  = 128

LR              = 1e-3
MAX_STEPS       = 1000
VAL_CHECK       = 100
EARLY_STOP      = 0
BATCH_SIZE      = 32

DROPOUT     = 0.1
FC_DROPOUT  = 0.1
HEAD_DROPOUT= 0.1

SCALER_TYPE = 'robust'
REVIN       = True

PROGRESS    = True
SEED        = 42
LOSS_NAME   = "MAE"

for n_layers in encoder_layer_values:
    run_name = f"PatchTST_enc{n_layers}"
    wandb.init(
        project="ML_Final",
        name=run_name,
        reinit=True,
        config={
            "horizon": HORIZON,
            "input_size": INPUT_SIZE,
            "patch_len": PATCH_LEN,
            "stride": STRIDE,
            "encoder_layers": n_layers,
            "n_heads": N_HEADS,
            "hidden_size": HIDDEN_SIZE,
            "learning_rate": LR,
            "max_steps": MAX_STEPS,
            "val_check_steps": VAL_CHECK,
            "early_stop_patience_steps": EARLY_STOP,
            "batch_size": BATCH_SIZE,
            "dropout": DROPOUT,
            "fc_dropout": FC_DROPOUT,
            "head_dropout": HEAD_DROPOUT,
            "scaler_type": SCALER_TYPE,
            "revin": REVIN,
            "enable_progress_bar": PROGRESS,
            "random_seed": SEED,
            "loss": LOSS_NAME,
        }
    )

    # 3) Instantiate the model with this number of encoder layers
    model = PatchTST(
        h=HORIZON,
        input_size=INPUT_SIZE,
        patch_len=PATCH_LEN,
        stride=STRIDE,
        encoder_layers=n_layers,
        n_heads=N_HEADS,
        hidden_size=HIDDEN_SIZE,

        loss=MAE(),
        learning_rate=LR,
        max_steps=MAX_STEPS,
        val_check_steps=VAL_CHECK,
        early_stop_patience_steps=EARLY_STOP,
        batch_size=BATCH_SIZE,

        dropout=DROPOUT,
        fc_dropout=FC_DROPOUT,
        head_dropout=HEAD_DROPOUT,

        scaler_type=SCALER_TYPE,
        revin=REVIN,

        enable_progress_bar=PROGRESS,
        random_seed=SEED
    )

    # 4) Train
    nf = NeuralForecast(models=[model], freq='W')
    nf.fit(df=train_final, val_size=0)

    # 5) Evaluate (+5-day shift internally)
    metrics = evaluate_and_log_patchtst(
        nf=nf,
        train_df=train_final,
        val_df=val_final,
        y_val=y_val,
        w_val=w_val,
        shift_days=5,
        split="val"
    )
    results[n_layers] = metrics["WMAE"]

    wandb.finish()

# 6) Plot WMAE vs. # encoder layers
plt.figure(figsize=(8,6))
plt.bar([str(n) for n in results], list(results.values()))
plt.xlabel("Number of Encoder Layers")
plt.ylabel("WMAE")
plt.title("PatchTST: WMAE vs. Encoder Depth")
plt.show()

"""# training 7 - tune n_heads"""

import wandb
import matplotlib.pyplot as plt
from neuralforecast import NeuralForecast
from neuralforecast.models import PatchTST
from neuralforecast.losses.pytorch import MAE

# 1) Sweep over different numbers of attention heads
head_values = [2, 4 , 16]
results = {}

# 2) Fixed hyper-parameters (best so far)
HORIZON       = forecast_horizon
INPUT_SIZE    = 58
PATCH_LEN     = 40
STRIDE        = 20
ENCODER_LAYERS= 1      # you found this to be best
HIDDEN_SIZE   = 128

LR            = 1e-3
MAX_STEPS     = 1000
VAL_CHECK     = 100
EARLY_STOP    = 0
BATCH_SIZE    = 32

DROPOUT       = 0.1
FC_DROPOUT    = 0.1
HEAD_DROPOUT  = 0.1

SCALER_TYPE   = 'robust'
REVIN         = True

PROGRESS      = True
SEED          = 42
LOSS_NAME     = "MAE"

for n_heads in head_values:
    run_name = f"PatchTST_heads{n_heads}"
    wandb.init(
        project="ML_Final",
        name=run_name,
        reinit=True,
        config={
            "horizon": HORIZON,
            "input_size": INPUT_SIZE,
            "patch_len": PATCH_LEN,
            "stride": STRIDE,
            "encoder_layers": ENCODER_LAYERS,
            "n_heads": n_heads,
            "hidden_size": HIDDEN_SIZE,
            "learning_rate": LR,
            "max_steps": MAX_STEPS,
            "val_check_steps": VAL_CHECK,
            "early_stop_patience_steps": EARLY_STOP,
            "batch_size": BATCH_SIZE,
            "dropout": DROPOUT,
            "fc_dropout": FC_DROPOUT,
            "head_dropout": HEAD_DROPOUT,
            "scaler_type": SCALER_TYPE,
            "revin": REVIN,
            "enable_progress_bar": PROGRESS,
            "random_seed": SEED,
            "loss": LOSS_NAME,
        }
    )

    # 3) Instantiate with this many heads
    model = PatchTST(
        h=HORIZON,
        input_size=INPUT_SIZE,
        patch_len=PATCH_LEN,
        stride=STRIDE,
        encoder_layers=ENCODER_LAYERS,
        n_heads=n_heads,
        hidden_size=HIDDEN_SIZE,

        loss=MAE(),
        learning_rate=LR,
        max_steps=MAX_STEPS,
        val_check_steps=VAL_CHECK,
        early_stop_patience_steps=EARLY_STOP,
        batch_size=BATCH_SIZE,

        dropout=DROPOUT,
        fc_dropout=FC_DROPOUT,
        head_dropout=HEAD_DROPOUT,

        scaler_type=SCALER_TYPE,
        revin=REVIN,

        enable_progress_bar=PROGRESS,
        random_seed=SEED
    )

    # 4) Train
    nf = NeuralForecast(models=[model], freq='W')
    nf.fit(df=train_final, val_size=0)

    # 5) Evaluate & log
    metrics = evaluate_and_log_patchtst(
        nf=nf,
        train_df=train_final,
        val_df=val_final,
        y_val=y_val,
        w_val=w_val,
        shift_days=5,
        split="val"
    )
    results[n_heads] = metrics["WMAE"]

    wandb.finish()

# 6) Plot WMAE vs number of heads
plt.figure(figsize=(8,6))
plt.bar([str(h) for h in results], list(results.values()))
plt.xlabel("Number of Attention Heads")
plt.ylabel("WMAE")
plt.title("PatchTST: WMAE vs. # of Attention Heads")
plt.show()

"""# training 8 - hidden size"""

import wandb
import matplotlib.pyplot as plt
from neuralforecast import NeuralForecast
from neuralforecast.models import PatchTST
from neuralforecast.losses.pytorch import MAE

# 1) Sweep over different hidden_size values
hidden_sizes = [64, 128, 256, 512]
results = {}

# 2) Fixed hyper-parameters (best so far)
HORIZON        = forecast_horizon
INPUT_SIZE     = 58
PATCH_LEN      = 40
STRIDE         = 20
ENCODER_LAYERS = 1
N_HEADS        = 16  # best
LR             = 1e-3
MAX_STEPS      = 1000
VAL_CHECK      = 100
EARLY_STOP     = 0
BATCH_SIZE     = 32

DROPOUT      = 0.1
FC_DROPOUT   = 0.1
HEAD_DROPOUT = 0.1

SCALER_TYPE = 'robust'
REVIN       = True

PROGRESS  = True
SEED      = 42
LOSS_NAME = "MAE"

for hid in hidden_sizes:
    run_name = f"PatchTST_hidden{hid}"
    wandb.init(
        project="ML_Final",
        name=run_name,
        reinit=True,
        config={
            "horizon": HORIZON,
            "input_size": INPUT_SIZE,
            "patch_len": PATCH_LEN,
            "stride": STRIDE,
            "encoder_layers": ENCODER_LAYERS,
            "n_heads": N_HEADS,
            "hidden_size": hid,
            "learning_rate": LR,
            "max_steps": MAX_STEPS,
            "val_check_steps": VAL_CHECK,
            "early_stop_patience_steps": EARLY_STOP,
            "batch_size": BATCH_SIZE,
            "dropout": DROPOUT,
            "fc_dropout": FC_DROPOUT,
            "head_dropout": HEAD_DROPOUT,
            "scaler_type": SCALER_TYPE,
            "revin": REVIN,
            "enable_progress_bar": PROGRESS,
            "random_seed": SEED,
            "loss": LOSS_NAME,
        }
    )

    # 3) Instantiate the model with this hidden_size
    model = PatchTST(
        h=HORIZON,
        input_size=INPUT_SIZE,
        patch_len=PATCH_LEN,
        stride=STRIDE,
        encoder_layers=ENCODER_LAYERS,
        n_heads=N_HEADS,
        hidden_size=hid,

        loss=MAE(),
        learning_rate=LR,
        max_steps=MAX_STEPS,
        val_check_steps=VAL_CHECK,
        early_stop_patience_steps=EARLY_STOP,
        batch_size=BATCH_SIZE,

        dropout=DROPOUT,
        fc_dropout=FC_DROPOUT,
        head_dropout=HEAD_DROPOUT,

        scaler_type=SCALER_TYPE,
        revin=REVIN,

        enable_progress_bar=PROGRESS,
        random_seed=SEED
    )

    # 4) Train
    nf = NeuralForecast(models=[model], freq='W')
    nf.fit(df=train_final, val_size=0)

    # 5) Evaluate (+5-day shift internally)
    metrics = evaluate_and_log_patchtst(
        nf=nf,
        train_df=train_final,
        val_df=val_final,
        y_val=y_val,
        w_val=w_val,
        shift_days=5,
        split="val"
    )
    results[hid] = metrics["WMAE"]

    wandb.finish()

# 6) Plot WMAE vs. hidden_size
plt.figure(figsize=(8,6))
plt.bar([str(h) for h in results], list(results.values()))
plt.xlabel("Hidden Size")
plt.ylabel("WMAE")
plt.title("PatchTST: WMAE vs. Hidden Dimension Size")
plt.show()

"""# training 9 - tune dropout"""

import wandb
import matplotlib.pyplot as plt
from neuralforecast import NeuralForecast
from neuralforecast.models import PatchTST
from neuralforecast.losses.pytorch import MAE

# 1) Sweep over different dropout rates
dropout_rates = [0.0, 0.1, 0.2, 0.3, 0.5]
results = {}

# 2) Fixed hyper-parameters (best so far)
HORIZON        = forecast_horizon
INPUT_SIZE     = 58
PATCH_LEN      = 40
STRIDE         = 20
ENCODER_LAYERS = 1
N_HEADS        = 16
HIDDEN_SIZE    = 128

LR             = 1e-3
MAX_STEPS      = 1000
VAL_CHECK      = 100
EARLY_STOP     = 0
BATCH_SIZE     = 32

FC_DROPOUT     = 0.1  # keep these fixed
HEAD_DROPOUT   = 0.1

SCALER_TYPE    = 'robust'
REVIN          = True

PROGRESS       = True
SEED           = 42
LOSS_NAME      = "MAE"

for dr in dropout_rates:
    run_name = f"PatchTST_dropout{dr}"
    wandb.init(
        project="ML_Final",
        name=run_name,
        reinit=True,
        config={
            "horizon":                HORIZON,
            "input_size":             INPUT_SIZE,
            "patch_len":              PATCH_LEN,
            "stride":                 STRIDE,
            "encoder_layers":         ENCODER_LAYERS,
            "n_heads":                N_HEADS,
            "hidden_size":            HIDDEN_SIZE,
            "learning_rate":          LR,
            "max_steps":              MAX_STEPS,
            "val_check_steps":        VAL_CHECK,
            "early_stop_patience_steps": EARLY_STOP,
            "batch_size":             BATCH_SIZE,
            "dropout":                dr,
            "fc_dropout":             FC_DROPOUT,
            "head_dropout":           HEAD_DROPOUT,
            "scaler_type":            SCALER_TYPE,
            "revin":                  REVIN,
            "enable_progress_bar":    PROGRESS,
            "random_seed":            SEED,
            "loss":                   LOSS_NAME,
        }
    )

    # 3) Instantiate the model with this dropout rate
    model = PatchTST(
        h=HORIZON,
        input_size=INPUT_SIZE,
        patch_len=PATCH_LEN,
        stride=STRIDE,
        encoder_layers=ENCODER_LAYERS,
        n_heads=N_HEADS,
        hidden_size=HIDDEN_SIZE,

        loss=MAE(),
        learning_rate=LR,
        max_steps=MAX_STEPS,
        val_check_steps=VAL_CHECK,
        early_stop_patience_steps=EARLY_STOP,
        batch_size=BATCH_SIZE,

        dropout=dr,
        fc_dropout=FC_DROPOUT,
        head_dropout=HEAD_DROPOUT,

        scaler_type=SCALER_TYPE,
        revin=REVIN,

        enable_progress_bar=PROGRESS,
        random_seed=SEED
    )

    # 4) Train
    nf = NeuralForecast(models=[model], freq='W')
    nf.fit(df=train_final, val_size=0)

    # 5) Evaluate & log
    metrics = evaluate_and_log_patchtst(
        nf=nf,
        train_df=train_final,
        val_df=val_final,
        y_val=y_val,
        w_val=w_val,
        shift_days=5,
        split="val"
    )
    results[dr] = metrics["WMAE"]

    wandb.finish()

# 6) Plot WMAE vs. dropout rate
plt.figure(figsize=(8,6))
plt.bar([str(d) for d in results], list(results.values()))
plt.xlabel("Dropout Rate")
plt.ylabel("WMAE")
plt.title("PatchTST: WMAE vs. Dropout Rate")
plt.show()

"""# training 10 - fc and head dropouts"""

import wandb
import matplotlib.pyplot as plt
from neuralforecast import NeuralForecast
from neuralforecast.models import PatchTST
from neuralforecast.losses.pytorch import MAE
import numpy as np

# 1) Sweep grid for fc_dropout and head_dropout
fc_rates   = [0.0, 0.1, 0.2, 0.3]
head_rates = [0.0, 0.1, 0.2, 0.3]

results = {}

# 2) All other hyper-parameters fixed at your best values
HORIZON        = forecast_horizon
INPUT_SIZE     = 58
PATCH_LEN      = 40
STRIDE         = 20
ENCODER_LAYERS = 1
N_HEADS        = 16
HIDDEN_SIZE    = 128

LR          = 1e-3
MAX_STEPS   = 1000
VAL_CHECK   = 100
EARLY_STOP  = 0
BATCH_SIZE  = 32

DROPOUT     = 0.1    # main residual-dropout

SCALER_TYPE = 'robust'
REVIN       = True

PROGRESS    = True
SEED        = 42
LOSS_NAME   = "MAE"

for fc_dr in fc_rates:
    for head_dr in head_rates:
        run_name = f"PatchTST_fc{fc_dr}_head{head_dr}"
        # 3) Start W&B run
        wandb.init(
            project="ML_Final",
            name=run_name,
            reinit=True,
            config={
                "horizon":                  HORIZON,
                "input_size":               INPUT_SIZE,
                "patch_len":                PATCH_LEN,
                "stride":                   STRIDE,
                "encoder_layers":           ENCODER_LAYERS,
                "n_heads":                  N_HEADS,
                "hidden_size":              HIDDEN_SIZE,
                "learning_rate":            LR,
                "max_steps":                MAX_STEPS,
                "val_check_steps":          VAL_CHECK,
                "early_stop_patience_steps":EARLY_STOP,
                "batch_size":               BATCH_SIZE,
                "dropout":                  DROPOUT,
                "fc_dropout":               fc_dr,
                "head_dropout":             head_dr,
                "scaler_type":              SCALER_TYPE,
                "revin":                    REVIN,
                "enable_progress_bar":      PROGRESS,
                "random_seed":              SEED,
                "loss":                     LOSS_NAME,
            }
        )

        # 4) Instantiate model
        model = PatchTST(
            h=HORIZON,
            input_size=INPUT_SIZE,
            patch_len=PATCH_LEN,
            stride=STRIDE,
            encoder_layers=ENCODER_LAYERS,
            n_heads=N_HEADS,
            hidden_size=HIDDEN_SIZE,

            loss=MAE(),
            learning_rate=LR,
            max_steps=MAX_STEPS,
            val_check_steps=VAL_CHECK,
            early_stop_patience_steps=EARLY_STOP,
            batch_size=BATCH_SIZE,

            dropout=DROPOUT,
            fc_dropout=fc_dr,
            head_dropout=head_dr,

            scaler_type=SCALER_TYPE,
            revin=REVIN,

            enable_progress_bar=PROGRESS,
            random_seed=SEED
        )

        # 5) Train
        nf = NeuralForecast(models=[model], freq='W')
        nf.fit(df=train_final, val_size=0)

        # 6) Evaluate (+5-day shift internally)
        metrics = evaluate_and_log_patchtst(
            nf=nf,
            train_df=train_final,
            val_df=val_final,
            y_val=y_val,
            w_val=w_val,
            shift_days=5,
            split="val"
        )
        results[(fc_dr, head_dr)] = metrics["WMAE"]

        wandb.finish()

# 7) Visualize results as a heatmap
fc_list   = fc_rates
head_list = head_rates
heatmap   = np.zeros((len(fc_list), len(head_list)))

for i, fc_dr in enumerate(fc_list):
    for j, head_dr in enumerate(head_list):
        heatmap[i, j] = results[(fc_dr, head_dr)]

fig, ax = plt.subplots(figsize=(6,5))
im = ax.imshow(heatmap, origin='lower', aspect='auto')
ax.set_xticks(np.arange(len(head_list)))
ax.set_yticks(np.arange(len(fc_list)))
ax.set_xticklabels(head_list)
ax.set_yticklabels(fc_list)
ax.set_xlabel("head_dropout")
ax.set_ylabel("fc_dropout")
ax.set_title("WMAE Heatmap")

# annotate
for i in range(len(fc_list)):
    for j in range(len(head_list)):
        text = ax.text(j, i, f"{heatmap[i,j]:.2f}",
                       ha="center", va="center", color="w" if heatmap[i,j]>np.mean(heatmap) else "black")
plt.colorbar(im, ax=ax, label="WMAE")
plt.show()

"""# training 11 - tune rl, max_steps"""

import wandb
import matplotlib.pyplot as plt
from neuralforecast import NeuralForecast
from neuralforecast.models import PatchTST
from neuralforecast.losses.pytorch import MAE

# 1) Define coupled (learning_rate, max_steps) pairs
#    More steps ‚Üí lower lr; fewer steps ‚Üí higher lr
lr_steps = [
    (1e-2,  500),
    (5e-3, 1000),
    (1e-3, 2000),
    (5e-4, 4000),
]

results = {}

# 2) Fixed best parameters from previous sweeps
HORIZON        = forecast_horizon
INPUT_SIZE     = 58
PATCH_LEN      = 40
STRIDE         = 20
ENCODER_LAYERS = 1
N_HEADS        = 16
HIDDEN_SIZE    = 128

VAL_CHECK      = 100
EARLY_STOP     = 0
BATCH_SIZE     = 32

DROPOUT        = 0.1
FC_DROPOUT     = 0.2
HEAD_DROPOUT   = 0.1

SCALER_TYPE    = 'robust'
REVIN          = True

PROGRESS       = True
SEED           = 42
LOSS_NAME      = "MAE"

for lr, max_steps in lr_steps:
    run_name = f"PatchTST_lr{lr}_steps{max_steps}"
    wandb.init(
        project="ML_Final",
        name=run_name,
        reinit=True,
        config={
            "horizon":                  HORIZON,
            "input_size":               INPUT_SIZE,
            "patch_len":                PATCH_LEN,
            "stride":                   STRIDE,
            "encoder_layers":           ENCODER_LAYERS,
            "n_heads":                  N_HEADS,
            "hidden_size":              HIDDEN_SIZE,
            "learning_rate":            lr,
            "max_steps":                max_steps,
            "val_check_steps":          VAL_CHECK,
            "early_stop_patience_steps":EARLY_STOP,
            "batch_size":               BATCH_SIZE,
            "dropout":                  DROPOUT,
            "fc_dropout":               FC_DROPOUT,
            "head_dropout":             HEAD_DROPOUT,
            "scaler_type":              SCALER_TYPE,
            "revin":                    REVIN,
            "enable_progress_bar":      PROGRESS,
            "random_seed":              SEED,
            "loss":                     LOSS_NAME,
        }
    )

    # 3) Instantiate with this lr & max_steps
    model = PatchTST(
        h=HORIZON,
        input_size=INPUT_SIZE,
        patch_len=PATCH_LEN,
        stride=STRIDE,
        encoder_layers=ENCODER_LAYERS,
        n_heads=N_HEADS,
        hidden_size=HIDDEN_SIZE,

        loss=MAE(),
        learning_rate=lr,
        max_steps=max_steps,
        val_check_steps=VAL_CHECK,
        early_stop_patience_steps=EARLY_STOP,
        batch_size=BATCH_SIZE,

        dropout=DROPOUT,
        fc_dropout=FC_DROPOUT,
        head_dropout=HEAD_DROPOUT,

        scaler_type=SCALER_TYPE,
        revin=REVIN,

        enable_progress_bar=PROGRESS,
        random_seed=SEED
    )

    # 4) Train
    nf = NeuralForecast(models=[model], freq='W')
    nf.fit(df=train_final, val_size=0)

    # 5) Evaluate & log
    metrics = evaluate_and_log_patchtst(
        nf=nf,
        train_df=train_final,
        val_df=val_final,
        y_val=y_val,
        w_val=w_val,
        shift_days=5,
        split="val"
    )
    results[(lr, max_steps)] = metrics["WMAE"]

    wandb.finish()

# 6) Plot WMAE for each (lr, max_steps)
labels = [f"{lr}/{steps}" for lr, steps in results]
values = list(results.values())

plt.figure(figsize=(10,6))
plt.bar(labels, values)
plt.xlabel("learning_rate / max_steps")
plt.ylabel("WMAE")
plt.title("PatchTST: WMAE vs. (LR, #Steps)")
plt.show()

"""# trianing 12 - fine tune rl, max_steps"""

import wandb
import matplotlib.pyplot as plt
from neuralforecast import NeuralForecast
from neuralforecast.models import PatchTST
from neuralforecast.losses.pytorch import MAE

# 1) Refined (learning_rate, max_steps) candidates around your best (1e-3, 1000)
lr_steps = [
    (2e-3,  1000),  # faster convergence, same budget
    (1.5e-3,1000),
    (1e-3,  1000),  # baseline
    (8e-4,  1000),
    (5e-4,  2000),  # lower LR, more updates
    (5e-4,  1000),
    (2e-3,   500),  # higher LR, fewer updates
    (1e-3,  1500),  # baseline LR, more updates
]

results = {}

# 2) All other hyper-parameters fixed at your best values
HORIZON        = forecast_horizon
INPUT_SIZE     = 58
PATCH_LEN      = 40
STRIDE         = 20
ENCODER_LAYERS = 1
N_HEADS        = 16
HIDDEN_SIZE    = 128

VAL_CHECK      = 100
EARLY_STOP     = 0
BATCH_SIZE     = 32

DROPOUT        = 0.1
FC_DROPOUT     = 0.2
HEAD_DROPOUT   = 0.1

SCALER_TYPE    = 'robust'
REVIN          = True

PROGRESS       = True
SEED           = 42
LOSS_NAME      = "MAE"

for lr, max_steps in lr_steps:
    run_name = f"PatchTST_lr{lr}_steps{max_steps}"
    wandb.init(
        project="ML_Final",
        name=run_name,
        reinit=True,
        config={
            "horizon":                  HORIZON,
            "input_size":               INPUT_SIZE,
            "patch_len":                PATCH_LEN,
            "stride":                   STRIDE,
            "encoder_layers":           ENCODER_LAYERS,
            "n_heads":                  N_HEADS,
            "hidden_size":              HIDDEN_SIZE,
            "learning_rate":            lr,
            "max_steps":                max_steps,
            "val_check_steps":          VAL_CHECK,
            "early_stop_patience_steps":EARLY_STOP,
            "batch_size":               BATCH_SIZE,
            "dropout":                  DROPOUT,
            "fc_dropout":               FC_DROPOUT,
            "head_dropout":             HEAD_DROPOUT,
            "scaler_type":              SCALER_TYPE,
            "revin":                    REVIN,
            "enable_progress_bar":      PROGRESS,
            "random_seed":              SEED,
            "loss":                     LOSS_NAME,
        }
    )

    # 3) Instantiate with this lr & max_steps
    model = PatchTST(
        h=HORIZON,
        input_size=INPUT_SIZE,
        patch_len=PATCH_LEN,
        stride=STRIDE,
        encoder_layers=ENCODER_LAYERS,
        n_heads=N_HEADS,
        hidden_size=HIDDEN_SIZE,

        loss=MAE(),
        learning_rate=lr,
        max_steps=max_steps,
        val_check_steps=VAL_CHECK,
        early_stop_patience_steps=EARLY_STOP,
        batch_size=BATCH_SIZE,

        dropout=DROPOUT,
        fc_dropout=FC_DROPOUT,
        head_dropout=HEAD_DROPOUT,

        scaler_type=SCALER_TYPE,
        revin=REVIN,

        enable_progress_bar=PROGRESS,
        random_seed=SEED
    )

    # 4) Train
    nf = NeuralForecast(models=[model], freq='W')
    nf.fit(df=train_final, val_size=0)

    # 5) Evaluate & log
    metrics = evaluate_and_log_patchtst(
        nf=nf,
        train_df=train_final,
        val_df=val_final,
        y_val=y_val,
        w_val=w_val,
        shift_days=5,
        split="val"
    )
    results[(lr, max_steps)] = metrics["WMAE"]

    wandb.finish()

# 6) Plot WMAE for each (lr, max_steps)
labels = [f"{lr}/{steps}" for lr, steps in results]
values = list(results.values())

plt.figure(figsize=(10,6))
plt.bar(labels, values)
plt.xlabel("learning_rate / max_steps")
plt.ylabel("WMAE")
plt.title("PatchTST: WMAE vs. (LR, #Steps) ‚Äî Refined Sweep")
plt.show()

"""# train best model for test"""

train_final = prepare_df_for_nf(train, is_train=True)
test_final = prepare_df_for_nf(test, is_train=False)

print(f"\nDate ranges:")
print(f"Train: {train_final['ds'].min()} to {train_final['ds'].max()}")
print(f"Test: {test_final['ds'].min()} to {test_final['ds'].max()}")
train_final.shape, train_final.columns, test_final.shape, test_final.columns,

forecast_horizon = len(test_final['ds'].unique())
LR          = 8e-4
MAX_STEPS   = 20000
HORIZON        = forecast_horizon
INPUT_SIZE     = 62
PATCH_LEN      = 40
STRIDE         = 20
ENCODER_LAYERS = 1
N_HEADS        = 16
HIDDEN_SIZE    = 128

VAL_CHECK      = 100
EARLY_STOP     = 0
BATCH_SIZE     = 64

DROPOUT        = 0.1
FC_DROPOUT     = 0.1
HEAD_DROPOUT   = 0.1

SCALER_TYPE    = 'robust'
REVIN          = True

PROGRESS       = True
SEED           = 42
LOSS_NAME      = "MAE"

best_model = PatchTST(
    h=HORIZON,
    input_size=INPUT_SIZE,
    patch_len=PATCH_LEN,
    stride=STRIDE,
    encoder_layers=ENCODER_LAYERS,
    n_heads=N_HEADS,
    hidden_size=HIDDEN_SIZE,

    loss=MAE(),
    learning_rate=LR,
    max_steps=MAX_STEPS,
    val_check_steps=VAL_CHECK,
    early_stop_patience_steps=EARLY_STOP,
    batch_size=BATCH_SIZE,

    dropout=DROPOUT,
    fc_dropout=FC_DROPOUT,
    head_dropout=HEAD_DROPOUT,

    scaler_type=SCALER_TYPE,
    revin=REVIN,

    enable_progress_bar=PROGRESS,
    random_seed=SEED
)


nf = NeuralForecast(models=[best_model], freq='W')
nf.fit(df=train_final, val_size=0)

wandb.init(
    project="ML_Final",
    name="PatchTST_Final_Model_2",
    reinit=True,
    config={
        "horizon":                         forecast_horizon,
        "input_size":                      INPUT_SIZE,
        "patch_len":                       PATCH_LEN,
        "stride":                          STRIDE,
        "encoder_layers":                  ENCODER_LAYERS,
        "n_heads":                         N_HEADS,
        "hidden_size":                     HIDDEN_SIZE,
        "learning_rate":                   LR,
        "max_steps":                       MAX_STEPS,
        "val_check_steps":                 VAL_CHECK,
        "early_stop_patience_steps":       EARLY_STOP,
        "batch_size":                      BATCH_SIZE,
        "dropout":                         DROPOUT,
        "fc_dropout":                      FC_DROPOUT,
        "head_dropout":                    HEAD_DROPOUT,
        "scaler_type":                     SCALER_TYPE,
        "revin":                           REVIN,
        "enable_progress_bar":             PROGRESS,
        "random_seed":                     SEED,
        "loss":                            LOSS_NAME,
    }
)

wandb.config.update({"model_arch": str(best_model)}, allow_val_change=True)
wandb.finish()

final_preds = nf.predict(df=train_final)
final_preds = shift_ds_by_days(final_preds, 5)
final_preds.shape, final_preds.head(), test.shape, test.head()

# 1. Prepare the test key‚Äêframe
test_df = test_final[['unique_id', 'ds']].copy()

# 2. Merge your PatchTST predictions into the test frame
sub = (
    test_df
    .merge(final_preds[['unique_id', 'ds', 'PatchTST']],
           on=['unique_id','ds'], how='left')
    .rename(columns={'PatchTST': 'Weekly_Sales'})
)

# 3. Recover Store and Dept from unique_id
sub[['Dept','Store']] = sub['unique_id'].str.split('_', expand=True)
sub['Store'] = sub['Store'].astype(int)
sub['Dept']  = sub['Dept'].astype(int)

# 4. Compute store‚Äêlevel average Weekly_Sales from the original train
store_means = train.groupby('Store')['Weekly_Sales'].mean()

# 5. Fill missing model forecasts with the store‚Äêlevel mean
sub['Weekly_Sales'] = sub['Weekly_Sales'].fillna(sub['Store'].map(store_means))

# 6. Build the Kaggle "Id" column: e.g. "1_1_2012-11-02"
sub['Id'] = (
    sub['Store'].astype(str) + '_' +
    sub['Dept'].astype(str)  + '_' +
    sub['ds'].dt.strftime('%Y-%m-%d')
)

# 7. Final submission: select only Id + Weekly_Sales
submission = sub[['Id','Weekly_Sales']]

# 8. Save to CSV
submission.to_csv('PatchTST_submission.csv', index=False)
print(f"Saved PatchTST_submission.csv with {len(submission)} rows.")

# 9. Preview first few lines
submission.shape, submission.head()

