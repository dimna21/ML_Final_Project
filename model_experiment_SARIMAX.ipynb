{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "WWSCUpCpXtD4"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "features = pd.read_csv(\"/content/drive/MyDrive/ML_Final_Project/features.csv\")\n",
        "train = pd.read_csv(\"/content/drive/MyDrive/ML_Final_Project/train.csv\")\n",
        "stores = pd.read_csv(\"/content/drive/MyDrive/ML_Final_Project/stores.csv\")\n",
        "test = pd.read_csv(\"/content/drive/MyDrive/ML_Final_Project/test.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "\n",
        "class BaseMerger(BaseEstimator, TransformerMixin):\n",
        "\n",
        "    def __init__(self, features, stores):\n",
        "        self.feature_store = features.merge(stores, how='inner', on='Store')\n",
        "        self.feature_store['Date'] = pd.to_datetime(self.feature_store['Date'])\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        X = X.copy()\n",
        "        X['Date'] = pd.to_datetime(X['Date'])\n",
        "        merged = X.merge(self.feature_store, how='inner', on=['Store', 'Date', 'IsHoliday'])\n",
        "        merged = merged.sort_values(by=['Date', 'Store', 'Dept']).reset_index(drop=True)\n",
        "        return merged\n",
        "\n",
        "class FeatureAdder(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self):\n",
        "        self.superbowl = pd.to_datetime(['2010-02-12', '2011-02-11', '2012-02-10', '2013-02-08'])\n",
        "        self.labor_day = pd.to_datetime(['2010-09-10', '2011-09-09', '2012-09-07', '2013-09-06'])\n",
        "        self.thanksgiving = pd.to_datetime(['2010-11-26', '2011-11-25', '2012-11-23', '2013-11-29'])\n",
        "        self.christmas = pd.to_datetime(['2010-12-31', '2011-12-30', '2012-12-28', '2013-12-27'])\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        X = X.copy()\n",
        "\n",
        "        # Convert temperature to Celsius\n",
        "        if 'Temperature' in X.columns:\n",
        "            X['Temperature'] = (X['Temperature'] - 32) * (5.0 / 9.0)\n",
        "\n",
        "        # Basic date parts\n",
        "        X['Day'] = X['Date'].dt.day\n",
        "        X['Month'] = X['Date'].dt.month\n",
        "        X['Year'] = X['Date'].dt.year\n",
        "\n",
        "        # Extract ISO week and year for holiday matching\n",
        "        X['Week'] = X['Date'].dt.isocalendar().week\n",
        "        X['YearNum'] = X['Date'].dt.year\n",
        "\n",
        "        # Helper to flag if a date is in same ISO week/year as a known holiday\n",
        "        def is_holiday_week(date_series, holidays):\n",
        "            holiday_weeks = set((d.isocalendar().week, d.year) for d in holidays)\n",
        "            return date_series.apply(lambda d: (d.isocalendar().week, d.year) in holiday_weeks if pd.notnull(d) else False).astype(int)\n",
        "\n",
        "        X['SuperbowlWeek'] = is_holiday_week(X['Date'], self.superbowl)\n",
        "        X['LaborDayWeek'] = is_holiday_week(X['Date'], self.labor_day)\n",
        "        X['ThanksgivingWeek'] = is_holiday_week(X['Date'], self.thanksgiving)\n",
        "        X['ChristmasWeek'] = is_holiday_week(X['Date'], self.christmas)\n",
        "\n",
        "        # Calculate days to Thanksgiving and Christmas (using Nov 24 and Dec 24 as anchor dates)\n",
        "        thanksgiving_dates = pd.to_datetime(X['Year'].astype(str) + \"-11-24\")\n",
        "        christmas_dates = pd.to_datetime(X['Year'].astype(str) + \"-12-24\")\n",
        "\n",
        "        X['Days_to_Thanksgiving'] = (thanksgiving_dates - X['Date']).dt.days\n",
        "        X['Days_to_Christmas'] = (christmas_dates - X['Date']).dt.days\n",
        "\n",
        "        # Clean up helper cols\n",
        "        X = X.drop(columns=['Week', 'YearNum'])\n",
        "\n",
        "        return X\n",
        "\n",
        "class MissingValueFiller(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self):\n",
        "        self.markdown_cols = ['MarkDown1', 'MarkDown2', 'MarkDown3', 'MarkDown4', 'MarkDown5']\n",
        "        self.mean_cols = ['CPI', 'Unemployment']\n",
        "        self.mean_values = {}\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        for col in self.mean_cols:\n",
        "            if col in X.columns:\n",
        "                self.mean_values[col] = X[col].mean()\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        X = X.copy()\n",
        "\n",
        "        # Fill markdowns with 0\n",
        "        for col in self.markdown_cols:\n",
        "            if col in X.columns:\n",
        "                X[col] = X[col].fillna(0.0)\n",
        "\n",
        "        # Fill CPI and Unemployment with learned mean\n",
        "        for col in self.mean_cols:\n",
        "            if col in X.columns and col in self.mean_values:\n",
        "                X[col] = X[col].fillna(self.mean_values[col])\n",
        "\n",
        "        return X\n",
        "\n",
        "class CategoricalEncoder(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self):\n",
        "        self.type_mapping = {'A': 3, 'B': 2, 'C': 1}\n",
        "        self.holiday_mapping = {False: 0, True: 1}\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        X = X.copy()\n",
        "\n",
        "        if 'Type' in X.columns:\n",
        "            X['Type'] = X['Type'].map(self.type_mapping)\n",
        "\n",
        "        if 'IsHoliday' in X.columns:\n",
        "            X['IsHoliday'] = X['IsHoliday'].map(self.holiday_mapping)\n",
        "\n",
        "        return X\n",
        "\n",
        "class StoreAggregator(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self):\n",
        "        self.timeseries = {}\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        self.timeseries = {}\n",
        "        for store in X['Store'].unique():\n",
        "            self.aggregate_store_info(store, X)\n",
        "        return self.timeseries\n",
        "\n",
        "    def aggregate_store_info(self, store_id, X):\n",
        "        store_data = X[X['Store'] == store_id].copy()\n",
        "\n",
        "        # Check if Weekly_Sales exists (train data) or not (test data)\n",
        "        has_weekly_sales = 'Weekly_Sales' in store_data.columns\n",
        "\n",
        "        if has_weekly_sales:\n",
        "            sum_columns = ['Weekly_Sales', 'MarkDown1', 'MarkDown2', 'MarkDown3', 'MarkDown4', 'MarkDown5']\n",
        "        else:\n",
        "            sum_columns = ['MarkDown1', 'MarkDown2', 'MarkDown3', 'MarkDown4', 'MarkDown5']\n",
        "\n",
        "        first_columns = ['IsHoliday', 'Temperature', 'Fuel_Price', 'CPI', 'Unemployment',\n",
        "                        'Type', 'Size', 'Day', 'Month', 'Year', 'SuperbowlWeek',\n",
        "                        'LaborDayWeek', 'ThanksgivingWeek', 'ChristmasWeek',\n",
        "                        'Days_to_Thanksgiving', 'Days_to_Christmas']\n",
        "\n",
        "        agg_dict = {}\n",
        "\n",
        "        # Add sum columns that exist in the data\n",
        "        for col in sum_columns:\n",
        "            if col in store_data.columns:\n",
        "                agg_dict[col] = 'sum'\n",
        "\n",
        "        # Add first columns that exist in the data\n",
        "        for col in first_columns:\n",
        "            if col in store_data.columns:\n",
        "                agg_dict[col] = 'first'\n",
        "\n",
        "        aggregated = store_data.groupby(['Date', 'Store']).agg(agg_dict).reset_index()\n",
        "        aggregated = aggregated.sort_values('Date').reset_index(drop=True)\n",
        "\n",
        "        # Calculate department proportions only if Weekly_Sales exists\n",
        "        if has_weekly_sales:\n",
        "            dept_proportions = self.calculate_dept_proportions(store_data)\n",
        "        else:\n",
        "            dept_proportions = None\n",
        "\n",
        "        self.timeseries[store_id] = (aggregated, dept_proportions)\n",
        "        return aggregated\n",
        "\n",
        "    def calculate_dept_proportions(self, store_data):\n",
        "        dept_totals = store_data.groupby('Dept')['Weekly_Sales'].sum()\n",
        "        store_total = store_data['Weekly_Sales'].sum()\n",
        "\n",
        "        if store_total == 0:\n",
        "            num_depts = len(dept_totals)\n",
        "            return {dept: 1.0/num_depts for dept in dept_totals.index}\n",
        "\n",
        "        dept_proportions = (dept_totals / store_total).to_dict()\n",
        "        return dept_proportions"
      ],
      "metadata": {
        "id": "zaaTlksM_yT0"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "# Pipeline with store aggregation\n",
        "pipeline = Pipeline([\n",
        "   ('merge', BaseMerger(features, stores)),\n",
        "   ('feature_add', FeatureAdder()),\n",
        "   ('value_fill', MissingValueFiller()),\n",
        "   ('cat_encoder', CategoricalEncoder()),\n",
        "   ('store_agg', StoreAggregator())\n",
        "])\n",
        "\n",
        "# Transform training data\n",
        "train_aggregated = pipeline.fit_transform(train)\n",
        "test_aggregated = pipeline.transform(test)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gwnga6Q0_0FK",
        "outputId": "8c9344de-c718-4480-d0e1-3289fb0bd60d"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/pipeline.py:62: FutureWarning: This Pipeline instance is not fitted yet. Call 'fit' with appropriate arguments before using other methods such as transform, predict, etc. This will raise an error in 1.8 instead of the current warning.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install dagshub\n",
        "!pip install mlflow"
      ],
      "metadata": {
        "id": "9GPyxh7Od-rY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import dagshub\n",
        "import mlflow\n",
        "dagshub.init(repo_owner='dimna21', repo_name='ML_Final_Project', mlflow=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 65
        },
        "id": "Qj-z6Dgqd7nD",
        "outputId": "36efe4c0-365d-44b6-99cb-0964bf08e1a5"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Accessing as dimna21\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Accessing as dimna21\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Initialized MLflow to track repo \u001b[32m\"dimna21/ML_Final_Project\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Initialized MLflow to track repo <span style=\"color: #008000; text-decoration-color: #008000\">\"dimna21/ML_Final_Project\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Repository dimna21/ML_Final_Project initialized!\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Repository dimna21/ML_Final_Project initialized!\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
        "from tqdm import tqdm\n",
        "ls = ['IsHoliday', 'Temperature', 'Fuel_Price', 'CPI', 'Unemployment',\n",
        "                        'Type', 'Size', 'SuperbowlWeek',\n",
        "                        'LaborDayWeek', 'ThanksgivingWeek', 'ChristmasWeek',\n",
        "                        'Days_to_Thanksgiving', 'Days_to_Christmas', 'MarkDown1',\n",
        "                        'MarkDown2', 'MarkDown3', 'MarkDown4', 'MarkDown5']\n",
        "store_models = {}\n",
        "for store_id in tqdm(train_aggregated.keys()):\n",
        "   store_ts, dept_props = train_aggregated[store_id]\n",
        "\n",
        "   sales = store_ts['Weekly_Sales'].values\n",
        "\n",
        "   exog = store_ts[ls].values\n",
        "\n",
        "   model = SARIMAX(\n",
        "       endog=sales,\n",
        "       exog=exog,\n",
        "       order=(2, 1, 1),\n",
        "       seasonal_order=(1, 1, 1, 52),\n",
        "       enforce_stationarity=False,\n",
        "       enforce_invertibility=False\n",
        "   )\n",
        "\n",
        "   fitted_model = model.fit(disp=False, maxiter=80, method='lbfgs')\n",
        "   store_models[store_id] = (fitted_model, dept_props)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mTt6Zwa1_2NX",
        "outputId": "07715ca1-848c-4121-8671-f4610f49006f"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 45/45 [1:13:40<00:00, 98.23s/it] \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import mlflow\n",
        "import pickle\n",
        "import os\n",
        "\n",
        "mlflow.set_experiment(\"SARIMAX_Store_Models (2,1,1)\")\n",
        "run = mlflow.start_run(run_name=\"SARIMAX_Model_Training(2,1,1)\")\n",
        "\n",
        "# Log training features\n",
        "feature_list = ['IsHoliday', 'Type', 'Size', 'SuperbowlWeek',\n",
        "                'LaborDayWeek', 'ThanksgivingWeek', 'ChristmasWeek']\n",
        "mlflow.log_param(\"features_used\", \", \".join(feature_list))\n",
        "\n",
        "# Log SARIMAX model parameters\n",
        "mlflow.log_param(\"order\", \"(2,1,1)\")\n",
        "mlflow.log_param(\"seasonal_order\", \"(1,1,1,52)\")\n",
        "mlflow.log_param(\"enforce_stationarity\", False)\n",
        "mlflow.log_param(\"enforce_invertibility\", False)\n",
        "mlflow.log_param(\"maxiter\", \"80\")\n",
        "mlflow.log_param(\"method\", \"lbfgs\")\n",
        "\n",
        "\n",
        "# Finish MLflow run\n",
        "mlflow.end_run()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jgl0cCqDeVzX",
        "outputId": "7c778c88-4c21-4ac9-faf5-025c8ac02b8b"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025/07/27 21:59:12 INFO mlflow.tracking.fluent: Experiment with name 'SARIMAX_Store_Models (2,1,1)' does not exist. Creating a new experiment.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🏃 View run SARIMAX_Model_Training(2,1,1) at: https://dagshub.com/dimna21/ML_Final_Project.mlflow/#/experiments/8/runs/051e53d25b7447bbab4897b2091eff31\n",
            "🧪 View experiment at: https://dagshub.com/dimna21/ML_Final_Project.mlflow/#/experiments/8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "max_train_date = pd.to_datetime(train['Date'].max())\n",
        "\n",
        "def get_forecast(idx):\n",
        "    sample = test.iloc[idx]\n",
        "    store = sample['Store']\n",
        "    dept = sample['Dept']\n",
        "    pred_date = pd.to_datetime(sample['Date'])\n",
        "    weeks = (pred_date-max_train_date).days//7\n",
        "    store_model, dept_props = store_models[store]\n",
        "    df, _ = test_aggregated[store]\n",
        "    exog = df[ls].values[:weeks]\n",
        "    forecast = store_model.forecast(steps=weeks, exog=exog)[-1]\n",
        "\n",
        "    if dept in dept_props.keys():\n",
        "        return forecast*dept_props[dept]\n",
        "    else:\n",
        "        return forecast/len(dept_props)\n",
        "\n"
      ],
      "metadata": {
        "id": "FSygFC-e_5e7"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "from datetime import datetime\n",
        "\n",
        "# Function to process a single row\n",
        "def process_row(idx):\n",
        "    sample = test.iloc[idx]\n",
        "    store = sample['Store']\n",
        "    dept = sample['Dept']\n",
        "    date = pd.to_datetime(sample['Date'])\n",
        "\n",
        "    prediction = get_forecast(idx)\n",
        "\n",
        "    return {\n",
        "        'Id': f\"{store}_{dept}_{date.strftime('%Y-%m-%d')}\",\n",
        "        'Weekly_Sales': max(0, prediction)\n",
        "    }\n",
        "\n",
        "# Run in parallel with progress bar\n",
        "submission_data = []\n",
        "with ThreadPoolExecutor() as executor:\n",
        "    futures = [executor.submit(process_row, idx) for idx in range(len(test))]\n",
        "    for future in tqdm(as_completed(futures), total=len(futures)):\n",
        "        submission_data.append(future.result())\n",
        "\n",
        "# Save the results\n",
        "submission_df = pd.DataFrame(submission_data)\n",
        "submission_df.to_csv(\"/content/drive/MyDrive/ML_Final_Project/SARIMAX_Submission2.csv\", index=False)\n",
        "print(f\"Submission created with {len(submission_df)} predictions\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "emJi0JerS5JU",
        "outputId": "e5a48506-ead1-42dc-93af-3995bac2fc87"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 115064/115064 [30:11<00:00, 63.51it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Submission created with 115064 predictions\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VcHOHDDmc8EK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}